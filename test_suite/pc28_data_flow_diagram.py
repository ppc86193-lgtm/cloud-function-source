#!/usr/bin/env python3
"""
PC28æ•°æ®æµç¨‹å›¾ç”Ÿæˆå™¨
åˆ›å»ºå®Œæ•´çš„æ•°æ®æµç¨‹å›¾ï¼Œå±•ç¤ºä»APIé‡‡é›†åˆ°æœ€ç»ˆè¾“å‡ºçš„å®Œæ•´é“¾è·¯
"""

import json
import subprocess
from datetime import datetime
from typing import Dict, List, Any, Optional
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PC28DataFlowDiagram:
    """PC28æ•°æ®æµç¨‹å›¾ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.project_id = "wprojectl"
        self.dataset_id = "pc28_lab"
        self.flow_data = {}
        
    def run_bq_query(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡ŒBigQueryæŸ¥è¯¢"""
        try:
            cmd = ['bq', 'query', '--use_legacy_sql=false', '--format=json', query]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            return {"success": True, "data": json.loads(result.stdout) if result.stdout.strip() else []}
        except subprocess.CalledProcessError as e:
            logger.error(f"BigQueryæŸ¥è¯¢å¤±è´¥: {e.stderr}")
            return {"success": False, "error": e.stderr}
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def analyze_data_flow(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®æµç¨‹"""
        logger.info("ğŸ” åˆ†æPC28æ•°æ®æµç¨‹...")
        
        flow_analysis = {
            "timestamp": datetime.now().isoformat(),
            "layers": {
                "1_raw_data": self._analyze_raw_data_layer(),
                "2_prediction_views": self._analyze_prediction_layer(),
                "3_canonical_views": self._analyze_canonical_layer(),
                "4_ensemble_layer": self._analyze_ensemble_layer(),
                "5_signal_pool": self._analyze_signal_pool_layer(),
                "6_decision_layer": self._analyze_decision_layer()
            },
            "dependencies": self._analyze_dependencies(),
            "data_volumes": self._analyze_data_volumes(),
            "bottlenecks": self._identify_bottlenecks()
        }
        
        return flow_analysis
    
    def _analyze_raw_data_layer(self) -> Dict[str, Any]:
        """åˆ†æåŸå§‹æ•°æ®å±‚"""
        logger.info("åˆ†æåŸå§‹æ•°æ®å±‚...")
        
        raw_tables = [
            'cloud_pred_today_norm',
            'p_cloud_clean_merged_dedup_v',
            'p_map_clean_merged_dedup_v', 
            'p_size_clean_merged_dedup_v'
        ]
        
        layer_info = {
            "description": "åŸå§‹æ•°æ®é‡‡é›†å±‚ - ä»APIç›´æ¥å†™å…¥çš„æ•°æ®",
            "tables": {},
            "total_rows": 0,
            "data_sources": ["Cloud API", "Map API", "Size API"],
            "update_frequency": "å®æ—¶"
        }
        
        for table in raw_tables:
            table_info = self._get_table_info(table)
            layer_info["tables"][table] = table_info
            layer_info["total_rows"] += table_info.get("row_count", 0)
        
        return layer_info
    
    def _analyze_prediction_layer(self) -> Dict[str, Any]:
        """åˆ†æé¢„æµ‹è§†å›¾å±‚"""
        logger.info("åˆ†æé¢„æµ‹è§†å›¾å±‚...")
        
        prediction_views = [
            'p_cloud_today_v',
            'p_map_today_v',
            'p_size_today_v'
        ]
        
        layer_info = {
            "description": "é¢„æµ‹è§†å›¾å±‚ - åŸºäºåŸå§‹æ•°æ®çš„é¢„æµ‹ç»“æœ",
            "views": {},
            "total_rows": 0,
            "transformations": ["æ—¶é—´è¿‡æ»¤", "æ•°æ®æ¸…æ´—", "é¢„æµ‹è®¡ç®—"]
        }
        
        for view in prediction_views:
            view_info = self._get_table_info(view)
            view_info["dependencies"] = self._get_view_dependencies(view)
            layer_info["views"][view] = view_info
            layer_info["total_rows"] += view_info.get("row_count", 0)
        
        return layer_info
    
    def _analyze_canonical_layer(self) -> Dict[str, Any]:
        """åˆ†ææ ‡å‡†åŒ–è§†å›¾å±‚"""
        logger.info("åˆ†ææ ‡å‡†åŒ–è§†å›¾å±‚...")
        
        canonical_views = [
            'p_map_today_canon_v',
            'p_size_today_canon_v'
        ]
        
        layer_info = {
            "description": "æ ‡å‡†åŒ–è§†å›¾å±‚ - ç»Ÿä¸€æ ¼å¼çš„é¢„æµ‹ç»“æœ",
            "views": {},
            "total_rows": 0,
            "transformations": ["æ ¼å¼æ ‡å‡†åŒ–", "æ¦‚ç‡è®¡ç®—", "å†³ç­–æ˜ å°„"]
        }
        
        for view in canonical_views:
            view_info = self._get_table_info(view)
            view_info["dependencies"] = self._get_view_dependencies(view)
            layer_info["views"][view] = view_info
            layer_info["total_rows"] += view_info.get("row_count", 0)
        
        return layer_info
    
    def _analyze_ensemble_layer(self) -> Dict[str, Any]:
        """åˆ†æé›†æˆå±‚"""
        logger.info("åˆ†æé›†æˆå±‚...")
        
        ensemble_views = [
            'ensemble_pool_today_v2'
        ]
        
        layer_info = {
            "description": "é›†æˆå±‚ - å¤šæ¨¡å‹èåˆé¢„æµ‹",
            "views": {},
            "total_rows": 0,
            "transformations": ["æƒé‡è®¡ç®—", "æ¨¡å‹èåˆ", "é›†æˆé¢„æµ‹"]
        }
        
        for view in ensemble_views:
            view_info = self._get_table_info(view)
            view_info["dependencies"] = self._get_view_dependencies(view)
            layer_info["views"][view] = view_info
            layer_info["total_rows"] += view_info.get("row_count", 0)
        
        return layer_info
    
    def _analyze_signal_pool_layer(self) -> Dict[str, Any]:
        """åˆ†æä¿¡å·æ± å±‚"""
        logger.info("åˆ†æä¿¡å·æ± å±‚...")
        
        signal_views = [
            'signal_pool_union_v3'
        ]
        
        layer_info = {
            "description": "ä¿¡å·æ± å±‚ - ç»Ÿä¸€çš„äº¤æ˜“ä¿¡å·é›†åˆ",
            "views": {},
            "total_rows": 0,
            "transformations": ["ä¿¡å·åˆå¹¶", "æ ¼å¼ç»Ÿä¸€", "å…ƒæ•°æ®æ·»åŠ "]
        }
        
        for view in signal_views:
            view_info = self._get_table_info(view)
            view_info["dependencies"] = self._get_view_dependencies(view)
            layer_info["views"][view] = view_info
            layer_info["total_rows"] += view_info.get("row_count", 0)
        
        return layer_info
    
    def _analyze_decision_layer(self) -> Dict[str, Any]:
        """åˆ†æå†³ç­–å±‚"""
        logger.info("åˆ†æå†³ç­–å±‚...")
        
        decision_views = [
            'lab_push_candidates_v2'
        ]
        
        layer_info = {
            "description": "å†³ç­–å±‚ - æœ€ç»ˆçš„äº¤æ˜“å†³ç­–å€™é€‰",
            "views": {},
            "total_rows": 0,
            "transformations": ["é£é™©è¯„ä¼°", "Kellyå…¬å¼", "å†³ç­–è¿‡æ»¤"],
            "parameters": self._get_runtime_params()
        }
        
        for view in decision_views:
            view_info = self._get_table_info(view)
            view_info["dependencies"] = self._get_view_dependencies(view)
            layer_info["views"][view] = view_info
            layer_info["total_rows"] += view_info.get("row_count", 0)
        
        return layer_info
    
    def _get_table_info(self, table_name: str) -> Dict[str, Any]:
        """è·å–è¡¨ä¿¡æ¯"""
        # è·å–è¡Œæ•°
        query = f"SELECT COUNT(*) as count FROM `{self.project_id}.{self.dataset_id}.{table_name}`"
        result = self.run_bq_query(query)
        
        row_count = 0
        if result["success"] and result["data"]:
            row_count = int(result["data"][0]["count"])
        
        # è·å–æœ€æ–°æ•°æ®æ—¶é—´
        query = f"""
        SELECT 
            MAX(DATE(ts_utc, 'Asia/Shanghai')) as latest_date,
            MIN(DATE(ts_utc, 'Asia/Shanghai')) as earliest_date
        FROM `{self.project_id}.{self.dataset_id}.{table_name}`
        WHERE ts_utc IS NOT NULL
        """
        
        date_result = self.run_bq_query(query)
        latest_date = None
        earliest_date = None
        
        if date_result["success"] and date_result["data"]:
            data = date_result["data"][0]
            latest_date = data.get("latest_date")
            earliest_date = data.get("earliest_date")
        
        return {
            "row_count": row_count,
            "latest_date": latest_date,
            "earliest_date": earliest_date,
            "status": "healthy" if row_count > 0 else "empty"
        }
    
    def _get_view_dependencies(self, view_name: str) -> List[str]:
        """è·å–è§†å›¾ä¾èµ–å…³ç³»"""
        try:
            cmd = ['bq', 'show', '--view', f'{self.project_id}:{self.dataset_id}.{view_name}']
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            
            # ç®€å•è§£æSQLä¸­çš„è¡¨å
            sql = result.stdout
            dependencies = []
            
            # æŸ¥æ‰¾FROMå’ŒJOINå­å¥ä¸­çš„è¡¨å
            import re
            pattern = r'`([^`]+\.[^`]+\.[^`]+)`'
            matches = re.findall(pattern, sql)
            
            for match in matches:
                table_name = match.split('.')[-1]  # åªå–è¡¨åéƒ¨åˆ†
                if table_name not in dependencies:
                    dependencies.append(table_name)
            
            return dependencies
            
        except subprocess.CalledProcessError:
            return []
    
    def _get_runtime_params(self) -> Dict[str, Any]:
        """è·å–è¿è¡Œæ—¶å‚æ•°"""
        query = f"SELECT * FROM `{self.project_id}.{self.dataset_id}.runtime_params`"
        result = self.run_bq_query(query)
        
        if result["success"] and result["data"]:
            return {"count": len(result["data"]), "markets": list(set([r["market"] for r in result["data"]]))}
        
        return {"count": 0, "markets": []}
    
    def _analyze_dependencies(self) -> Dict[str, List[str]]:
        """åˆ†æä¾èµ–å…³ç³»"""
        logger.info("åˆ†æä¾èµ–å…³ç³»...")
        
        dependencies = {}
        
        # ä¸»è¦è§†å›¾çš„ä¾èµ–å…³ç³»
        views_to_analyze = [
            'p_cloud_today_v',
            'p_map_today_v',
            'p_size_today_v',
            'p_map_today_canon_v',
            'p_size_today_canon_v',
            'ensemble_pool_today_v2',
            'signal_pool_union_v3',
            'lab_push_candidates_v2'
        ]
        
        for view in views_to_analyze:
            dependencies[view] = self._get_view_dependencies(view)
        
        return dependencies
    
    def _analyze_data_volumes(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®é‡"""
        logger.info("åˆ†ææ•°æ®é‡...")
        
        # è·å–å„å±‚æ•°æ®é‡
        query = f"""
        SELECT 
            'raw_data' as layer,
            SUM(CASE WHEN table_name IN ('cloud_pred_today_norm', 'p_cloud_clean_merged_dedup_v', 'p_map_clean_merged_dedup_v', 'p_size_clean_merged_dedup_v') THEN row_count ELSE 0 END) as total_rows
        FROM `{self.project_id}.{self.dataset_id}.__TABLES__`
        WHERE table_name IN ('cloud_pred_today_norm', 'p_cloud_clean_merged_dedup_v', 'p_map_clean_merged_dedup_v', 'p_size_clean_merged_dedup_v')
        """
        
        # ç”±äº__TABLES__å¯èƒ½ä¸å¯ç”¨ï¼Œæˆ‘ä»¬æ‰‹åŠ¨è®¡ç®—
        volumes = {
            "raw_data_layer": 0,
            "prediction_layer": 0,
            "canonical_layer": 0,
            "signal_pool": 0,
            "decision_layer": 0
        }
        
        # åŸå§‹æ•°æ®å±‚
        raw_tables = ['cloud_pred_today_norm', 'p_cloud_clean_merged_dedup_v', 'p_map_clean_merged_dedup_v', 'p_size_clean_merged_dedup_v']
        for table in raw_tables:
            info = self._get_table_info(table)
            volumes["raw_data_layer"] += info.get("row_count", 0)
        
        # é¢„æµ‹å±‚
        pred_tables = ['p_cloud_today_v', 'p_map_today_v', 'p_size_today_v']
        for table in pred_tables:
            info = self._get_table_info(table)
            volumes["prediction_layer"] += info.get("row_count", 0)
        
        # æ ‡å‡†åŒ–å±‚
        canon_tables = ['p_map_today_canon_v', 'p_size_today_canon_v']
        for table in canon_tables:
            info = self._get_table_info(table)
            volumes["canonical_layer"] += info.get("row_count", 0)
        
        # ä¿¡å·æ± 
        signal_info = self._get_table_info('signal_pool_union_v3')
        volumes["signal_pool"] = signal_info.get("row_count", 0)
        
        # å†³ç­–å±‚
        decision_info = self._get_table_info('lab_push_candidates_v2')
        volumes["decision_layer"] = decision_info.get("row_count", 0)
        
        return volumes
    
    def _identify_bottlenecks(self) -> List[Dict[str, Any]]:
        """è¯†åˆ«ç“¶é¢ˆ"""
        logger.info("è¯†åˆ«ç³»ç»Ÿç“¶é¢ˆ...")
        
        bottlenecks = []
        
        # æ£€æŸ¥ç©ºè¡¨
        empty_tables = []
        critical_tables = [
            'lab_push_candidates_v2',
            'ensemble_pool_today_v2',
            'signal_pool_union_v3'
        ]
        
        for table in critical_tables:
            info = self._get_table_info(table)
            if info["row_count"] == 0:
                empty_tables.append(table)
        
        if empty_tables:
            bottlenecks.append({
                "type": "empty_tables",
                "severity": "high",
                "description": f"å…³é”®è¡¨æ— æ•°æ®: {', '.join(empty_tables)}",
                "tables": empty_tables
            })
        
        # æ£€æŸ¥æ•°æ®æ–°é²œåº¦
        stale_tables = []
        for table in ['p_cloud_today_v', 'p_map_today_v', 'p_size_today_v']:
            info = self._get_table_info(table)
            if info["latest_date"]:
                from datetime import datetime, date
                latest = datetime.strptime(info["latest_date"], "%Y-%m-%d").date()
                today = date.today()
                days_behind = (today - latest).days
                
                if days_behind > 1:
                    stale_tables.append({"table": table, "days_behind": days_behind})
        
        if stale_tables:
            bottlenecks.append({
                "type": "stale_data",
                "severity": "medium",
                "description": "æ•°æ®è¿‡æœŸ",
                "details": stale_tables
            })
        
        return bottlenecks
    
    def generate_mermaid_diagram(self, flow_analysis: Dict[str, Any]) -> str:
        """ç”ŸæˆMermaidæµç¨‹å›¾"""
        logger.info("ç”ŸæˆMermaidæµç¨‹å›¾...")
        
        mermaid = """
graph TD
    %% åŸå§‹æ•°æ®å±‚
    API1[Cloud API] --> T1[cloud_pred_today_norm]
    API2[Map API] --> T2[p_map_clean_merged_dedup_v]
    API3[Size API] --> T3[p_size_clean_merged_dedup_v]
    
    %% é¢„æµ‹è§†å›¾å±‚
    T1 --> V1[p_cloud_today_v]
    T2 --> V2[p_map_today_v]
    T3 --> V3[p_size_today_v]
    
    %% æ ‡å‡†åŒ–è§†å›¾å±‚
    V2 --> C1[p_map_today_canon_v]
    V3 --> C2[p_size_today_canon_v]
    
    %% é›†æˆå±‚
    V1 --> E1[ensemble_pool_today_v2]
    V2 --> E1
    V3 --> E1
    
    %% ä¿¡å·æ± å±‚
    E1 --> S1[signal_pool_union_v3]
    C1 --> S1
    C2 --> S1
    
    %% å†³ç­–å±‚
    S1 --> D1[lab_push_candidates_v2]
    P1[runtime_params] --> D1
    
    %% æ ·å¼
    classDef apiClass fill:#e1f5fe
    classDef rawClass fill:#f3e5f5
    classDef viewClass fill:#e8f5e8
    classDef canonClass fill:#fff3e0
    classDef ensembleClass fill:#fce4ec
    classDef signalClass fill:#e0f2f1
    classDef decisionClass fill:#ffebee
    
    class API1,API2,API3 apiClass
    class T1,T2,T3 rawClass
    class V1,V2,V3 viewClass
    class C1,C2 canonClass
    class E1 ensembleClass
    class S1 signalClass
    class D1,P1 decisionClass
"""
        
        return mermaid.strip()
    
    def generate_ascii_diagram(self, flow_analysis: Dict[str, Any]) -> str:
        """ç”ŸæˆASCIIæµç¨‹å›¾"""
        logger.info("ç”ŸæˆASCIIæµç¨‹å›¾...")
        
        volumes = flow_analysis["data_volumes"]
        
        ascii_diagram = f"""
PC28æ•°æ®æµç¨‹å›¾
================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cloud API     â”‚    â”‚    Map API      â”‚    â”‚   Size API      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚cloud_pred_today â”‚    â”‚p_map_clean_     â”‚    â”‚p_size_clean_    â”‚
â”‚_norm            â”‚    â”‚merged_dedup_v   â”‚    â”‚merged_dedup_v   â”‚
â”‚({volumes["raw_data_layer"]} rows)        â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚p_cloud_today_v  â”‚    â”‚p_map_today_v    â”‚    â”‚p_size_today_v   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â”‚                      â–¼                      â–¼
          â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚            â”‚p_map_today_     â”‚    â”‚p_size_today_    â”‚
          â”‚            â”‚canon_v          â”‚    â”‚canon_v          â”‚
          â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ensemble_pool_   â”‚
                       â”‚today_v2         â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚signal_pool_     â”‚
                       â”‚union_v3         â”‚
                       â”‚({volumes["signal_pool"]} rows)        â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚lab_push_        â”‚
                       â”‚candidates_v2    â”‚
                       â”‚({volumes["decision_layer"]} rows)        â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ•°æ®æµç¨‹è¯´æ˜:
1. åŸå§‹æ•°æ®å±‚: APIç›´æ¥å†™å…¥ ({volumes["raw_data_layer"]} æ€»è¡Œæ•°)
2. é¢„æµ‹è§†å›¾å±‚: åŸºç¡€é¢„æµ‹å¤„ç† ({volumes["prediction_layer"]} æ€»è¡Œæ•°)
3. æ ‡å‡†åŒ–å±‚: æ ¼å¼ç»Ÿä¸€ ({volumes["canonical_layer"]} æ€»è¡Œæ•°)
4. ä¿¡å·æ± å±‚: ä¿¡å·åˆå¹¶ ({volumes["signal_pool"]} è¡Œæ•°)
5. å†³ç­–å±‚: æœ€ç»ˆå†³ç­– ({volumes["decision_layer"]} è¡Œæ•°)
"""
        
        return ascii_diagram
    
    def save_analysis_report(self, flow_analysis: Dict[str, Any]):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_filename = f"pc28_data_flow_analysis_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(flow_analysis, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_filename = f"pc28_data_flow_report_{timestamp}.md"
        with open(md_filename, 'w', encoding='utf-8') as f:
            f.write(self._generate_markdown_report(flow_analysis))
        
        # ç”ŸæˆMermaidå›¾
        mermaid_filename = f"pc28_data_flow_diagram_{timestamp}.mmd"
        with open(mermaid_filename, 'w', encoding='utf-8') as f:
            f.write(self.generate_mermaid_diagram(flow_analysis))
        
        logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜:")
        logger.info(f"  JSON: {json_filename}")
        logger.info(f"  Markdown: {md_filename}")
        logger.info(f"  Mermaid: {mermaid_filename}")
    
    def _generate_markdown_report(self, flow_analysis: Dict[str, Any]) -> str:
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        layers = flow_analysis["layers"]
        volumes = flow_analysis["data_volumes"]
        bottlenecks = flow_analysis["bottlenecks"]
        
        report = f"""# PC28æ•°æ®æµç¨‹åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {flow_analysis["timestamp"]}

## æ¦‚è§ˆ

PC28ç³»ç»Ÿé‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œæ•°æ®ä»APIé‡‡é›†å¼€å§‹ï¼Œç»è¿‡å¤šå±‚å¤„ç†æœ€ç»ˆç”Ÿæˆäº¤æ˜“å†³ç­–ã€‚

### æ•°æ®é‡ç»Ÿè®¡
- åŸå§‹æ•°æ®å±‚: {volumes["raw_data_layer"]:,} è¡Œ
- é¢„æµ‹è§†å›¾å±‚: {volumes["prediction_layer"]:,} è¡Œ  
- æ ‡å‡†åŒ–å±‚: {volumes["canonical_layer"]:,} è¡Œ
- ä¿¡å·æ± : {volumes["signal_pool"]:,} è¡Œ
- å†³ç­–å±‚: {volumes["decision_layer"]:,} è¡Œ

## æ•°æ®æµç¨‹å±‚çº§

### 1. åŸå§‹æ•°æ®å±‚
{layers["1_raw_data"]["description"]}

**æ•°æ®æº**: {", ".join(layers["1_raw_data"]["data_sources"])}
**æ›´æ–°é¢‘ç‡**: {layers["1_raw_data"]["update_frequency"]}
**æ€»è¡Œæ•°**: {layers["1_raw_data"]["total_rows"]:,}

**è¡¨è¯¦æƒ…**:
"""
        
        for table, info in layers["1_raw_data"]["tables"].items():
            report += f"- `{table}`: {info['row_count']:,} è¡Œ ({info['status']})\n"
        
        report += f"""
### 2. é¢„æµ‹è§†å›¾å±‚
{layers["2_prediction_views"]["description"]}

**è½¬æ¢æ“ä½œ**: {", ".join(layers["2_prediction_views"]["transformations"])}
**æ€»è¡Œæ•°**: {layers["2_prediction_views"]["total_rows"]:,}

**è§†å›¾è¯¦æƒ…**:
"""
        
        for view, info in layers["2_prediction_views"]["views"].items():
            deps = ", ".join(info.get("dependencies", []))
            report += f"- `{view}`: {info['row_count']:,} è¡Œ, ä¾èµ–: {deps}\n"
        
        report += f"""
### 3. æ ‡å‡†åŒ–è§†å›¾å±‚
{layers["3_canonical_views"]["description"]}

**è½¬æ¢æ“ä½œ**: {", ".join(layers["3_canonical_views"]["transformations"])}
**æ€»è¡Œæ•°**: {layers["3_canonical_views"]["total_rows"]:,}

**è§†å›¾è¯¦æƒ…**:
"""
        
        for view, info in layers["3_canonical_views"]["views"].items():
            deps = ", ".join(info.get("dependencies", []))
            report += f"- `{view}`: {info['row_count']:,} è¡Œ, ä¾èµ–: {deps}\n"
        
        report += f"""
### 4. é›†æˆå±‚
{layers["4_ensemble_layer"]["description"]}

**è½¬æ¢æ“ä½œ**: {", ".join(layers["4_ensemble_layer"]["transformations"])}
**æ€»è¡Œæ•°**: {layers["4_ensemble_layer"]["total_rows"]:,}

### 5. ä¿¡å·æ± å±‚
{layers["5_signal_pool"]["description"]}

**è½¬æ¢æ“ä½œ**: {", ".join(layers["5_signal_pool"]["transformations"])}
**æ€»è¡Œæ•°**: {layers["5_signal_pool"]["total_rows"]:,}

### 6. å†³ç­–å±‚
{layers["6_decision_layer"]["description"]}

**è½¬æ¢æ“ä½œ**: {", ".join(layers["6_decision_layer"]["transformations"])}
**æ€»è¡Œæ•°**: {layers["6_decision_layer"]["total_rows"]:,}
**å‚æ•°é…ç½®**: {layers["6_decision_layer"]["parameters"]["count"]} ä¸ªå‚æ•°, æ”¯æŒå¸‚åœº: {", ".join(layers["6_decision_layer"]["parameters"]["markets"])}

## ç³»ç»Ÿç“¶é¢ˆåˆ†æ

"""
        
        if bottlenecks:
            for bottleneck in bottlenecks:
                report += f"### {bottleneck['type']} ({bottleneck['severity']})\n"
                report += f"{bottleneck['description']}\n\n"
        else:
            report += "âœ… æœªå‘ç°æ˜æ˜¾ç“¶é¢ˆ\n\n"
        
        report += """
## ASCIIæ•°æ®æµç¨‹å›¾

```
""" + self.generate_ascii_diagram(flow_analysis) + """
```

## å»ºè®®

1. **ç›‘æ§æ•°æ®æ–°é²œåº¦**: ç¡®ä¿åŸå§‹æ•°æ®åŠæ—¶æ›´æ–°
2. **ä¼˜åŒ–ç©ºè¡¨é—®é¢˜**: é‡ç‚¹å…³æ³¨å†³ç­–å±‚æ•°æ®ç”Ÿæˆ
3. **å»ºç«‹å‘Šè­¦æœºåˆ¶**: å¯¹å…³é”®èŠ‚ç‚¹è®¾ç½®ç›‘æ§å‘Šè­¦
4. **å®šæœŸå¥åº·æ£€æŸ¥**: å»ºè®®æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ç³»ç»ŸçŠ¶æ€

"""
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    diagram = PC28DataFlowDiagram()
    
    if len(sys.argv) > 1 and sys.argv[1] == "analyze":
        print("ğŸ” å¼€å§‹åˆ†æPC28æ•°æ®æµç¨‹...")
        flow_analysis = diagram.analyze_data_flow()
        
        print("\nğŸ“Š æ•°æ®æµç¨‹åˆ†æå®Œæˆ!")
        print(f"æ€»å±‚çº§: {len(flow_analysis['layers'])}")
        print(f"æ•°æ®é‡: {sum(flow_analysis['data_volumes'].values()):,} æ€»è¡Œæ•°")
        print(f"ç“¶é¢ˆ: {len(flow_analysis['bottlenecks'])} ä¸ª")
        
        # ä¿å­˜æŠ¥å‘Š
        diagram.save_analysis_report(flow_analysis)
        
        # æ˜¾ç¤ºASCIIå›¾
        print("\n" + "="*60)
        print(diagram.generate_ascii_diagram(flow_analysis))
        print("="*60)
        
    else:
        print("ç”¨æ³•: python pc28_data_flow_diagram.py analyze")

if __name__ == "__main__":
    main()