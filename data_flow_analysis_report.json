{
  "analysis_metadata": {
    "generated_at": "2025-09-29T06:01:22.229586",
    "analyzer_version": "1.0",
    "project_root": "/Users/a606/cloud_function_source"
  },
  "analysis_results": {
    "field_dependencies": {},
    "data_flow_paths": {},
    "code_references": {
      "score_ledger": {
        "result_digits": {
          "references": [
            {
              "file": "database_table_optimizer.py",
              "line": 65,
              "context": "63:                     {'name': 'profit_loss', 'type': 'FLOAT64', 'nullable': True, 'usage': 'high'},\n64:                     {'name': 'status', 'type': 'STRING', 'nullable': True, 'usage': 'high'},\n65:                     {'name': 'result_digits', 'type': 'REPEATED INTEGER', 'nullable': True, 'usage': 'low'},  # 冗余字段\n66:                     {'name': 'numbers', 'type': 'REPEATED INTEGER', 'nullable': True, 'usage': 'high'},\n67:                     {'name': 'result_sum', 'type': 'INTEGER', 'nullable': True, 'usage': 'high'},",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 212,
              "context": "210:         \"\"\"检查是否为冗余字段\"\"\"\n211:         redundant_patterns = {\n212:             'result_digits': 'numbers',  # result_digits与numbers重复\n213:             'ts_utc': 'timestamp',       # ts_utc与timestamp重复\n214:             'data_source': 'source',     # 类似功能字段",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 212,
              "context": "210:         \"\"\"检查是否为冗余字段\"\"\"\n211:         redundant_patterns = {\n212:             'result_digits': 'numbers',  # result_digits与numbers重复\n213:             'ts_utc': 'timestamp',       # ts_utc与timestamp重复\n214:             'data_source': 'source',     # 类似功能字段",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 224,
              "context": "222:             ('timestamp', 'ts_utc', 'created_at'),\n223:             ('source', 'data_source'),\n224:             ('numbers', 'result_digits')\n225:         ]\n226:         ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "field_cleanup_sql_generator.py",
              "line": 33,
              "context": "31:                 {\n32:                     'table': 'score_ledger',\n33:                     'field': 'result_digits',\n34:                     'reason': '使用率极低，与numbers字段功能重复',\n35:                     'risk': 'medium',",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_migration_generator.py",
              "line": 43,
              "context": "41:                     table_name='score_ledger',\n42:                     optimization_type='remove_field',\n43:                     description='移除冗余字段 result_digits',\n44:                     impact='15%',\n45:                     sql_before='SELECT * FROM score_ledger',",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_migration_generator.py",
              "line": 47,
              "context": "45:                     sql_before='SELECT * FROM score_ledger',\n46:                     sql_after='SELECT order_id, numbers FROM score_ledger',\n47:                     migration_script='ALTER TABLE score_ledger DROP COLUMN result_digits',\n48:                     estimated_savings={'storage': '15%', 'query_time': '10%'}\n49:                 ),",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_migration_generator.py",
              "line": 417,
              "context": "415:         \"\"\"测试字段名提取\"\"\"\n416:         descriptions = [\n417:             \"移除冗余字段 result_digits\",\n418:             \"删除未使用字段 curtime\",\n419:             \"归档字段 old_data\"",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_migration_generator.py",
              "line": 422,
              "context": "420:         ]\n421:         \n422:         expected_fields = [\"result_digits\", \"curtime\", \"old_data\"]\n423:         \n424:         for desc, expected in zip(descriptions, expected_fields):",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "field_usage_analysis.py",
              "line": 72,
              "context": "70:             'numbers': '开奖号码数组（映射自number）',\n71:             'result_sum': '开奖号码总和（计算得出）',\n72:             'result_digits': '开奖号码数组（与numbers相同）',\n73:             'source': '数据来源标识',\n74:             'created_at': '数据创建时间'",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "field_usage_analysis.py",
              "line": 250,
              "context": "248:                         'number -> numbers': '字符串数组转整数数组',\n249:                         'calculated -> result_sum': '计算字段（数组求和）',\n250:                         'calculated -> result_digits': '冗余字段（与numbers相同）'\n251:                     },\n252:                     'data_transformation': {",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "field_usage_analysis.py",
              "line": 260,
              "context": "258:                 'efficiency_issues': {\n259:                     'redundant_fields': [\n260:                         'result_digits字段与numbers字段完全相同，存在冗余',\n261:                         'curtime字段未被使用但仍在传输'\n262:                     ],",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "field_usage_analysis.py",
              "line": 320,
              "context": "318:                     {\n319:                         'title': '移除冗余字段',\n320:                         'description': '移除result_digits字段，统一使用numbers字段',\n321:                         'implementation': '修改数据模型和BigQuery表结构',\n322:                         'expected_benefit': '减少数据传输和存储开销15%'",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "demo_field_optimization.py",
              "line": 38,
              "context": "36:             \"drawTime\": \"2024-09-25 10:05:30\",\n37:             \"intervalM\": 5,\n38:             \"result_digits\": [4, 5, 6],  # 冗余字段\n39:             \"numbers\": [4, 5, 6],        # 与result_digits重复\n40:             \"curtime\": \"2024-09-25 10:05:30\",  # 未使用字段",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "demo_field_optimization.py",
              "line": 39,
              "context": "37:             \"intervalM\": 5,\n38:             \"result_digits\": [4, 5, 6],  # 冗余字段\n39:             \"numbers\": [4, 5, 6],        # 与result_digits重复\n40:             \"curtime\": \"2024-09-25 10:05:30\",  # 未使用字段\n41:             \"next\": \"20240925003\",       # 未使用字段",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "demo_field_optimization.py",
              "line": 54,
              "context": "52:     \n53:     # 模拟冗余字段检测结果（因为实际API分析可能不包含这些字段）\n54:     redundant_fields = ['result_digits', 'numbers']  # 模拟检测到的冗余字段\n55:     analysis_result['redundant_fields'] = redundant_fields\n56:     print(f\"   - 模拟检测到 {len(redundant_fields)} 个冗余字段\")",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "demo_field_optimization.py",
              "line": 69,
              "context": "67:                 {\"name\": \"issue\", \"type\": \"VARCHAR(20)\", \"nullable\": False},\n68:                 {\"name\": \"openCode\", \"type\": \"VARCHAR(50)\", \"nullable\": False},\n69:                 {\"name\": \"result_digits\", \"type\": \"JSON\", \"nullable\": True},\n70:                 {\"name\": \"numbers\", \"type\": \"JSON\", \"nullable\": True},\n71:                 {\"name\": \"curtime\", \"type\": \"TIMESTAMP\", \"nullable\": True},",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "demo_field_optimization.py",
              "line": 92,
              "context": "90:     field_opts = [\n91:         FieldOptimization(\n92:             field_name=\"result_digits\",\n93:             optimization_type=\"remove_redundant\",\n94:             reason=\"与numbers字段功能重复\",",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "fix_timestamps.py",
              "line": 53,
              "context": "51:         timestamp,\n52:         CAST(REGEXP_EXTRACT(result, r'(\\\\d+)') AS INT64) as result_sum,\n53:         result as result_digits,\n54:         timestamp as created_at,\n55:         MOD(CAST(REGEXP_EXTRACT(result, r'(\\\\d+)') AS INT64), 2) = 0 as is_even,",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 31,
              "context": "29:         # 需要分析的字段\n30:         self.target_fields = {\n31:             'score_ledger': ['result_digits', 'source'],\n32:             'draws_14w_dedup_v': ['ts_utc', 'legacy_format', 'data_source'],\n33:             'p_size_clean_merged_dedup_v': ['model_version', 'raw_features', 'processing_time']",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 190,
              "context": "188:                 \n189:                 # 分析可能的依赖关系\n190:                 if field == 'result_digits':\n191:                     deps['upstream_sources'] = ['API响应中的numbers字段']\n192:                     deps['downstream_targets'] = ['可能用于结果验证']",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "fix_timestamps_client.py",
              "line": 46,
              "context": "44:         timestamp,\n45:         CAST(REGEXP_EXTRACT(result, r'(\\\\d+)') AS INT64) as result_sum,\n46:         result as result_digits,\n47:         timestamp as created_at,\n48:         MOD(CAST(REGEXP_EXTRACT(result, r'(\\\\d+)') AS INT64), 2) = 0 as is_even,",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_database_optimizer.py",
              "line": 36,
              "context": "34:             'fields': [\n35:                 {'name': 'id', 'type': 'STRING', 'nullable': False, 'usage': 'high'},\n36:                 {'name': 'result_digits', 'type': 'REPEATED INTEGER', 'nullable': True, 'usage': 'low'},\n37:                 {'name': 'numbers', 'type': 'REPEATED INTEGER', 'nullable': True, 'usage': 'high'},\n38:                 {'name': 'curtime', 'type': 'STRING', 'nullable': True, 'usage': 'low'},",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_database_optimizer.py",
              "line": 49,
              "context": "47:         \"\"\"测试冗余字段检测\"\"\"\n48:         # 测试已知冗余字段\n49:         assert optimizer._is_redundant_field('result_digits')\n50:         assert optimizer._is_redundant_field('ts_utc')\n51:         ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_database_optimizer.py",
              "line": 352,
              "context": "350:         # 测试已知冗余字段对\n351:         redundant_pairs = [\n352:             ('result_digits', 'numbers'),\n353:             ('ts_utc', 'timestamp'),\n354:             ('data_source', 'source')",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_field_usage_analysis.py",
              "line": 77,
              "context": "75:     def test_redundant_field_detection(self, analyzer):\n76:         \"\"\"测试冗余字段检测\"\"\"\n77:         # 测试result_digits与numbers字段重复\n78:         internal_fields = analyzer.internal_fields\n79:         ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_field_usage_analysis.py",
              "line": 81,
              "context": "79:         \n80:         # 检查是否存在冗余字段定义\n81:         assert 'result_digits' in internal_fields\n82:         assert 'numbers' in internal_fields\n83:         ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_field_usage_analysis.py",
              "line": 85,
              "context": "83:         \n84:         # 验证字段描述表明它们是重复的\n85:         assert '与numbers相同' in internal_fields['result_digits'] or \\\n86:                'numbers' in internal_fields['result_digits'].lower()\n87:     ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_field_usage_analysis.py",
              "line": 86,
              "context": "84:         # 验证字段描述表明它们是重复的\n85:         assert '与numbers相同' in internal_fields['result_digits'] or \\\n86:                'numbers' in internal_fields['result_digits'].lower()\n87:     \n88:     def test_unused_field_identification(self, analyzer, sample_realtime_data):",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_field_usage_analysis.py",
              "line": 131,
              "context": "129:                 'numbers': [1, 2, 3],  # 转换为整数\n130:                 'result_sum': 6,       # 计算得出\n131:                 'result_digits': [1, 2, 3]  # 冗余字段\n132:             }]\n133:             ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_field_usage_analysis.py",
              "line": 177,
              "context": "175:             assert 'missing_optimizations' in efficiency_issues\n176:             \n177:             # 应该检测到result_digits冗余\n178:             redundant_fields = efficiency_issues['redundant_fields']\n179:             assert any('result_digits' in issue for issue in redundant_fields)",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_field_usage_analysis.py",
              "line": 179,
              "context": "177:             # 应该检测到result_digits冗余\n178:             redundant_fields = efficiency_issues['redundant_fields']\n179:             assert any('result_digits' in issue for issue in redundant_fields)\n180:     \n181:     def test_threading_analysis(self, analyzer):",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_field_usage_analysis.py",
              "line": 281,
              "context": "279:         analyzer = FieldUsageAnalyzer()\n280:         \n281:         # result_digits和numbers字段功能重复\n282:         internal_fields = analyzer.internal_fields\n283:         ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_field_usage_analysis.py",
              "line": 285,
              "context": "283:         \n284:         # 验证冗余字段存在\n285:         assert 'result_digits' in internal_fields\n286:         assert 'numbers' in internal_fields\n287:         ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_field_usage_analysis.py",
              "line": 289,
              "context": "287:         \n288:         # 验证描述表明冗余\n289:         digits_desc = internal_fields['result_digits']\n290:         numbers_desc = internal_fields['numbers']\n291:         ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "test_field_usage_analysis.py",
              "line": 292,
              "context": "290:         numbers_desc = internal_fields['numbers']\n291:         \n292:         # result_digits应该被标记为与numbers相同或类似\n293:         assert 'numbers' in digits_desc.lower() or '相同' in digits_desc\n294: ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/pc28_upstream_api.py",
              "line": 195,
              "context": "193:                         'numbers': current_data.get('number', []),\n194:                         'result_sum': None,\n195:                         'result_digits': None,\n196:                         'source': 'upstream_api',\n197:                         'created_at': datetime.now(timezone.utc).isoformat()",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/pc28_upstream_api.py",
              "line": 205,
              "context": "203:                         numbers_int = [int(x) for x in parsed_item['numbers'] if str(x).isdigit()]\n204:                         parsed_item['result_sum'] = sum(numbers_int)\n205:                         parsed_item['result_digits'] = numbers_int\n206:                         parsed_item['numbers'] = numbers_int\n207:                     ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/pc28_upstream_api.py",
              "line": 218,
              "context": "216:                         'numbers': item.get('number', []),\n217:                         'result_sum': None,\n218:                         'result_digits': None,\n219:                         'source': 'upstream_api',\n220:                         'created_at': datetime.now(timezone.utc).isoformat()",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/pc28_upstream_api.py",
              "line": 228,
              "context": "226:                         numbers_int = [int(x) for x in parsed_item['numbers'] if str(x).isdigit()]\n227:                         parsed_item['result_sum'] = sum(numbers_int)\n228:                         parsed_item['result_digits'] = numbers_int\n229:                         parsed_item['numbers'] = numbers_int\n230:                     ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 51,
              "context": "49:     result_numbers: List[int]\n50:     result_sum: int\n51:     result_digits: List[int]\n52:     \n53:     # 扩展字段（最大化利用API）",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 187,
              "context": "185:                         result_numbers TEXT,\n186:                         result_sum INTEGER,\n187:                         result_digits TEXT,\n188:                         current_time TEXT,\n189:                         short_issue TEXT,",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 368,
              "context": "366:                 result_numbers=base_data.get('result_numbers', []),\n367:                 result_sum=base_data.get('result_sum', 0),\n368:                 result_digits=base_data.get('result_digits', []),\n369:                 \n370:                 # 扩展字段（最大化利用API）",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 368,
              "context": "366:                 result_numbers=base_data.get('result_numbers', []),\n367:                 result_sum=base_data.get('result_sum', 0),\n368:                 result_digits=base_data.get('result_digits', []),\n369:                 \n370:                 # 扩展字段（最大化利用API）",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 497,
              "context": "495:                 cursor.execute(\"\"\"\n496:                     INSERT OR REPLACE INTO realtime_cache \n497:                     (draw_id, timestamp, date, result_numbers, result_sum, result_digits,\n498:                      current_time, short_issue, award_time, next_issue, next_time,\n499:                      big_small, odd_even, dragon_tiger, data_source, fetch_timestamp,",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 505,
              "context": "503:                     draw_data.draw_id, draw_data.timestamp, draw_data.date,\n504:                     json.dumps(draw_data.result_numbers), draw_data.result_sum,\n505:                     json.dumps(draw_data.result_digits), draw_data.current_time,\n506:                     draw_data.short_issue, draw_data.award_time, draw_data.next_issue,\n507:                     draw_data.next_time, draw_data.big_small, draw_data.odd_even,",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 561,
              "context": "559:                         result_numbers=json.loads(row[3]) if row[3] else [],\n560:                         result_sum=row[4] or 0,\n561:                         result_digits=json.loads(row[5]) if row[5] else []\n562:                     )\n563:         ",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 822,
              "context": "820:                 'result_numbers': enhanced_data.result_numbers,\n821:                 'result_sum': enhanced_data.result_sum,\n822:                 'result_digits': enhanced_data.result_digits,\n823:                 'validation_status': enhanced_data.validation_status\n824:             }",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 822,
              "context": "820:                 'result_numbers': enhanced_data.result_numbers,\n821:                 'result_sum': enhanced_data.result_sum,\n822:                 'result_digits': enhanced_data.result_digits,\n823:                 'validation_status': enhanced_data.validation_status\n824:             }",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 845,
              "context": "843:                 'result_numbers': cached_data.result_numbers,\n844:                 'result_sum': cached_data.result_sum,\n845:                 'result_digits': cached_data.result_digits,\n846:                 'validation_status': cached_data.validation_status\n847:             }",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 845,
              "context": "843:                 'result_numbers': cached_data.result_numbers,\n844:                 'result_sum': cached_data.result_sum,\n845:                 'result_digits': cached_data.result_digits,\n846:                 'validation_status': cached_data.validation_status\n847:             }",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/realtime_lottery_service.py",
              "line": 178,
              "context": "176:                 'timestamp': timestamp_utc,\n177:                 'result_sum': int(draw_data.get('result_sum', 0)),\n178:                 'result_digits': json.dumps(draw_data.get('numbers', [])),\n179:                 'created_at': datetime.now(timezone.utc).isoformat(),\n180:                 'source': 'upstream_api_realtime'",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter_client.py",
              "line": 303,
              "context": "301:             draw_id as long_issue,\n302:             timestamp as kjtime,\n303:             result_digits,\n304:             result_sum\n305:         FROM `{self.bq.project}.{self.bq.ds_draw}.draws_14w_dedup_v`",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "python/history_backfill_service.py",
              "line": 258,
              "context": "256:                     'timestamp': timestamp_utc,\n257:                     'result_sum': int(data.get('result_sum', 0)),\n258:                     'result_digits': json.dumps(data.get('numbers', [])),\n259:                     'created_at': datetime.now(timezone.utc).isoformat(),\n260:                     'source': 'upstream_api_history'",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            },
            {
              "file": "CHANGESETS/BACKUPS/fix_draws_14w_dedup_v.sql",
              "line": 6,
              "context": "4:   timestamp,\n5:   sum as result_sum,\n6:   CONCAT(CAST(a AS STRING), ',', CAST(b AS STRING), ',', CAST(c AS STRING)) AS result_digits\n7: FROM `wprojectl.pc28.draws_14w`\n8: WHERE timestamp >= TIMESTAMP_SUB(TIMESTAMP('2024-12-19 17:50:00'), INTERVAL 14*7*24 HOUR);",
              "pattern": "['\\\"]?result_digits['\\\"]?"
            }
          ],
          "usage_patterns": [],
          "last_used": null
        },
        "source": {
          "references": [
            {
              "file": "deploy_ops_system.py",
              "line": 34,
              "context": "32:                 {\n33:                     'name': 'pc28-ops-monitor',\n34:                     'source': 'monitoring_dashboard.py',\n35:                     'entry_point': 'monitor_handler',\n36:                     'runtime': 'python39',",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 43,
              "context": "41:                 {\n42:                     'name': 'pc28-data-quality',\n43:                     'source': 'data_quality_checker.py',\n44:                     'entry_point': 'quality_check_handler',\n45:                     'runtime': 'python39',",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 53,
              "context": "51:                 {\n52:                     'name': 'pc28-alert-system',\n53:                     'source': 'alert_notification_system.py',\n54:                     'entry_point': 'alert_handler',\n55:                     'runtime': 'python39',",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 317,
              "context": "315:         \"\"\"为Cloud Function创建处理函数\"\"\"\n316:         function_name = function_config['name']\n317:         source_file = function_config['source']\n318:         entry_point = function_config['entry_point']\n319:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 317,
              "context": "315:         \"\"\"为Cloud Function创建处理函数\"\"\"\n316:         function_name = function_config['name']\n317:         source_file = function_config['source']\n318:         entry_point = function_config['entry_point']\n319:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 328,
              "context": "326: # 导入源模块\n327: try:\n328:     from {source_file.replace('.py', '')} import *\n329: except ImportError as e:\n330:     logging.error(f\"导入模块失败: {{e}}\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 394,
              "context": "392:     title = data.get('title', '系统告警')\n393:     message = data.get('message', '检测到系统异常')\n394:     source = data.get('source', 'cloud_function')\n395:     \n396:     alert_system.create_alert(",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 394,
              "context": "392:     title = data.get('title', '系统告警')\n393:     message = data.get('message', '检测到系统异常')\n394:     source = data.get('source', 'cloud_function')\n395:     \n396:     alert_system.create_alert(",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 401,
              "context": "399:         title=title,\n400:         message=message,\n401:         source=source,\n402:         metadata=data.get('metadata', {{}})\n403:     )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 401,
              "context": "399:         title=title,\n400:         message=message,\n401:         source=source,\n402:         metadata=data.get('metadata', {{}})\n403:     )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 417,
              "context": "415:         \n416:         # 复制源文件\n417:         source_path = os.path.join(self.deployment_dir, source_file)\n418:         if os.path.exists(source_path):\n419:             import shutil",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 417,
              "context": "415:         \n416:         # 复制源文件\n417:         source_path = os.path.join(self.deployment_dir, source_file)\n418:         if os.path.exists(source_path):\n419:             import shutil",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 418,
              "context": "416:         # 复制源文件\n417:         source_path = os.path.join(self.deployment_dir, source_file)\n418:         if os.path.exists(source_path):\n419:             import shutil\n420:             shutil.copy2(source_path, function_dir)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 420,
              "context": "418:         if os.path.exists(source_path):\n419:             import shutil\n420:             shutil.copy2(source_path, function_dir)\n421:         \n422:         # 复制requirements.txt",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 451,
              "context": "449:                 deploy_cmd = [\n450:                     'gcloud', 'functions', 'deploy', function_name,\n451:                     f'--source={function_dir}',\n452:                     f'--entry-point={function_config[\"entry_point\"]}',\n453:                     f'--runtime={function_config[\"runtime\"]}',",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 546,
              "context": "544:                                 \"timeSeriesQuery\": {\n545:                                     \"timeSeriesFilter\": {\n546:                                         \"filter\": 'resource.type=\"cloud_function\"',\n547:                                         \"aggregation\": {\n548:                                             \"alignmentPeriod\": \"60s\",",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "deploy_ops_system.py",
              "line": 566,
              "context": "564:                                     \"timeSeriesQuery\": {\n565:                                         \"timeSeriesFilter\": {\n566:                                             \"filter\": 'resource.type=\"cloud_function\"',\n567:                                             \"aggregation\": {\n568:                                                 \"alignmentPeriod\": \"60s\",",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 33,
              "context": "31:     description: str\n32:     evidence: Dict[str, Any]\n33:     source_detector: str\n34:     recommendation: str\n35: ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 166,
              "context": "164:                 description=f\"检测到所有数字相同: {numbers[0]}\",\n165:                 evidence={\"pattern\": \"consecutive_identical\", \"numbers\": numbers},\n166:                 source_detector=\"ai_pattern_detector\",\n167:                 recommendation=\"立即拒绝，典型的AI生成模式\"\n168:             ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 180,
              "context": "178:                     description=f\"检测到等差数列，公差: {differences[0]}\",\n179:                     evidence={\"pattern\": \"arithmetic_sequence\", \"numbers\": numbers, \"difference\": differences[0]},\n180:                     source_detector=\"ai_pattern_detector\",\n181:                     recommendation=\"强烈建议拒绝，AI常生成规律数列\"\n182:                 ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 192,
              "context": "190:                 description=\"检测到对称数字模式\",\n191:                 evidence={\"pattern\": \"symmetric\", \"numbers\": numbers},\n192:                 source_detector=\"ai_pattern_detector\",\n193:                 recommendation=\"建议人工审核，对称模式较少自然出现\"\n194:             ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 205,
              "context": "203:                 description=f\"检测到整数偏好: {round_count}/{len(numbers)} 个数字是5的倍数\",\n204:                 evidence={\"pattern\": \"round_numbers\", \"numbers\": numbers, \"round_count\": round_count},\n205:                 source_detector=\"ai_pattern_detector\",\n206:                 recommendation=\"注意监控，AI可能偏好整数\"\n207:             ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 227,
              "context": "225:                     description=f\"时间戳来自未来: {timestamp}\",\n226:                     evidence={\"timestamp\": timestamp, \"current_time\": current_time.isoformat()},\n227:                     source_detector=\"temporal_detector\",\n228:                     recommendation=\"立即拒绝，时间戳不可能来自未来\"\n229:                 ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 239,
              "context": "237:                     description=f\"时间戳过于古老: {timestamp}\",\n238:                     evidence={\"timestamp\": timestamp, \"age_days\": (current_time - record_time).days},\n239:                     source_detector=\"temporal_detector\",\n240:                     recommendation=\"检查数据来源，可能是历史数据回填\"\n241:                 ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 250,
              "context": "248:                 description=f\"无效时间戳格式: {timestamp}\",\n249:                 evidence={\"timestamp\": timestamp, \"error\": str(e)},\n250:                 source_detector=\"temporal_detector\",\n251:                 recommendation=\"拒绝，时间戳格式错误\"\n252:             ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 257,
              "context": "255:     \n256:     def integrate_detector_results(self, numbers: List[int], record_id: str, \n257:                                  timestamp: str, source: str) -> List[ContaminationThreat]:\n258:         \"\"\"整合各检测器结果\"\"\"\n259:         all_threats = []",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 275,
              "context": "273:                 \"numbers\": numbers,\n274:                 \"timestamp\": timestamp,\n275:                 \"source\": source\n276:             })\n277:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 275,
              "context": "273:                 \"numbers\": numbers,\n274:                 \"timestamp\": timestamp,\n275:                 \"source\": source\n276:             })\n277:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 285,
              "context": "283:                     description=f\"检测到重复数据，相似度: {dedup_result.similarity_score:.2f}\",\n284:                     evidence={\"similar_records\": dedup_result.similar_records},\n285:                     source_detector=\"deduplication_system\",\n286:                     recommendation=\"拒绝重复数据，避免数据污染\"\n287:                 ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 295,
              "context": "293:                 description=f\"去重检测失败: {str(e)}\",\n294:                 evidence={\"error\": str(e)},\n295:                 source_detector=\"deduplication_system\",\n296:                 recommendation=\"人工检查去重系统\"\n297:             ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 315,
              "context": "313:                         description=f\"随机性检测失败: {issue.description}\",\n314:                         evidence={\"randomness_score\": protection_result.randomness_score},\n315:                         source_detector=\"integrity_protector\",\n316:                         recommendation=\"拒绝伪随机数据\"\n317:                     ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 325,
              "context": "323:                 description=f\"随机性检测错误: {str(e)}\",\n324:                 evidence={\"error\": str(e)},\n325:                 source_detector=\"integrity_protector\",\n326:                 recommendation=\"人工检查随机性检测系统\"\n327:             ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 342,
              "context": "340:                         description=f\"和值模式异常: {pattern.description}\",\n341:                         evidence=pattern.evidence,\n342:                         source_detector=\"sum_detector\",\n343:                         recommendation=\"检查和值分布异常\"\n344:                     ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 352,
              "context": "350:                 description=f\"和值检测错误: {str(e)}\",\n351:                 evidence={\"error\": str(e)},\n352:                 source_detector=\"sum_detector\",\n353:                 recommendation=\"人工检查和值检测系统\"\n354:             ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 362,
              "context": "360:                 \"timestamp\": timestamp,\n361:                 \"draw_id\": record_id,\n362:                 \"source\": source\n363:             })\n364:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 362,
              "context": "360:                 \"timestamp\": timestamp,\n361:                 \"draw_id\": record_id,\n362:                 \"source\": source\n363:             })\n364:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 373,
              "context": "371:                     description=f\"数据验证失败: {issue.description}\",\n372:                     evidence={\"field\": issue.field, \"value\": issue.value},\n373:                     source_detector=\"data_validator\",\n374:                     recommendation=\"检查数据格式和业务规则\"\n375:                 ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 383,
              "context": "381:                 description=f\"数据验证错误: {str(e)}\",\n382:                 evidence={\"error\": str(e)},\n383:                 source_detector=\"data_validator\",\n384:                 recommendation=\"人工检查数据验证系统\"\n385:             ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 439,
              "context": "437:     \n438:     def guard_data(self, numbers: List[int], record_id: str, \n439:                    timestamp: str, source: str = \"unknown\") -> GuardResult:\n440:         \"\"\"执行数据防护检查\"\"\"\n441:         # 收集所有威胁",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 442,
              "context": "440:         \"\"\"执行数据防护检查\"\"\"\n441:         # 收集所有威胁\n442:         threats = self.integrate_detector_results(numbers, record_id, timestamp, source)\n443:         \n444:         # 计算风险分数",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 464,
              "context": "462:         \n463:         # 保存结果\n464:         self.save_guard_result(result, numbers, timestamp, source)\n465:         \n466:         return result",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 469,
              "context": "467:     \n468:     def save_guard_result(self, result: GuardResult, numbers: List[int], \n469:                          timestamp: str, source: str):\n470:         \"\"\"保存防护结果\"\"\"\n471:         with sqlite3.connect(self.db_path) as conn:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 491,
              "context": "489:                     \"description\": t.description,\n490:                     \"evidence\": t.evidence,\n491:                     \"source_detector\": t.source_detector,\n492:                     \"recommendation\": t.recommendation\n493:                 } for t in result.threats]),",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 491,
              "context": "489:                     \"description\": t.description,\n490:                     \"evidence\": t.evidence,\n491:                     \"source_detector\": t.source_detector,\n492:                     \"recommendation\": t.recommendation\n493:                 } for t in result.threats]),",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 497,
              "context": "495:                     \"numbers\": numbers,\n496:                     \"timestamp\": timestamp,\n497:                     \"source\": source\n498:                 }),\n499:                 result.timestamp",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 497,
              "context": "495:                     \"numbers\": numbers,\n496:                     \"timestamp\": timestamp,\n497:                     \"source\": source\n498:                 }),\n499:                 result.timestamp",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 579,
              "context": "577:             \"record_id\": \"real_001\",\n578:             \"timestamp\": \"2024-12-01T10:00:00Z\",\n579:             \"source\": \"official_api\",\n580:             \"description\": \"真实开奖数据\"\n581:         },",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 586,
              "context": "584:             \"record_id\": \"fake_001\",\n585:             \"timestamp\": \"2024-12-01T10:05:00Z\",\n586:             \"source\": \"unknown\",\n587:             \"description\": \"AI生成 - 连续相同\"\n588:         },",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 593,
              "context": "591:             \"record_id\": \"fake_002\",\n592:             \"timestamp\": \"2024-12-01T10:10:00Z\",\n593:             \"source\": \"simulation\",\n594:             \"description\": \"AI生成 - 等差数列\"\n595:         },",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 600,
              "context": "598:             \"record_id\": \"fake_003\",\n599:             \"timestamp\": \"2024-12-01T10:15:00Z\",\n600:             \"source\": \"test\",\n601:             \"description\": \"极端异常数据\"\n602:         },",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 607,
              "context": "605:             \"record_id\": \"normal_001\",\n606:             \"timestamp\": \"2025-01-01T00:00:00Z\",  # 未来时间\n607:             \"source\": \"official_api\",\n608:             \"description\": \"时间异常数据\"\n609:         }",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 619,
              "context": "617:         print(f\"记录ID: {test_case['record_id']}\")\n618:         print(f\"时间戳: {test_case['timestamp']}\")\n619:         print(f\"数据源: {test_case['source']}\")\n620:         \n621:         # 执行防护检查",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 626,
              "context": "624:             test_case['record_id'],\n625:             test_case['timestamp'],\n626:             test_case['source']\n627:         )\n628:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_contamination_guard.py",
              "line": 641,
              "context": "639:                 print(f\"    描述: {threat.description}\")\n640:                 print(f\"    置信度: {threat.confidence:.2f}\")\n641:                 print(f\"    检测器: {threat.source_detector}\")\n642:                 print(f\"    建议: {threat.recommendation}\")\n643:         else:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "simple_ops_system.py",
              "line": 90,
              "context": "88:                 'timestamp': datetime.now().isoformat(),\n89:                 'uptime_seconds': uptime_seconds,\n90:                 'system_resources': {\n91:                     'cpu_percent': cpu_percent,\n92:                     'memory_percent': memory.percent,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "simple_ops_system.py",
              "line": 431,
              "context": "429:                     document.getElementById('health-status').innerHTML = `\n430:                         <div class=\"${statusClass}\">状态: ${status}</div>\n431:                         <div class=\"metric\"><span class=\"metric-label\">CPU:</span> ${data.system_resources?.cpu_percent || 'N/A'}%</div>\n432:                         <div class=\"metric\"><span class=\"metric-label\">内存:</span> ${data.system_resources?.memory_percent || 'N/A'}%</div>\n433:                         <div class=\"metric\"><span class=\"metric-label\">运行时间:</span> ${Math.floor((data.uptime_seconds || 0) / 60)}分钟</div>",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "simple_ops_system.py",
              "line": 432,
              "context": "430:                         <div class=\"${statusClass}\">状态: ${status}</div>\n431:                         <div class=\"metric\"><span class=\"metric-label\">CPU:</span> ${data.system_resources?.cpu_percent || 'N/A'}%</div>\n432:                         <div class=\"metric\"><span class=\"metric-label\">内存:</span> ${data.system_resources?.memory_percent || 'N/A'}%</div>\n433:                         <div class=\"metric\"><span class=\"metric-label\">运行时间:</span> ${Math.floor((data.uptime_seconds || 0) / 60)}分钟</div>\n434:                     `;",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_ops_system.py",
              "line": 270,
              "context": "268:                 'data_checker_to_alert_system': True\n269:             },\n270:             'shared_resources': {\n271:                 'database_config': config.get('database', {}) != {},\n272:                 'api_config': config.get('upstream_api', {}) != {},",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "sum_pattern_detector.py",
              "line": 98,
              "context": "96:                     numbers TEXT NOT NULL,\n97:                     timestamp TEXT NOT NULL,\n98:                     source TEXT NOT NULL,\n99:                     created_at TEXT NOT NULL\n100:                 )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "sum_pattern_detector.py",
              "line": 445,
              "context": "443:             conn.commit()\n444:     \n445:     def add_historical_sum(self, draw_id: str, numbers: List[int], timestamp: str, source: str):\n446:         \"\"\"添加历史和值数据\"\"\"\n447:         sum_value = self.calculate_sum(numbers)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "sum_pattern_detector.py",
              "line": 454,
              "context": "452:             cursor.execute(\"\"\"\n453:                 INSERT OR REPLACE INTO historical_sums \n454:                 (draw_id, sum_value, numbers, timestamp, source, created_at)\n455:                 VALUES (?, ?, ?, ?, ?, ?)\n456:             \"\"\", (",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "sum_pattern_detector.py",
              "line": 461,
              "context": "459:                 json.dumps(numbers),\n460:                 timestamp,\n461:                 source,\n462:                 datetime.now().isoformat()\n463:             ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "database_performance_optimizer.py",
              "line": 255,
              "context": "253:                     numbers TEXT NOT NULL,\n254:                     timestamp TEXT NOT NULL,\n255:                     source TEXT NOT NULL,\n256:                     data_hash TEXT NOT NULL,\n257:                     created_at TEXT NOT NULL,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "database_performance_optimizer.py",
              "line": 390,
              "context": "388:                 json.dumps(data.get('numbers', [])),\n389:                 data.get('timestamp'),\n390:                 data.get('source', 'unknown'),\n391:                 data_hash,\n392:                 datetime.now().isoformat(),",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "database_performance_optimizer.py",
              "line": 398,
              "context": "396:         cursor.executemany(\"\"\"\n397:             INSERT OR REPLACE INTO lottery_data \n398:             (draw_id, numbers, timestamp, source, data_hash, created_at, updated_at)\n399:             VALUES (?, ?, ?, ?, ?, ?, ?)\n400:         \"\"\", values)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "database_performance_optimizer.py",
              "line": 567,
              "context": "565:             \"numbers\": [1, 3, 7, 2, 8],\n566:             \"timestamp\": f\"2024-12-01T10:{i:02d}:00Z\",\n567:             \"source\": \"performance_test\"\n568:         }\n569:         for i in range(100)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 68,
              "context": "66:                     {'name': 'numbers', 'type': 'REPEATED INTEGER', 'nullable': True, 'usage': 'high'},\n67:                     {'name': 'result_sum', 'type': 'INTEGER', 'nullable': True, 'usage': 'high'},\n68:                     {'name': 'source', 'type': 'STRING', 'nullable': True, 'usage': 'low'},  # 很少使用\n69:                     {'name': 'created_at', 'type': 'TIMESTAMP', 'nullable': True, 'usage': 'medium'}\n70:                 ],",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 83,
              "context": "81:                     {'name': 'ts_utc', 'type': 'TIMESTAMP', 'nullable': True, 'usage': 'medium'},  # 与timestamp重复\n82:                     {'name': 'legacy_format', 'type': 'STRING', 'nullable': True, 'usage': 'low'},  # 遗留字段\n83:                     {'name': 'data_source', 'type': 'STRING', 'nullable': True, 'usage': 'low'},\n84:                     {'name': 'validation_status', 'type': 'STRING', 'nullable': True, 'usage': 'medium'}\n85:                 ],",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 214,
              "context": "212:             'result_digits': 'numbers',  # result_digits与numbers重复\n213:             'ts_utc': 'timestamp',       # ts_utc与timestamp重复\n214:             'data_source': 'source',     # 类似功能字段\n215:         }\n216:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 214,
              "context": "212:             'result_digits': 'numbers',  # result_digits与numbers重复\n213:             'ts_utc': 'timestamp',       # ts_utc与timestamp重复\n214:             'data_source': 'source',     # 类似功能字段\n215:         }\n216:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 223,
              "context": "221:         similar_patterns = [\n222:             ('timestamp', 'ts_utc', 'created_at'),\n223:             ('source', 'data_source'),\n224:             ('numbers', 'result_digits')\n225:         ]",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 223,
              "context": "221:         similar_patterns = [\n222:             ('timestamp', 'ts_utc', 'created_at'),\n223:             ('source', 'data_source'),\n224:             ('numbers', 'result_digits')\n225:         ]",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite_manager.py",
              "line": 410,
              "context": "408:     \"\"\"测试覆盖率分析器\"\"\"\n409:     \n410:     def __init__(self, source_dirs: List[str]):\n411:         self.source_dirs = source_dirs\n412:         self.cov = None",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite_manager.py",
              "line": 411,
              "context": "409:     \n410:     def __init__(self, source_dirs: List[str]):\n411:         self.source_dirs = source_dirs\n412:         self.cov = None\n413:     ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite_manager.py",
              "line": 411,
              "context": "409:     \n410:     def __init__(self, source_dirs: List[str]):\n411:         self.source_dirs = source_dirs\n412:         self.cov = None\n413:     ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite_manager.py",
              "line": 416,
              "context": "414:     def start_coverage(self):\n415:         \"\"\"开始覆盖率收集\"\"\"\n416:         self.cov = coverage.Coverage(source=self.source_dirs)\n417:         self.cov.start()\n418:     ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite_manager.py",
              "line": 416,
              "context": "414:     def start_coverage(self):\n415:         \"\"\"开始覆盖率收集\"\"\"\n416:         self.cov = coverage.Coverage(source=self.source_dirs)\n417:         self.cov.start()\n418:     ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "component_updater.py",
              "line": 713,
              "context": "711:     def _create_backup(self, component: ComponentInfo) -> str:\n712:         \"\"\"创建备份\"\"\"\n713:         source_path = Path(component.file_path)\n714:         timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n715:         backup_filename = f\"{component.component_id}_{timestamp}{source_path.suffix}\"",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "component_updater.py",
              "line": 715,
              "context": "713:         source_path = Path(component.file_path)\n714:         timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n715:         backup_filename = f\"{component.component_id}_{timestamp}{source_path.suffix}\"\n716:         backup_path = self.backup_dir / backup_filename\n717:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "component_updater.py",
              "line": 718,
              "context": "716:         backup_path = self.backup_dir / backup_filename\n717:         \n718:         shutil.copy2(source_path, backup_path)\n719:         logger.info(f\"已创建备份: {backup_path}\")\n720:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "field_cleanup_sql_generator.py",
              "line": 40,
              "context": "38:                 {\n39:                     'table': 'score_ledger',\n40:                     'field': 'source',\n41:                     'reason': '使用率极低',\n42:                     'risk': 'medium',",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "field_cleanup_sql_generator.py",
              "line": 65,
              "context": "63:                 {\n64:                     'table': 'draws_14w_dedup_v',\n65:                     'field': 'data_source',\n66:                     'reason': '使用率极低',\n67:                     'risk': 'low',",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "field_usage_analysis.py",
              "line": 73,
              "context": "71:             'result_sum': '开奖号码总和（计算得出）',\n72:             'result_digits': '开奖号码数组（与numbers相同）',\n73:             'source': '数据来源标识',\n74:             'created_at': '数据创建时间'\n75:         }",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notifier.py",
              "line": 25,
              "context": "23:     \n24:     def send_alert(self, title: str, message: str, severity: str = 'info', \n25:                    category: str = 'system', source: str = 'data_quality_checker',\n26:                    metadata: Dict[str, Any] = None) -> bool:\n27:         \"\"\"发送告警",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notifier.py",
              "line": 34,
              "context": "32:             severity: 严重程度 ('info', 'warning', 'critical', 'emergency')\n33:             category: 告警类别 ('system', 'data_quality', 'api', 'performance', 'security')\n34:             source: 告警来源\n35:             metadata: 额外元数据\n36:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notifier.py",
              "line": 67,
              "context": "65:                 title=title,\n66:                 message=message,\n67:                 source=source,\n68:                 metadata=metadata\n69:             )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notifier.py",
              "line": 67,
              "context": "65:                 title=title,\n66:                 message=message,\n67:                 source=source,\n68:                 metadata=metadata\n69:             )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 48,
              "context": "46:     title: str\n47:     message: str\n48:     source: str\n49:     timestamp: datetime\n50:     metadata: Dict[str, Any] = None",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 133,
              "context": "131:             <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px;\">\n132:                 <p><strong>告警时间:</strong> {alert.timestamp.strftime('%Y-%m-%d %H:%M:%S')}</p>\n133:                 <p><strong>告警来源:</strong> {alert.source}</p>\n134:                 <p><strong>告警类型:</strong> {alert.type.value}</p>\n135:             </div>",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 225,
              "context": "223:                         {\n224:                             'title': '告警来源',\n225:                             'value': alert.source,\n226:                             'short': True\n227:                         },",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 373,
              "context": "371:     \n372:     def create_alert(self, level: AlertLevel, alert_type: AlertType, title: str, \n373:                     message: str, source: str, metadata: Dict[str, Any] = None) -> Alert:\n374:         \"\"\"创建告警\"\"\"\n375:         alert = Alert(",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 376,
              "context": "374:         \"\"\"创建告警\"\"\"\n375:         alert = Alert(\n376:             id=f\"{source}_{alert_type.value}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(self.alerts)}\",\n377:             level=level,\n378:             type=alert_type,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 381,
              "context": "379:             title=title,\n380:             message=message,\n381:             source=source,\n382:             timestamp=datetime.now(),\n383:             metadata=metadata or {}",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 381,
              "context": "379:             title=title,\n380:             message=message,\n381:             source=source,\n382:             timestamp=datetime.now(),\n383:             metadata=metadata or {}",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 452,
              "context": "450:         \n451:         # 检查来源\n452:         if 'sources' in rule and alert.source not in rule['sources']:\n453:             return False\n454:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 452,
              "context": "450:         \n451:         # 检查来源\n452:         if 'sources' in rule and alert.source not in rule['sources']:\n453:             return False\n454:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 452,
              "context": "450:         \n451:         # 检查来源\n452:         if 'sources' in rule and alert.source not in rule['sources']:\n453:             return False\n454:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 539,
              "context": "537:             'by_level': {},\n538:             'by_type': {},\n539:             'by_source': {}\n540:         }\n541:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 553,
              "context": "551:         \n552:         # 按来源统计\n553:         sources = set(a.source for a in recent_alerts)\n554:         for source in sources:\n555:             count = len([a for a in recent_alerts if a.source == source])",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 553,
              "context": "551:         \n552:         # 按来源统计\n553:         sources = set(a.source for a in recent_alerts)\n554:         for source in sources:\n555:             count = len([a for a in recent_alerts if a.source == source])",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 554,
              "context": "552:         # 按来源统计\n553:         sources = set(a.source for a in recent_alerts)\n554:         for source in sources:\n555:             count = len([a for a in recent_alerts if a.source == source])\n556:             stats['by_source'][source] = count",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 554,
              "context": "552:         # 按来源统计\n553:         sources = set(a.source for a in recent_alerts)\n554:         for source in sources:\n555:             count = len([a for a in recent_alerts if a.source == source])\n556:             stats['by_source'][source] = count",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 555,
              "context": "553:         sources = set(a.source for a in recent_alerts)\n554:         for source in sources:\n555:             count = len([a for a in recent_alerts if a.source == source])\n556:             stats['by_source'][source] = count\n557:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 555,
              "context": "553:         sources = set(a.source for a in recent_alerts)\n554:         for source in sources:\n555:             count = len([a for a in recent_alerts if a.source == source])\n556:             stats['by_source'][source] = count\n557:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 556,
              "context": "554:         for source in sources:\n555:             count = len([a for a in recent_alerts if a.source == source])\n556:             stats['by_source'][source] = count\n557:         \n558:         return stats",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 556,
              "context": "554:         for source in sources:\n555:             count = len([a for a in recent_alerts if a.source == source])\n556:             stats['by_source'][source] = count\n557:         \n558:         return stats",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 567,
              "context": "565:             title=\"通知渠道测试\",\n566:             message=\"这是一条测试消息，用于验证通知渠道是否正常工作。\",\n567:             source=\"alert_system_test\",\n568:             metadata={'test': True}\n569:         )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 603,
              "context": "601:     parser.add_argument('--title', type=str, help='告警标题')\n602:     parser.add_argument('--message', type=str, help='告警消息')\n603:     parser.add_argument('--source', type=str, help='告警来源')\n604:     \n605:     args = parser.parse_args()",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 625,
              "context": "623:             if active_alerts:\n624:                 for alert in active_alerts:\n625:                     print(f\"[{alert.level.value.upper()}] {alert.title} - {alert.source} ({alert.timestamp.strftime('%Y-%m-%d %H:%M:%S')})\")\n626:             else:\n627:                 print(\"✅ 没有活跃告警\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 630,
              "context": "628:                 \n629:         elif args.command == 'send':\n630:             if not all([args.level, args.type, args.title, args.message, args.source]):\n631:                 print(\"❌ 发送告警需要提供: --level, --type, --title, --message, --source\")\n632:                 return",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 631,
              "context": "629:         elif args.command == 'send':\n630:             if not all([args.level, args.type, args.title, args.message, args.source]):\n631:                 print(\"❌ 发送告警需要提供: --level, --type, --title, --message, --source\")\n632:                 return\n633:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 639,
              "context": "637:                 title=args.title,\n638:                 message=args.message,\n639:                 source=args.source\n640:             )\n641:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "alert_notification_system.py",
              "line": 639,
              "context": "637:                 title=args.title,\n638:                 message=args.message,\n639:                 source=args.source\n640:             )\n641:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_api.py",
              "line": 251,
              "context": "249:         \n250:         # 尝试多个数据源\n251:         data_sources = [\n252:             (\"BigQuery历史表\", self.fetch_from_bigquery_history),\n253:             (\"外部API\", self.fetch_from_external_api),",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_api.py",
              "line": 257,
              "context": "255:         ]\n256:         \n257:         for source_name, fetch_func in data_sources:\n258:             try:\n259:                 logger.info(f\"尝试从 {source_name} 获取数据\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_api.py",
              "line": 257,
              "context": "255:         ]\n256:         \n257:         for source_name, fetch_func in data_sources:\n258:             try:\n259:                 logger.info(f\"尝试从 {source_name} 获取数据\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_api.py",
              "line": 259,
              "context": "257:         for source_name, fetch_func in data_sources:\n258:             try:\n259:                 logger.info(f\"尝试从 {source_name} 获取数据\")\n260:                 data = fetch_func(start_time, end_time)\n261:                 ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_api.py",
              "line": 263,
              "context": "261:                 \n262:                 if data:\n263:                     logger.info(f\"成功从 {source_name} 获取到 {len(data)} 条记录\")\n264:                     return data\n265:                 else:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_api.py",
              "line": 266,
              "context": "264:                     return data\n265:                 else:\n266:                     logger.warning(f\"{source_name} 未返回数据\")\n267:                     \n268:             except Exception as e:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_api.py",
              "line": 269,
              "context": "267:                     \n268:             except Exception as e:\n269:                 logger.error(f\"从 {source_name} 获取数据失败: {e}\")\n270:                 continue\n271:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "ops_manager_main.py",
              "line": 241,
              "context": "239:                     title='API服务异常',\n240:                     message=f\"API服务状态异常: {api_status.get('error', '未知错误')}\",\n241:                     source='monitoring_dashboard'\n242:                 )\n243:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "ops_manager_main.py",
              "line": 253,
              "context": "251:                     title='数据质量告警',\n252:                     message=f\"数据质量评分较低: {quality_score}%\",\n253:                     source='monitoring_dashboard'\n254:                 )\n255:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "ops_manager_main.py",
              "line": 265,
              "context": "263:                     title='系统错误率过高',\n264:                     message=f\"系统错误率: {error_rate}%\",\n265:                     source='monitoring_dashboard'\n266:                 )\n267:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "ops_manager_main.py",
              "line": 283,
              "context": "281:                     title=f\"数据质量问题: {issue.get('table', '未知表')}\",\n282:                     message=issue.get('description', '数据质量检查发现问题'),\n283:                     source='data_quality_checker',\n284:                     metadata=issue\n285:                 )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 31,
              "context": "29:         # 需要分析的字段\n30:         self.target_fields = {\n31:             'score_ledger': ['result_digits', 'source'],\n32:             'draws_14w_dedup_v': ['ts_utc', 'legacy_format', 'data_source'],\n33:             'p_size_clean_merged_dedup_v': ['model_version', 'raw_features', 'processing_time']",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 32,
              "context": "30:         self.target_fields = {\n31:             'score_ledger': ['result_digits', 'source'],\n32:             'draws_14w_dedup_v': ['ts_utc', 'legacy_format', 'data_source'],\n33:             'p_size_clean_merged_dedup_v': ['model_version', 'raw_features', 'processing_time']\n34:         }",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 183,
              "context": "181:             for field in fields:\n182:                 deps = {\n183:                     'upstream_sources': [],    # 上游数据源\n184:                     'downstream_targets': [],  # 下游使用目标\n185:                     'transformation_logic': [], # 转换逻辑",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 191,
              "context": "189:                 # 分析可能的依赖关系\n190:                 if field == 'result_digits':\n191:                     deps['upstream_sources'] = ['API响应中的numbers字段']\n192:                     deps['downstream_targets'] = ['可能用于结果验证']\n193:                     deps['transformation_logic'] = ['numbers数组的副本']",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 196,
              "context": "194:                 \n195:                 elif field == 'ts_utc':\n196:                     deps['upstream_sources'] = ['timestamp字段的UTC转换']\n197:                     deps['downstream_targets'] = ['时区相关查询']\n198:                     deps['transformation_logic'] = ['timestamp -> UTC转换']",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 201,
              "context": "199:                 \n200:                 elif field == 'raw_features':\n201:                     deps['upstream_sources'] = ['机器学习特征提取']\n202:                     deps['downstream_targets'] = ['模型调试和分析']\n203:                     deps['transformation_logic'] = ['特征工程输出']",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 206,
              "context": "204:                 \n205:                 elif field == 'legacy_format':\n206:                     deps['upstream_sources'] = ['历史数据格式']\n207:                     deps['downstream_targets'] = ['兼容性处理']\n208:                     deps['transformation_logic'] = ['格式转换逻辑']",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 211,
              "context": "209:                 \n210:                 dependencies[table][field] = deps\n211:                 print(f\"  {table}.{field}: {len(deps['upstream_sources'])} 个上游源\")\n212:         \n213:         return dependencies",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 73,
              "context": "71:     target_throughput: float\n72:     max_error_rate: float\n73:     optimization_strategy: str  # 'throughput', 'latency', 'balanced', 'resource_conservative'\n74:     priority_weight: float\n75: ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 85,
              "context": "83:     new_config: ConcurrencyConfig\n84:     performance_improvement: float\n85:     resource_efficiency: float\n86:     recommendation_reason: str\n87:     confidence_score: float",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 129,
              "context": "127:             'latency': self._optimize_for_latency,\n128:             'balanced': self._optimize_balanced,\n129:             'resource_conservative': self._optimize_resource_conservative\n130:         }\n131:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 129,
              "context": "127:             'latency': self._optimize_for_latency,\n128:             'balanced': self._optimize_balanced,\n129:             'resource_conservative': self._optimize_resource_conservative\n130:         }\n131:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 207,
              "context": "205:                     new_config TEXT NOT NULL,\n206:                     performance_improvement REAL NOT NULL,\n207:                     resource_efficiency REAL NOT NULL,\n208:                     recommendation_reason TEXT NOT NULL,\n209:                     confidence_score REAL NOT NULL,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 264,
              "context": "262:             ),\n263:             PerformanceProfile(\n264:                 profile_name=\"resource_conservative\",\n265:                 target_cpu_usage=60.0,\n266:                 target_memory_usage=50.0,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 270,
              "context": "268:                 target_throughput=300.0,\n269:                 max_error_rate=0.3,\n270:                 optimization_strategy=\"resource_conservative\",\n271:                 priority_weight=0.8\n272:             )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 296,
              "context": "294:                 condition=\"time_based\",\n295:                 trigger_condition={\"hours\": [0, 1, 2, 3, 4, 5, 6]},\n296:                 target_config={\"profile\": \"resource_conservative\", \"max_workers_multiplier\": 0.5},\n297:                 priority=2,\n298:                 enabled=True,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 306,
              "context": "304:                 condition=\"load_based\",\n305:                 trigger_condition={\"cpu_usage\": {\"threshold\": 90.0, \"duration\": 300}},\n306:                 target_config={\"profile\": \"resource_conservative\", \"max_workers_multiplier\": 0.7},\n307:                 priority=0,\n308:                 enabled=True,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 630,
              "context": "628:         return new_config\n629:     \n630:     def _optimize_resource_conservative(self, current_config: ConcurrencyConfig, metrics: SystemMetrics, profile: PerformanceProfile) -> ConcurrencyConfig:\n631:         \"\"\"资源保守优化策略\"\"\"\n632:         new_config = ConcurrencyConfig(",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 694,
              "context": "692:         \n693:         # 计算资源效率\n694:         resource_efficiency = self._calculate_resource_efficiency(new_config, metrics)\n695:         \n696:         # 生成推荐原因",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 694,
              "context": "692:         \n693:         # 计算资源效率\n694:         resource_efficiency = self._calculate_resource_efficiency(new_config, metrics)\n695:         \n696:         # 生成推荐原因",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 709,
              "context": "707:             new_config=new_config,\n708:             performance_improvement=performance_improvement,\n709:             resource_efficiency=resource_efficiency,\n710:             recommendation_reason=recommendation_reason,\n711:             confidence_score=confidence_score,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 709,
              "context": "707:             new_config=new_config,\n708:             performance_improvement=performance_improvement,\n709:             resource_efficiency=resource_efficiency,\n710:             recommendation_reason=recommendation_reason,\n711:             confidence_score=confidence_score,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 736,
              "context": "734:         return max(min(improvement, 1.0), -0.5)\n735:     \n736:     def _calculate_resource_efficiency(self, config: ConcurrencyConfig, metrics: SystemMetrics) -> float:\n737:         \"\"\"计算资源效率\"\"\"\n738:         # 基于当前资源使用情况计算效率",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 814,
              "context": "812:                 INSERT INTO optimization_results \n813:                 (optimization_id, timestamp, service_name, old_config, new_config,\n814:                  performance_improvement, resource_efficiency, recommendation_reason,\n815:                  confidence_score, applied)\n816:                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 820,
              "context": "818:                 result.optimization_id, result.timestamp, result.service_name,\n819:                 json.dumps(old_config_dict), json.dumps(new_config_dict),\n820:                 result.performance_improvement, result.resource_efficiency,\n821:                 result.recommendation_reason, result.confidence_score, result.applied\n822:             ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 895,
              "context": "893:                         logger.info(f\"服务 {service_name} 优化建议:\")\n894:                         logger.info(f\"  - 性能改进预期: {optimization_result.performance_improvement:.2%}\")\n895:                         logger.info(f\"  - 资源效率: {optimization_result.resource_efficiency:.2%}\")\n896:                         logger.info(f\"  - 置信度: {optimization_result.confidence_score:.2%}\")\n897:                         logger.info(f\"  - 原因: {optimization_result.recommendation_reason}\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 920,
              "context": "918:                     results = conn.execute(\"\"\"\n919:                         SELECT optimization_id, timestamp, service_name, performance_improvement,\n920:                                resource_efficiency, recommendation_reason, confidence_score, applied\n921:                         FROM optimization_results \n922:                         WHERE service_name = ?",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 929,
              "context": "927:                     results = conn.execute(\"\"\"\n928:                         SELECT optimization_id, timestamp, service_name, performance_improvement,\n929:                                resource_efficiency, recommendation_reason, confidence_score, applied\n930:                         FROM optimization_results \n931:                         ORDER BY timestamp DESC ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 941,
              "context": "939:                         \"service_name\": result[2],\n940:                         \"performance_improvement\": result[3],\n941:                         \"resource_efficiency\": result[4],\n942:                         \"recommendation_reason\": result[5],\n943:                         \"confidence_score\": result[6],",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "concurrency_optimizer.py",
              "line": 1001,
              "context": "999:             print(f\"  - 新队列大小: {optimization_result.new_config.queue_size}\")\n1000:             print(f\"  - 性能改进预期: {optimization_result.performance_improvement:.2%}\")\n1001:             print(f\"  - 资源效率: {optimization_result.resource_efficiency:.2%}\")\n1002:             print(f\"  - 置信度: {optimization_result.confidence_score:.2%}\")\n1003:             print(f\"  - 推荐原因: {optimization_result.recommendation_reason}\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "real_api_data_system.py",
              "line": 68,
              "context": "66:     next_draw_time: Optional[str] = None\n67:     countdown_seconds: Optional[int] = None\n68:     source: str = \"real_api\"\n69:     raw_data: Optional[Dict] = None\n70: ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "real_api_data_system.py",
              "line": 134,
              "context": "132:                         next_draw_time TEXT,\n133:                         countdown_seconds INTEGER,\n134:                         source TEXT DEFAULT 'real_api',\n135:                         raw_data TEXT,\n136:                         created_at DATETIME DEFAULT CURRENT_TIMESTAMP,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "real_api_data_system.py",
              "line": 447,
              "context": "445:                     (draw_id, issue, numbers, sum_value, big_small, odd_even, dragon_tiger,\n446:                      timestamp, server_time, next_draw_id, next_draw_time, countdown_seconds,\n447:                      source, raw_data, updated_at)\n448:                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n449:                 \"\"\", (",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "real_api_data_system.py",
              "line": 462,
              "context": "460:                     record.next_draw_time,\n461:                     record.countdown_seconds,\n462:                     record.source,\n463:                     json.dumps(record.raw_data) if record.raw_data else None,\n464:                     datetime.now()",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 84,
              "context": "82:                     timestamp TEXT NOT NULL,\n83:                     record_hash TEXT NOT NULL,\n84:                     data_source TEXT NOT NULL,\n85:                     randomness_score REAL NOT NULL,\n86:                 is_blocked BOOLEAN NOT NULL,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 443,
              "context": "441:         return len(critical_issues) == 0, randomness_tests, integrity_issues\n442:     \n443:     def protect_batch_write(self, records: List[Dict[str, Any]], data_source: str = \"unknown\") -> ProtectionReport:\n444:         \"\"\"保护批量写入\"\"\"\n445:         timestamp = datetime.now().isoformat()",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 471,
              "context": "469:                 cursor.execute(\"\"\"\n470:                     INSERT INTO protection_records \n471:                     (timestamp, record_hash, data_source, randomness_score, is_blocked, block_reason, created_at)\n472:                     VALUES (?, ?, ?, ?, ?, ?, ?)\n473:                 \"\"\", (timestamp, record_hash, data_source, randomness_score, not is_valid, block_reason, datetime.now().isoformat()))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 473,
              "context": "471:                     (timestamp, record_hash, data_source, randomness_score, is_blocked, block_reason, created_at)\n472:                     VALUES (?, ?, ?, ?, ?, ?, ?)\n473:                 \"\"\", (timestamp, record_hash, data_source, randomness_score, not is_valid, block_reason, datetime.now().isoformat()))\n474:                 \n475:                 # 记录随机性测试结果",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 533,
              "context": "531:             # 按数据源统计\n532:             cursor.execute(\"\"\"\n533:                 SELECT data_source, COUNT(*) as total, \n534:                        SUM(CASE WHEN is_blocked = 1 THEN 1 ELSE 0 END) as blocked\n535:                 FROM protection_records ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 536,
              "context": "534:                        SUM(CASE WHEN is_blocked = 1 THEN 1 ELSE 0 END) as blocked\n535:                 FROM protection_records \n536:                 GROUP BY data_source\n537:             \"\"\")\n538:             source_stats = {row[0]: {\"total\": row[1], \"blocked\": row[2]} for row in cursor.fetchall()}",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 538,
              "context": "536:                 GROUP BY data_source\n537:             \"\"\")\n538:             source_stats = {row[0]: {\"total\": row[1], \"blocked\": row[2]} for row in cursor.fetchall()}\n539:             \n540:             # 随机性测试统计",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 561,
              "context": "559:             \"blocked_records\": blocked_records,\n560:             \"block_rate\": (blocked_records / total_records) * 100 if total_records > 0 else 0,\n561:             \"source_statistics\": source_stats,\n562:             \"test_statistics\": test_stats\n563:         }",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 561,
              "context": "559:             \"blocked_records\": blocked_records,\n560:             \"block_rate\": (blocked_records / total_records) * 100 if total_records > 0 else 0,\n561:             \"source_statistics\": source_stats,\n562:             \"test_statistics\": test_stats\n563:         }",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 575,
              "context": "573:             \"numbers\": [1, 3, 7, 2, 8],  # 相对随机\n574:             \"timestamp\": \"2024-12-01T10:00:00Z\",\n575:             \"source\": \"real_api\"\n576:         },\n577:         {",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 581,
              "context": "579:             \"numbers\": [1, 1, 1, 1, 1],  # 明显伪随机\n580:             \"timestamp\": \"2024-12-01T10:05:00Z\",\n581:             \"source\": \"suspicious\"\n582:         },\n583:         {",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 587,
              "context": "585:             \"numbers\": [1, 2, 3, 4, 5],  # 连续序列\n586:             \"timestamp\": \"2024-12-01T10:10:00Z\",\n587:             \"source\": \"test_data\"\n588:         },\n589:         {",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 593,
              "context": "591:             \"numbers\": [9, 2, 5, 1, 7],  # 相对随机\n592:             \"timestamp\": \"2024-12-01T10:15:00Z\",\n593:             \"source\": \"real_api\"\n594:         }\n595:     ]",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 533,
              "context": "531:             # 按数据源统计\n532:             cursor.execute(\"\"\"\n533:                 SELECT data_source, COUNT(*) as total, \n534:                        SUM(CASE WHEN is_blocked = 1 THEN 1 ELSE 0 END) as blocked\n535:                 FROM protection_records ",
              "pattern": "SELECT.*source"
            },
            {
              "file": "test_database_optimizer.py",
              "line": 64,
              "context": "62:         \n63:         # 测试数据源相关字段\n64:         assert optimizer._has_similar_field('source')\n65:         assert optimizer._has_similar_field('data_source')\n66:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_database_optimizer.py",
              "line": 65,
              "context": "63:         # 测试数据源相关字段\n64:         assert optimizer._has_similar_field('source')\n65:         assert optimizer._has_similar_field('data_source')\n66:         \n67:         # 测试独特字段",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_database_optimizer.py",
              "line": 354,
              "context": "352:             ('result_digits', 'numbers'),\n353:             ('ts_utc', 'timestamp'),\n354:             ('data_source', 'source')\n355:         ]\n356:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_database_optimizer.py",
              "line": 354,
              "context": "352:             ('result_digits', 'numbers'),\n353:             ('ts_utc', 'timestamp'),\n354:             ('data_source', 'source')\n355:         ]\n356:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "ops_manager.py",
              "line": 150,
              "context": "148:         \n149:         print(\"\\n数据质量:\")\n150:         for source, status in health_status.data_quality_status.items():\n151:             print(f\"  {source}: {self._get_status_emoji(status)} {status}\")\n152:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "ops_manager.py",
              "line": 151,
              "context": "149:         print(\"\\n数据质量:\")\n150:         for source, status in health_status.data_quality_status.items():\n151:             print(f\"  {source}: {self._get_status_emoji(status)} {status}\")\n152:         \n153:         if health_status.active_alerts:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "ops_manager.py",
              "line": 186,
              "context": "184:             \n185:             print(\"\\n数据质量报告:\")\n186:             for source, summary in quality_summary.items():\n187:                 status_emoji = self._get_status_emoji(summary['status'])\n188:                 print(f\"\\n{source.upper()}:\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "ops_manager.py",
              "line": 188,
              "context": "186:             for source, summary in quality_summary.items():\n187:                 status_emoji = self._get_status_emoji(summary['status'])\n188:                 print(f\"\\n{source.upper()}:\")\n189:                 print(f\"  状态: {status_emoji} {summary['status'].upper()}\")\n190:                 print(f\"  质量评分: {summary['latest_score']:.3f}\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_api.py",
              "line": 108,
              "context": "106:         health_data = health_result['data']\n107:         print(f\"✅ 系统健康状态: {health_data.get('overall_health', 'unknown')}\")\n108:         print(f\"   CPU使用率: {health_data.get('system_resources', {}).get('cpu_percent', 'N/A')}%\")\n109:         print(f\"   内存使用率: {health_data.get('system_resources', {}).get('memory_percent', 'N/A')}%\")\n110:     ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_api.py",
              "line": 109,
              "context": "107:         print(f\"✅ 系统健康状态: {health_data.get('overall_health', 'unknown')}\")\n108:         print(f\"   CPU使用率: {health_data.get('system_resources', {}).get('cpu_percent', 'N/A')}%\")\n109:         print(f\"   内存使用率: {health_data.get('system_resources', {}).get('memory_percent', 'N/A')}%\")\n110:     \n111:     # 验证数据质量检查",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "main_handler.py",
              "line": 181,
              "context": "179:         quality_report_files = [\n180:             'python/data_quality_report_20250925_045814.json',\n181:             'function-source修复/python/data_quality_report_20250925_045814.json'\n182:         ]\n183:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_protection.py",
              "line": 63,
              "context": "61:     target_date: datetime\n62:     recovery_type: str  # 'full', 'partial', 'point_in_time'\n63:     backup_sources: List[str]\n64:     estimated_records: int\n65:     estimated_time: int  # 秒",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_protection.py",
              "line": 166,
              "context": "164:                         operation_time DATETIME NOT NULL,\n165:                         recovery_type TEXT NOT NULL,\n166:                         source_backup TEXT NOT NULL,\n167:                         target_table TEXT NOT NULL,\n168:                         records_recovered INTEGER NOT NULL,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "historical_data_protection.py",
              "line": 807,
              "context": "805:                 conn.execute(\"\"\"\n806:                     INSERT INTO recovery_operations \n807:                     (operation_id, operation_time, recovery_type, source_backup,\n808:                      target_table, records_recovered, status, error_message)\n809:                     VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/pc28_upstream_api.py",
              "line": 196,
              "context": "194:                         'result_sum': None,\n195:                         'result_digits': None,\n196:                         'source': 'upstream_api',\n197:                         'created_at': datetime.now(timezone.utc).isoformat()\n198:                     }",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/pc28_upstream_api.py",
              "line": 219,
              "context": "217:                         'result_sum': None,\n218:                         'result_digits': None,\n219:                         'source': 'upstream_api',\n220:                         'created_at': datetime.now(timezone.utc).isoformat()\n221:                     }",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 74,
              "context": "72:         data_quality_healthy = True\n73:         \n74:         for source, summary in data_quality_summary.items():\n75:             data_quality_status[source] = summary['status']\n76:             if summary['status'] != 'good':",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 75,
              "context": "73:         \n74:         for source, summary in data_quality_summary.items():\n75:             data_quality_status[source] = summary['status']\n76:             if summary['status'] != 'good':\n77:                 data_quality_healthy = False",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 97,
              "context": "95:         \n96:         # 数据质量告警\n97:         for source, summary in data_quality_summary.items():\n98:             if summary['status'] == 'critical':\n99:                 active_alerts.append(f\"数据质量严重问题: {source} (评分: {summary['latest_score']:.3f})\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 99,
              "context": "97:         for source, summary in data_quality_summary.items():\n98:             if summary['status'] == 'critical':\n99:                 active_alerts.append(f\"数据质量严重问题: {source} (评分: {summary['latest_score']:.3f})\")\n100:             elif summary['status'] == 'warning':\n101:                 active_alerts.append(f\"数据质量警告: {source} (评分: {summary['latest_score']:.3f})\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 101,
              "context": "99:                 active_alerts.append(f\"数据质量严重问题: {source} (评分: {summary['latest_score']:.3f})\")\n100:             elif summary['status'] == 'warning':\n101:                 active_alerts.append(f\"数据质量警告: {source} (评分: {summary['latest_score']:.3f})\")\n102:         \n103:         # 错误率告警",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 311,
              "context": "309:         \n310:         report += \"\\n【数据质量】\\n\"\n311:         for source, summary in dashboard_data['data_quality_summary'].items():\n312:             report += f\"- {source.upper()}: {summary['status'].upper()} (评分: {summary['latest_score']:.3f})\\n\"\n313:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 312,
              "context": "310:         report += \"\\n【数据质量】\\n\"\n311:         for source, summary in dashboard_data['data_quality_summary'].items():\n312:             report += f\"- {source.upper()}: {summary['status'].upper()} (评分: {summary['latest_score']:.3f})\\n\"\n313:         \n314:         if current_status['active_alerts']:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 66,
              "context": "64:     \n65:     # 元数据\n66:     data_source: str = \"realtime\"  # 数据来源\n67:     fetch_timestamp: Optional[str] = None  # 获取时间戳\n68:     data_hash: Optional[str] = None  # 数据哈希",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 196,
              "context": "194:                         odd_even TEXT,\n195:                         dragon_tiger TEXT,\n196:                         data_source TEXT,\n197:                         fetch_timestamp TEXT,\n198:                         data_hash TEXT,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 377,
              "context": "375:                 next_time=next_data.get('next_time'),\n376:                 \n377:                 data_source=\"realtime_enhanced\"\n378:             )\n379:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 499,
              "context": "497:                     (draw_id, timestamp, date, result_numbers, result_sum, result_digits,\n498:                      current_time, short_issue, award_time, next_issue, next_time,\n499:                      big_small, odd_even, dragon_tiger, data_source, fetch_timestamp,\n500:                      data_hash, validation_status, created_at)\n501:                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 508,
              "context": "506:                     draw_data.short_issue, draw_data.award_time, draw_data.next_issue,\n507:                     draw_data.next_time, draw_data.big_small, draw_data.odd_even,\n508:                     draw_data.dragon_tiger, draw_data.data_source, draw_data.fetch_timestamp,\n509:                     draw_data.data_hash, draw_data.validation_status,\n510:                     datetime.now(timezone.utc).isoformat()",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/integration_test_suite.py",
              "line": 388,
              "context": "386:         # 测试资源管理\n387:         test_result = await self._run_single_test(\n388:             \"resource_management\",\n389:             self._test_resource_management\n390:         )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/integration_test_suite.py",
              "line": 389,
              "context": "387:         test_result = await self._run_single_test(\n388:             \"resource_management\",\n389:             self._test_resource_management\n390:         )\n391:         tests.append(test_result)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/integration_test_suite.py",
              "line": 737,
              "context": "735:         return {'success': True, 'message': '性能优化测试通过'}\n736:         \n737:     async def _test_resource_management(self):\n738:         await asyncio.sleep(0.1)\n739:         return {'success': True, 'message': '资源管理测试通过'}",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/realtime_lottery_service.py",
              "line": 180,
              "context": "178:                 'result_digits': json.dumps(draw_data.get('numbers', [])),\n179:                 'created_at': datetime.now(timezone.utc).isoformat(),\n180:                 'source': 'upstream_api_realtime'\n181:             }\n182:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/system_integration_manager.py",
              "line": 395,
              "context": "393:         \n394:         # 检查系统资源\n395:         self._check_system_resources()\n396:     \n397:     def _check_realtime_service_health(self):",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/system_integration_manager.py",
              "line": 504,
              "context": "502:             self.service_health[\"backfill\"] = health\n503:     \n504:     def _check_system_resources(self):\n505:         \"\"\"检查系统资源\"\"\"\n506:         try:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/system_integration_manager.py",
              "line": 582,
              "context": "580:             \n581:             # 检查资源使用告警\n582:             self._check_resource_usage_alert()\n583:             \n584:             # 检查服务健康告警",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/system_integration_manager.py",
              "line": 624,
              "context": "622:             logger.error(f\"响应时间告警检查失败: {e}\")\n623:     \n624:     def _check_resource_usage_alert(self):\n625:         \"\"\"检查资源使用告警\"\"\"\n626:         try:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/api_field_optimization.py",
              "line": 38,
              "context": "36:     \n37:     # 数据质量和元信息\n38:     source: str = \"upstream_api\"\n39:     api_response_code: int = 10000\n40:     api_message: str = \"操作成功!\"",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/test_api_integration.py",
              "line": 52,
              "context": "50:                 \"max_retries\": 3\n51:             },\n52:             \"data_source\": {\n53:                 \"use_upstream_api\": True,\n54:                 \"fallback_to_bigquery\": True,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/test_api_integration.py",
              "line": 231,
              "context": "229:             },\n230:             'upstream_api': config['upstream_api'],\n231:             'data_source': {\n232:                 'use_upstream_api': True,\n233:                 'fallback_to_bigquery': False,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/test_api_integration.py",
              "line": 245,
              "context": "243:         if current_info:\n244:             logger.info(\"✓ 集成适配器成功获取当前开奖信息\")\n245:             logger.info(f\"  数据源: {current_info.get('source', 'unknown')}\")\n246:         else:\n247:             logger.warning(\"⚠ 集成适配器未获取到当前开奖信息\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/alert_integration.py",
              "line": 200,
              "context": "198:                         title=f\"数据质量告警: {threshold.description}\",\n199:                         message=f\"{threshold.metric_name}: {metric_value:.3f} (阈值: {threshold.threshold_value})\",\n200:                         source=\"data_quality_monitor\",\n201:                         metadata={\n202:                             'metric': threshold.metric_name,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/alert_integration.py",
              "line": 235,
              "context": "233:                         title=f\"{api_type.upper()} API健康检查失败\",\n234:                         message=f\"API状态: {health_data.get('status', 'unknown')}, 错误: {health_data.get('error', 'N/A')}\",\n235:                         source=f\"{api_type}_api_monitor\",\n236:                         metadata=health_data\n237:                     )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/alert_integration.py",
              "line": 251,
              "context": "249:                                     title=f\"{api_type.upper()} API响应时间告警\",\n250:                                     message=f\"响应时间: {response_time}ms (阈值: {threshold.threshold_value}ms)\",\n251:                                     source=f\"{api_type}_api_monitor\",\n252:                                     metadata={\n253:                                         'api_type': api_type,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/alert_integration.py",
              "line": 285,
              "context": "283:                     title=\"系统健康检查失败\",\n284:                     message=f\"系统状态异常，详情: {system_health.get('issues', [])}\",\n285:                     source=\"system_monitor\",\n286:                     metadata=system_health\n287:                 )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/alert_integration.py",
              "line": 299,
              "context": "297:                     title=\"系统可用性告警\",\n298:                     message=f\"系统可用性: {uptime_percentage:.2f}% (低于99%)\",\n299:                     source=\"system_monitor\",\n300:                     metadata={'uptime_percentage': uptime_percentage}\n301:                 )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter_client.py",
              "line": 57,
              "context": "55:         \n56:         # 数据源优先级配置\n57:         self.use_upstream_api = config.get('data_source', {}).get('use_upstream_api', True)\n58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter_client.py",
              "line": 58,
              "context": "56:         # 数据源优先级配置\n57:         self.use_upstream_api = config.get('data_source', {}).get('use_upstream_api', True)\n58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)\n60:         self.validation_enabled = config.get('data_source', {}).get('validation_enabled', True)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter_client.py",
              "line": 59,
              "context": "57:         self.use_upstream_api = config.get('data_source', {}).get('use_upstream_api', True)\n58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)\n60:         self.validation_enabled = config.get('data_source', {}).get('validation_enabled', True)\n61:     ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter_client.py",
              "line": 60,
              "context": "58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)\n60:         self.validation_enabled = config.get('data_source', {}).get('validation_enabled', True)\n61:     \n62:     def get_current_draw_info(self) -> Optional[Dict[str, Any]]:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_sync_validator.py",
              "line": 189,
              "context": "187:         }\n188:     \n189:     def validate_draw_data(self, draw_data: Dict[str, Any], source: str = 'unknown') -> ValidationResult:\n190:         \"\"\"\n191:         验证单条开奖数据",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_sync_validator.py",
              "line": 195,
              "context": "193:         Args:\n194:             draw_data: 开奖数据\n195:             source: 数据来源\n196:             \n197:         Returns:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_sync_validator.py",
              "line": 292,
              "context": "290:                 message=message,\n291:                 details={\n292:                     'source': source,\n293:                     'errors': errors,\n294:                     'warnings': warnings,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_sync_validator.py",
              "line": 292,
              "context": "290:                 message=message,\n291:                 details={\n292:                     'source': source,\n293:                     'errors': errors,\n294:                     'warnings': warnings,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_sync_validator.py",
              "line": 306,
              "context": "304:                 status='error',\n305:                 message=f\"验证过程异常: {str(e)}\",\n306:                 details={'exception': str(e), 'source': source},\n307:                 timestamp=datetime.datetime.now().isoformat()\n308:             )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_sync_validator.py",
              "line": 306,
              "context": "304:                 status='error',\n305:                 message=f\"验证过程异常: {str(e)}\",\n306:                 details={'exception': str(e), 'source': source},\n307:                 timestamp=datetime.datetime.now().isoformat()\n308:             )",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_sync_validator.py",
              "line": 337,
              "context": "335:         return value\n336: \n337:     def validate_batch_data(self, data_list: List[Dict[str, Any]], source: str = 'unknown') -> Dict[str, Any]:\n338:         \"\"\"\n339:         批量验证开奖数据",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_sync_validator.py",
              "line": 343,
              "context": "341:         Args:\n342:             data_list: 开奖数据列表\n343:             source: 数据来源\n344:             \n345:         Returns:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_sync_validator.py",
              "line": 362,
              "context": "360:         \n361:         for i, data in enumerate(data_list):\n362:             validation_result = self.validate_draw_data(data, source)\n363:             results['details'].append({\n364:                 'index': i,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_sync_validator.py",
              "line": 449,
              "context": "447:         }\n448:     \n449:     def compare_data_sources(self, upstream_data: List[Dict[str, Any]], \n450:                            bigquery_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n451:         \"\"\"",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_sync_validator.py",
              "line": 518,
              "context": "516:             validated_data = []\n517:             for data in data_list:\n518:                 validation_result = self.validate_draw_data(data, 'sync_source')\n519:                 if validation_result.status in ['ok', 'warning']:\n520:                     validated_data.append(data)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 19,
              "context": "17:     \"\"\"数据质量指标\"\"\"\n18:     timestamp: datetime\n19:     data_source: str\n20:     completeness_score: float  # 完整性评分 (0-1)\n21:     consistency_score: float   # 一致性评分 (0-1)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 62,
              "context": "60:             return DataQualityMetrics(\n61:                 timestamp=datetime.now(),\n62:                 data_source='historical',\n63:                 completeness_score=0.0,\n64:                 consistency_score=0.0,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 103,
              "context": "101:         return DataQualityMetrics(\n102:             timestamp=datetime.now(),\n103:             data_source='historical',\n104:             completeness_score=avg_completeness,\n105:             consistency_score=avg_consistency,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 138,
              "context": "136:         return DataQualityMetrics(\n137:             timestamp=datetime.now(),\n138:             data_source=data_type,\n139:             completeness_score=completeness_score,\n140:             consistency_score=consistency_score,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 259,
              "context": "257:     def record_quality_metrics(self, metrics: DataQualityMetrics):\n258:         \"\"\"记录数据质量指标\"\"\"\n259:         self.quality_history[metrics.data_source].append(metrics)\n260:         \n261:         # 保持历史记录在合理范围内",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 263,
              "context": "261:         # 保持历史记录在合理范围内\n262:         max_history = 100\n263:         if len(self.quality_history[metrics.data_source]) > max_history:\n264:             self.quality_history[metrics.data_source] = self.quality_history[metrics.data_source][-max_history:]\n265:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 264,
              "context": "262:         max_history = 100\n263:         if len(self.quality_history[metrics.data_source]) > max_history:\n264:             self.quality_history[metrics.data_source] = self.quality_history[metrics.data_source][-max_history:]\n265:         \n266:         # 记录日志",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 264,
              "context": "262:         max_history = 100\n263:         if len(self.quality_history[metrics.data_source]) > max_history:\n264:             self.quality_history[metrics.data_source] = self.quality_history[metrics.data_source][-max_history:]\n265:         \n266:         # 记录日志",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 267,
              "context": "265:         \n266:         # 记录日志\n267:         self.logger.info(f\"数据质量评估 - {metrics.data_source}: 总分={metrics.overall_score:.3f}, \"\n268:                         f\"完整性={metrics.completeness_score:.3f}, 一致性={metrics.consistency_score:.3f}, \"\n269:                         f\"时效性={metrics.timeliness_score:.3f}, 准确性={metrics.accuracy_score:.3f}\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 272,
              "context": "270:         \n271:         if metrics.issues:\n272:             self.logger.warning(f\"数据质量问题 - {metrics.data_source}: {', '.join(metrics.issues)}\")\n273:     \n274:     def get_quality_summary(self, hours: int = 24) -> Dict[str, Any]:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 279,
              "context": "277:         summary = {}\n278:         \n279:         for data_source, metrics_list in self.quality_history.items():\n280:             recent_metrics = [m for m in metrics_list if m.timestamp >= cutoff_time]\n281:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 312,
              "context": "310:                 status = 'critical'\n311:             \n312:             summary[data_source] = {\n313:                 'status': status,\n314:                 'avg_scores': avg_scores,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/history_backfill_service.py",
              "line": 260,
              "context": "258:                     'result_digits': json.dumps(data.get('numbers', [])),\n259:                     'created_at': datetime.now(timezone.utc).isoformat(),\n260:                     'source': 'upstream_api_history'\n261:                 }\n262:                 ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/enhanced_voting.py",
              "line": 84,
              "context": "82:         # 如果candidates有具体的概率数据，从中提取\n83:         for candidate in candidates:\n84:             if candidate.get('source') == 'cloud':\n85:                 p_cloud = candidate.get('p_win', 0.5)\n86:             elif candidate.get('source') == 'map':",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/enhanced_voting.py",
              "line": 86,
              "context": "84:             if candidate.get('source') == 'cloud':\n85:                 p_cloud = candidate.get('p_win', 0.5)\n86:             elif candidate.get('source') == 'map':\n87:                 p_map = candidate.get('p_win', 0.5)\n88:             elif candidate.get('source') == 'size':",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/enhanced_voting.py",
              "line": 88,
              "context": "86:             elif candidate.get('source') == 'map':\n87:                 p_map = candidate.get('p_win', 0.5)\n88:             elif candidate.get('source') == 'size':\n89:                 p_size = candidate.get('p_win', 0.5)\n90:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter.py",
              "line": 57,
              "context": "55:         \n56:         # 数据源优先级配置\n57:         self.use_upstream_api = config.get('data_source', {}).get('use_upstream_api', True)\n58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter.py",
              "line": 58,
              "context": "56:         # 数据源优先级配置\n57:         self.use_upstream_api = config.get('data_source', {}).get('use_upstream_api', True)\n58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)\n60:         self.validation_enabled = config.get('data_source', {}).get('validation_enabled', True)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter.py",
              "line": 59,
              "context": "57:         self.use_upstream_api = config.get('data_source', {}).get('use_upstream_api', True)\n58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)\n60:         self.validation_enabled = config.get('data_source', {}).get('validation_enabled', True)\n61:     ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter.py",
              "line": 60,
              "context": "58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)\n60:         self.validation_enabled = config.get('data_source', {}).get('validation_enabled', True)\n61:     \n62:     def get_current_draw_info(self) -> Optional[Dict[str, Any]]:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 42,
              "context": "40:     title: str\n41:     message: str\n42:     source: str\n43:     created_at: str\n44:     resolved_at: Optional[str] = None",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 118,
              "context": "116:                     title TEXT NOT NULL,\n117:                     message TEXT NOT NULL,\n118:                     source TEXT NOT NULL,\n119:                     created_at TEXT NOT NULL,\n120:                     resolved_at TEXT,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 276,
              "context": "274:                             title=f\"数据新鲜度告警: {table_name}\",\n275:                             message=f\"表 {table_name} 数据已过期 {value:.1f} 小时\",\n276:                             source='monitoring_system',\n277:                             created_at=datetime.now().isoformat(),\n278:                             metadata={'table_name': table_name, 'age_hours': value}",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 289,
              "context": "287:                     title=\"API连接异常\",\n288:                     message=\"PC28上游API连接失败\",\n289:                     source='monitoring_system',\n290:                     created_at=datetime.now().isoformat()\n291:                 ))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 302,
              "context": "300:                     title=\"数据同步异常\",\n301:                     message=f\"有 {failed_count} 个表同步失败\",\n302:                     source='monitoring_system',\n303:                     created_at=datetime.now().isoformat(),\n304:                     metadata={'failed_count': failed_count}",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 317,
              "context": "315:                         title=\"内存使用率过高\",\n316:                         message=f\"内存使用率: {memory_usage:.1f}%\",\n317:                         source='monitoring_system',\n318:                         created_at=datetime.now().isoformat(),\n319:                         metadata={'memory_usage': memory_usage}",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 331,
              "context": "329:                         title=\"磁盘使用率过高\",\n330:                         message=f\"磁盘使用率: {disk_usage:.1f}%\",\n331:                         source='monitoring_system',\n332:                         created_at=datetime.now().isoformat(),\n333:                         metadata={'disk_usage': disk_usage}",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 348,
              "context": "346:             existing = self.db.execute_query(\"\"\"\n347:                 SELECT id FROM alerts \n348:                 WHERE alert_type = ? AND source = ? AND status = 'active'\n349:                 AND created_at > ?\n350:             \"\"\", (alert.alert_type, alert.source, ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 350,
              "context": "348:                 WHERE alert_type = ? AND source = ? AND status = 'active'\n349:                 AND created_at > ?\n350:             \"\"\", (alert.alert_type, alert.source, \n351:                   (datetime.now() - timedelta(minutes=self.alert_config['alert_cooldown_minutes'])).isoformat()))\n352:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 359,
              "context": "357:             # 存储告警\n358:             self.db.execute_update(\"\"\"\n359:                 INSERT INTO alerts (id, alert_type, severity, title, message, source, created_at, status, metadata)\n360:                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n361:             \"\"\", (alert.id, alert.alert_type, alert.severity, alert.title, alert.message,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 362,
              "context": "360:                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n361:             \"\"\", (alert.id, alert.alert_type, alert.severity, alert.title, alert.message,\n362:                   alert.source, alert.created_at, alert.status, json.dumps(alert.metadata or {})))\n363:             \n364:             # 发送通知",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 416,
              "context": "414: - 标题: {alert.title}\n415: - 消息: {alert.message}\n416: - 来源: {alert.source}\n417: - 时间: {alert.created_at}\n418: ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 448,
              "context": "446:                 'title': alert.title,\n447:                 'message': alert.message,\n448:                 'source': alert.source,\n449:                 'created_at': alert.created_at,\n450:                 'metadata': alert.metadata",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 448,
              "context": "446:                 'title': alert.title,\n447:                 'message': alert.message,\n448:                 'source': alert.source,\n449:                 'created_at': alert.created_at,\n450:                 'metadata': alert.metadata",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 525,
              "context": "523:                 'api_service': 'healthy' if metrics.get('api_health_status', 0) == 1 else 'unhealthy',\n524:                 'data_sync': 'healthy' if metrics.get('sync_health_score', 0) == 1 else 'unhealthy',\n525:                 'system_resources': 'healthy' if metrics.get('memory_usage_percent', 0) < 85 else 'degraded'\n526:             }\n527:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/monitoring_alerting_system.py",
              "line": 359,
              "context": "357:             # 存储告警\n358:             self.db.execute_update(\"\"\"\n359:                 INSERT INTO alerts (id, alert_type, severity, title, message, source, created_at, status, metadata)\n360:                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n361:             \"\"\", (alert.id, alert.alert_type, alert.severity, alert.title, alert.message,",
              "pattern": "INSERT.*source"
            },
            {
              "file": "local_system/cloud_sync_manager.py",
              "line": 86,
              "context": "84:                     'local_table': 'signal_pool_union_v3',\n85:                     'cloud_table': f'{self.project_id}.{self.dataset_id}.signal_pool_union_v3',\n86:                     'key_fields': ['draw_id', 'market', 'pick', 'source'],\n87:                     'sync_interval_minutes': 15\n88:                 },",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_database.py",
              "line": 62,
              "context": "60:                     pick TEXT,\n61:                     p_win REAL,\n62:                     source TEXT DEFAULT 'cloud_api',\n63:                     created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n64:                     data_date TEXT,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_database.py",
              "line": 79,
              "context": "77:                     pick TEXT,\n78:                     p_win REAL,\n79:                     source TEXT DEFAULT 'map_api',\n80:                     created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n81:                     data_date TEXT,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_database.py",
              "line": 96,
              "context": "94:                     pick TEXT,\n95:                     p_win REAL,\n96:                     source TEXT DEFAULT 'size_api',\n97:                     created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n98:                     data_date TEXT,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_database.py",
              "line": 128,
              "context": "126:                     pick TEXT,\n127:                     p_win REAL,\n128:                     source TEXT,\n129:                     vote_ratio REAL,\n130:                     pick_zh TEXT,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_database.py",
              "line": 133,
              "context": "131:                     day_id_cst TEXT,\n132:                     created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n133:                     UNIQUE(draw_id, market, pick, source)\n134:                 )\n135:             \"\"\",",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_database.py",
              "line": 149,
              "context": "147:                     ev REAL,\n148:                     kelly_frac REAL,\n149:                     source TEXT,\n150:                     vote_ratio REAL,\n151:                     pick_zh TEXT,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/pc28_self_healing_system.py",
              "line": 47,
              "context": "45:     def __init__(self, config_path: Optional[str] = None):\n46:         \"\"\"初始化自愈系统\"\"\"\n47:         self.config_path = config_path or \"/Users/a606/cloud_function_source/local_system/system_config.json\"\n48:         self.config = self._load_config()\n49:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/pc28_self_healing_system.py",
              "line": 106,
              "context": "104:             \"logging\": {\n105:                 \"level\": \"INFO\",\n106:                 \"file_path\": \"/Users/a606/cloud_function_source/local_system/logs/system.log\",\n107:                 \"max_file_size_mb\": 100,\n108:                 \"backup_count\": 5",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/test_system.py",
              "line": 43,
              "context": "41:             'pick': 'big',\n42:             'p_win': 0.65,\n43:             'source': 'test',\n44:             'data_date': datetime.now().strftime('%Y-%m-%d')\n45:         }",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/test_system.py",
              "line": 49,
              "context": "47:         db.execute_update(\"\"\"\n48:             INSERT INTO cloud_pred_today_norm \n49:             (draw_id, timestamp, period, market, pick, p_win, source, data_date)\n50:             VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n51:         \"\"\", tuple(test_data.values()))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 35,
              "context": "33:         markets = ['pc28']\n34:         picks = ['big', 'small', 'odd', 'even']\n35:         sources = ['model_v1', 'model_v2', 'ensemble']\n36:         \n37:         for i in range(count):",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 49,
              "context": "47:                 'pick': random.choice(picks),\n48:                 'p_win': round(random.uniform(0.45, 0.75), 4),\n49:                 'source': random.choice(sources),\n50:                 'data_date': timestamp.strftime('%Y-%m-%d')\n51:             })",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 49,
              "context": "47:                 'pick': random.choice(picks),\n48:                 'p_win': round(random.uniform(0.45, 0.75), 4),\n49:                 'source': random.choice(sources),\n50:                 'data_date': timestamp.strftime('%Y-%m-%d')\n51:             })",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 61,
              "context": "59:         \n60:         picks = ['odd', 'even']\n61:         sources = ['map_model_v1', 'map_ensemble']\n62:         \n63:         for i in range(count):",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 74,
              "context": "72:                 'pick': random.choice(picks),\n73:                 'p_win': round(random.uniform(0.48, 0.72), 4),\n74:                 'source': random.choice(sources),\n75:                 'data_date': timestamp.strftime('%Y-%m-%d')\n76:             })",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 74,
              "context": "72:                 'pick': random.choice(picks),\n73:                 'p_win': round(random.uniform(0.48, 0.72), 4),\n74:                 'source': random.choice(sources),\n75:                 'data_date': timestamp.strftime('%Y-%m-%d')\n76:             })",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 86,
              "context": "84:         \n85:         picks = ['big', 'small']\n86:         sources = ['size_model_v1', 'size_ensemble']\n87:         \n88:         for i in range(count):",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 99,
              "context": "97:                 'pick': random.choice(picks),\n98:                 'p_win': round(random.uniform(0.46, 0.74), 4),\n99:                 'source': random.choice(sources),\n100:                 'data_date': timestamp.strftime('%Y-%m-%d')\n101:             })",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 99,
              "context": "97:                 'pick': random.choice(picks),\n98:                 'p_win': round(random.uniform(0.46, 0.74), 4),\n99:                 'source': random.choice(sources),\n100:                 'data_date': timestamp.strftime('%Y-%m-%d')\n101:             })",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 116,
              "context": "114:                 self.db.execute_update(\"\"\"\n115:                     INSERT OR REPLACE INTO cloud_pred_today_norm \n116:                     (draw_id, timestamp, period, market, pick, p_win, source, data_date)\n117:                     VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n118:                 \"\"\", tuple(record.values()))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 126,
              "context": "124:                 self.db.execute_update(\"\"\"\n125:                     INSERT OR REPLACE INTO p_map_clean_merged_dedup_v \n126:                     (draw_id, timestamp, period, market, pick, p_win, source, data_date)\n127:                     VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n128:                 \"\"\", tuple(record.values()))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 136,
              "context": "134:                 self.db.execute_update(\"\"\"\n135:                     INSERT OR REPLACE INTO p_size_clean_merged_dedup_v \n136:                     (draw_id, timestamp, period, market, pick, p_win, source, data_date)\n137:                     VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n138:                 \"\"\", tuple(record.values()))",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 173,
              "context": "171:                 # 显示最新几条记录\n172:                 recent_records = self.db.execute_query(f\"\"\"\n173:                     SELECT draw_id, market, pick, p_win, source \n174:                     FROM {table} \n175:                     ORDER BY timestamp DESC ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 182,
              "context": "180:                     print(\"  最新记录:\")\n181:                     for record in recent_records:\n182:                         print(f\"    {record['draw_id']} | {record['market']} | {record['pick']} | {record['p_win']} | {record['source']}\")\n183:                 print()\n184:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 219,
              "context": "217:                 self.db.execute_update(\"\"\"\n218:                     INSERT OR REPLACE INTO signal_pool_union_v3 \n219:                     (draw_id, ts_utc, period, market, pick, p_win, source, vote_ratio, pick_zh, day_id_cst)\n220:                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n221:                 \"\"\", (",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 276,
              "context": "274:                     INSERT OR REPLACE INTO lab_push_candidates_v2 \n275:                     (id, created_at, ts_utc, period, market, pick, p_win, ev, kelly_frac, \n276:                      source, vote_ratio, pick_zh, day_id_cst, draw_id)\n277:                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n278:                 \"\"\", (",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 173,
              "context": "171:                 # 显示最新几条记录\n172:                 recent_records = self.db.execute_query(f\"\"\"\n173:                     SELECT draw_id, market, pick, p_win, source \n174:                     FROM {table} \n175:                     ORDER BY timestamp DESC ",
              "pattern": "SELECT.*source"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 41,
              "context": "39:                     pick,\n40:                     p_win,\n41:                     source,\n42:                     created_at,\n43:                     data_date",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 57,
              "context": "55:                     c.pick,\n56:                     c.p_win,\n57:                     c.source,\n58:                     c.created_at,\n59:                     c.data_date",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 82,
              "context": "80:                         ELSE s.p_win\n81:                     END as p_win,\n82:                     s.source,\n83:                     s.created_at,\n84:                     s.data_date",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 103,
              "context": "101:                     pick,\n102:                     p_win,\n103:                     source,\n104:                     1.0 as vote_ratio,\n105:                     CASE ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 125,
              "context": "123:                     pick,\n124:                     p_win,\n125:                     source,\n126:                     1.0 as vote_ratio,\n127:                     CASE ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 149,
              "context": "147:                         pick,\n148:                         p_win,\n149:                         source,\n150:                         vote_ratio,\n151:                         pick_zh,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 166,
              "context": "164:                         pick,\n165:                         p_win,\n166:                         source,\n167:                         vote_ratio,\n168:                         pick_zh,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 199,
              "context": "197:                         ELSE 0.0\n198:                     END as kelly_frac,\n199:                     s.source,\n200:                     s.vote_ratio,\n201:                     s.pick_zh,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 273,
              "context": "271:                     pick,\n272:                     p_win,\n273:                     source,\n274:                     vote_ratio,\n275:                     pick_zh,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 313,
              "context": "311:                     ev,\n312:                     kelly_frac,\n313:                     source,\n314:                     vote_ratio,\n315:                     pick_zh,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_api_collector.py",
              "line": 144,
              "context": "142:                             'pick': pick,\n143:                             'p_win': p_win,\n144:                             'source': 'realtime_api',\n145:                             'data_date': current_date\n146:                         })",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "local_system/local_api_collector.py",
              "line": 235,
              "context": "233:                             'pick': pick,\n234:                             'p_win': p_win,\n235:                             'source': f'{table}_api',\n236:                             'data_date': data_date\n237:                         })",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 282,
              "context": "280:                 CASE WHEN p_even >= 0.5 THEN 'even' ELSE 'odd' END as pick,\n281:                 CASE WHEN p_even >= 0.5 THEN p_even ELSE 1-p_even END as p_win,\n282:                 'map' as source\n283:             FROM `{self.project_id}.{self.dataset_lab}.p_map_today_v`\n284:             WHERE p_even IS NOT NULL",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 307,
              "context": "305:             insert_query = f\"\"\"\n306:             INSERT INTO `{self.project_id}.{self.dataset_lab}.signal_pool`\n307:             (id, created_at, ts_utc, period, market, pick, p_win, source, vote_ratio, params, features, notes)\n308:             SELECT \n309:                 id, created_at, ts_utc, period, market, pick, p_win, source, vote_ratio, params, features, notes",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 309,
              "context": "307:             (id, created_at, ts_utc, period, market, pick, p_win, source, vote_ratio, params, features, notes)\n308:             SELECT \n309:                 id, created_at, ts_utc, period, market, pick, p_win, source, vote_ratio, params, features, notes\n310:             FROM `{self.project_id}.{self.dataset_lab}.signal_pool_union_v3`\n311:             WHERE DATE(ts_utc, 'Asia/Shanghai') = CURRENT_DATE('Asia/Shanghai')",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 466,
              "context": "464:     \n465:     # 保存报告\n466:     report_file = f\"/Users/a606/cloud_function_source/test_suite/repair_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n467:     with open(report_file, 'w', encoding='utf-8') as f:\n468:         f.write(report)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 471,
              "context": "469:     \n470:     # 保存JSON结果\n471:     json_file = f\"/Users/a606/cloud_function_source/test_suite/repair_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n472:     with open(json_file, 'w', encoding='utf-8') as f:\n473:         json.dump(summary, f, indent=2, ensure_ascii=False)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/pc28_data_flow_test.py",
              "line": 437,
              "context": "435:     \n436:     # 保存报告\n437:     report_file = f\"/Users/a606/cloud_function_source/test_suite/test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n438:     with open(report_file, 'w', encoding='utf-8') as f:\n439:         f.write(report)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/pc28_data_flow_test.py",
              "line": 442,
              "context": "440:     \n441:     # 保存JSON结果\n442:     json_file = f\"/Users/a606/cloud_function_source/test_suite/test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n443:     with open(json_file, 'w', encoding='utf-8') as f:\n444:         json.dump(summary, f, indent=2, ensure_ascii=False)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/pc28_data_flow_diagram.py",
              "line": 74,
              "context": "72:             \"tables\": {},\n73:             \"total_rows\": 0,\n74:             \"data_sources\": [\"Cloud API\", \"Map API\", \"Size API\"],\n75:             \"update_frequency\": \"实时\"\n76:         }",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/pc28_data_flow_diagram.py",
              "line": 567,
              "context": "565: {layers[\"1_raw_data\"][\"description\"]}\n566: \n567: **数据源**: {\", \".join(layers[\"1_raw_data\"][\"data_sources\"])}\n568: **更新频率**: {layers[\"1_raw_data\"][\"update_frequency\"]}\n569: **总行数**: {layers[\"1_raw_data\"][\"total_rows\"]:,}",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 29,
              "context": "27:         \n28:         # 数据源表配置\n29:         self.source_tables = {\n30:             \"p_cloud_clean_merged_dedup_v\": {\n31:                 \"base_table\": \"cloud_pred_today_norm\",",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 178,
              "context": "176:             return False\n177:     \n178:     def create_missing_source_tables(self) -> Dict[str, bool]:\n179:         \"\"\"创建缺失的源表\"\"\"\n180:         creation_results = {}",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 182,
              "context": "180:         creation_results = {}\n181:         \n182:         for table_name, config in self.source_tables.items():\n183:             try:\n184:                 # 检查表是否存在",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 248,
              "context": "246:         # 2. 创建缺失的源表\n247:         logger.info(\"创建缺失的源表...\")\n248:         creation_results = self.create_missing_source_tables()\n249:         repair_results[\"table_creation\"] = creation_results\n250:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 281,
              "context": "279:             \n280:             # 插入到各个源表\n281:             for table_name in self.source_tables.keys():\n282:                 if creation_results.get(table_name, False):\n283:                     # 为不同类型的预测调整数据",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 289,
              "context": "287:                         \n288:                         # 根据预测类型调整概率\n289:                         prediction_type = self.source_tables[table_name][\"prediction_type\"]\n290:                         if prediction_type == \"map\":\n291:                             adjusted_record[\"p_even\"] = min(0.8, adjusted_record[\"p_even\"] + 0.1)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 362,
              "context": "360:         # 保存结果\n361:         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n362:         report_file = f\"/Users/a606/cloud_function_source/test_suite/data_collection_repair_report_{timestamp}.json\"\n363:         \n364:         with open(report_file, 'w', encoding='utf-8') as f:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/cloud_data_pipeline_test.py",
              "line": 76,
              "context": "74:                 \"confidence\": round(random.uniform(0.6, 0.95), 3),\n75:                 \"model_version\": \"test_v1.0\",\n76:                 \"source\": \"api_test\",\n77:                 \"features\": json.dumps({\n78:                     \"feature_1\": random.uniform(-1, 1),",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/cloud_data_pipeline_test.py",
              "line": 109,
              "context": "107:             \n108:             # 验证必要字段\n109:             required_fields = [\"id\", \"period\", \"ts_utc\", \"p_even\", \"source\"]\n110:             for record in mock_data[:5]:  # 检查前5条记录\n111:                 for field in required_fields:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/cloud_data_pipeline_test.py",
              "line": 616,
              "context": "614:         # 保存结果到文件\n615:         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n616:         report_file = f\"/Users/a606/cloud_function_source/test_suite/cloud_pipeline_test_report_{timestamp}.json\"\n617:         \n618:         with open(report_file, 'w', encoding='utf-8') as f:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/table_logic_exporter.py",
              "line": 360,
              "context": "358:         if not output_file:\n359:             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n360:             output_file = f\"/Users/a606/cloud_function_source/test_suite/table_logic_export_{timestamp}.json\"\n361:         \n362:         try:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/monitoring_system.py",
              "line": 66,
              "context": "64:     \"\"\"PC28监控系统\"\"\"\n65:     \n66:     def __init__(self, db_path: str = \"/Users/a606/cloud_function_source/test_suite/monitoring.db\"):\n67:         self.db_path = db_path\n68:         self.tester = PC28DataFlowTester()",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 32,
              "context": "30:             \"p_cloud_today_v\": {\n31:                 \"type\": \"date_filter_view\",\n32:                 \"source_table\": \"p_cloud_clean_merged_dedup_v\",\n33:                 \"expected_fields\": [\"period\", \"ts_utc\", \"p_even\", \"src\", \"n_src\"]\n34:             },",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 37,
              "context": "35:             \"p_map_today_v\": {\n36:                 \"type\": \"prediction_view\", \n37:                 \"source_table\": \"p_map_clean_merged_dedup_v\",\n38:                 \"expected_fields\": [\"period\", \"ts_utc\", \"p_even\", \"src\", \"n_src\"]\n39:             },",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 42,
              "context": "40:             \"p_size_today_v\": {\n41:                 \"type\": \"prediction_view\",\n42:                 \"source_table\": \"p_size_clean_merged_dedup_v\", \n43:                 \"expected_fields\": [\"period\", \"ts_utc\", \"p_even\", \"src\", \"n_src\"]\n44:             },",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 47,
              "context": "45:             \"signal_pool_union_v3\": {\n46:                 \"type\": \"union_view\",\n47:                 \"source_views\": [\"p_ensemble_today_canon_v\", \"p_map_today_canon_v\", \"p_size_today_canon_v\"],\n48:                 \"expected_fields\": [\"id\", \"created_at\", \"ts_utc\", \"period\", \"market\", \"pick\", \"p_win\", \"source\"]\n49:             }",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 48,
              "context": "46:                 \"type\": \"union_view\",\n47:                 \"source_views\": [\"p_ensemble_today_canon_v\", \"p_map_today_canon_v\", \"p_size_today_canon_v\"],\n48:                 \"expected_fields\": [\"id\", \"created_at\", \"ts_utc\", \"period\", \"market\", \"pick\", \"p_win\", \"source\"]\n49:             }\n50:         }",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 125,
              "context": "123:             }\n124:     \n125:     def diagnose_date_filter_issue(self, view_name: str, source_table: str) -> Dict[str, Any]:\n126:         \"\"\"诊断日期过滤问题\"\"\"\n127:         diagnosis = {",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 129,
              "context": "127:         diagnosis = {\n128:             \"view_name\": view_name,\n129:             \"source_table\": source_table,\n130:             \"issue_type\": None,\n131:             \"description\": \"\",",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 129,
              "context": "127:         diagnosis = {\n128:             \"view_name\": view_name,\n129:             \"source_table\": source_table,\n130:             \"issue_type\": None,\n131:             \"description\": \"\",",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 136,
              "context": "134:         \n135:         # 检查源表数据\n136:         source_data = self.check_table_data(source_table)\n137:         \n138:         # 检查视图数据",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 136,
              "context": "134:         \n135:         # 检查源表数据\n136:         source_data = self.check_table_data(source_table)\n137:         \n138:         # 检查视图数据",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 146,
              "context": "144:         current_date = str(current_date_result[0]['today'])\n145:         \n146:         if source_data[\"has_data\"] and not view_data[\"has_data\"]:\n147:             # 源表有数据但视图没有数据，可能是日期过滤问题\n148:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 150,
              "context": "148:             \n149:             # 检查源表最新数据日期\n150:             if source_data[\"date_distribution\"]:\n151:                 latest_source_date = source_data[\"date_distribution\"][0][\"date\"]\n152:                 ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 151,
              "context": "149:             # 检查源表最新数据日期\n150:             if source_data[\"date_distribution\"]:\n151:                 latest_source_date = source_data[\"date_distribution\"][0][\"date\"]\n152:                 \n153:                 if latest_source_date != current_date:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 151,
              "context": "149:             # 检查源表最新数据日期\n150:             if source_data[\"date_distribution\"]:\n151:                 latest_source_date = source_data[\"date_distribution\"][0][\"date\"]\n152:                 \n153:                 if latest_source_date != current_date:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 153,
              "context": "151:                 latest_source_date = source_data[\"date_distribution\"][0][\"date\"]\n152:                 \n153:                 if latest_source_date != current_date:\n154:                     diagnosis[\"issue_type\"] = \"date_mismatch\"\n155:                     diagnosis[\"description\"] = f\"源表最新数据日期 {latest_source_date} 与当前日期 {current_date} 不匹配\"",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 155,
              "context": "153:                 if latest_source_date != current_date:\n154:                     diagnosis[\"issue_type\"] = \"date_mismatch\"\n155:                     diagnosis[\"description\"] = f\"源表最新数据日期 {latest_source_date} 与当前日期 {current_date} 不匹配\"\n156:                     diagnosis[\"suggested_fix\"] = \"expand_date_range\"\n157:                 else:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 162,
              "context": "160:                     diagnosis[\"suggested_fix\"] = \"fix_filter_logic\"\n161:             else:\n162:                 diagnosis[\"issue_type\"] = \"no_source_data\"\n163:                 diagnosis[\"description\"] = \"源表没有有效的时间戳数据\"\n164:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 165,
              "context": "163:                 diagnosis[\"description\"] = \"源表没有有效的时间戳数据\"\n164:         \n165:         elif not source_data[\"has_data\"]:\n166:             diagnosis[\"issue_type\"] = \"empty_source\"\n167:             diagnosis[\"description\"] = f\"源表 {source_table} 为空\"",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 166,
              "context": "164:         \n165:         elif not source_data[\"has_data\"]:\n166:             diagnosis[\"issue_type\"] = \"empty_source\"\n167:             diagnosis[\"description\"] = f\"源表 {source_table} 为空\"\n168:         ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 167,
              "context": "165:         elif not source_data[\"has_data\"]:\n166:             diagnosis[\"issue_type\"] = \"empty_source\"\n167:             diagnosis[\"description\"] = f\"源表 {source_table} 为空\"\n168:         \n169:         diagnosis[\"source_data\"] = source_data",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 169,
              "context": "167:             diagnosis[\"description\"] = f\"源表 {source_table} 为空\"\n168:         \n169:         diagnosis[\"source_data\"] = source_data\n170:         diagnosis[\"view_data\"] = view_data\n171:         diagnosis[\"current_date\"] = current_date",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 169,
              "context": "167:             diagnosis[\"description\"] = f\"源表 {source_table} 为空\"\n168:         \n169:         diagnosis[\"source_data\"] = source_data\n170:         diagnosis[\"view_data\"] = view_data\n171:         diagnosis[\"current_date\"] = current_date",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 265,
              "context": "263:         return fixed_sql\n264:     \n265:     def create_missing_canonical_view(self, view_name: str, source_view: str) -> bool:\n266:         \"\"\"创建缺失的标准化视图\"\"\"\n267:         try:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 281,
              "context": "279:                     END as pick,\n280:                     p_even as p_win,\n281:                     src as source,\n282:                     NULL as vote_ratio,\n283:                     NULL as params,",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 286,
              "context": "284:                     NULL as features,\n285:                     NULL as notes\n286:                 FROM `{self.project_id}.{self.dataset_id}.{source_view}`\n287:                 WHERE p_even IS NOT NULL\n288:             \"\"\"",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 321,
              "context": "319:             if view_name in self.repair_targets:\n320:                 target_info = self.repair_targets[view_name]\n321:                 source_table = target_info[\"source_table\"]\n322:                 \n323:                 logger.info(f\"诊断视图: {view_name}\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 321,
              "context": "319:             if view_name in self.repair_targets:\n320:                 target_info = self.repair_targets[view_name]\n321:                 source_table = target_info[\"source_table\"]\n322:                 \n323:                 logger.info(f\"诊断视图: {view_name}\")",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 324,
              "context": "322:                 \n323:                 logger.info(f\"诊断视图: {view_name}\")\n324:                 diagnosis = self.diagnose_date_filter_issue(view_name, source_table)\n325:                 \n326:                 repair_result = {",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 356,
              "context": "354:         }\n355:         \n356:         for canon_view, source_view in canonical_views.items():\n357:             # 检查标准化视图是否存在且有数据\n358:             canon_data = self.check_table_data(canon_view)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 359,
              "context": "357:             # 检查标准化视图是否存在且有数据\n358:             canon_data = self.check_table_data(canon_view)\n359:             source_data = self.check_table_data(source_view)\n360:             \n361:             repair_result = {",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 359,
              "context": "357:             # 检查标准化视图是否存在且有数据\n358:             canon_data = self.check_table_data(canon_view)\n359:             source_data = self.check_table_data(source_view)\n360:             \n361:             repair_result = {",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 363,
              "context": "361:             repair_result = {\n362:                 \"view_name\": canon_view,\n363:                 \"source_view\": source_view,\n364:                 \"repair_attempted\": False,\n365:                 \"repair_successful\": False",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 363,
              "context": "361:             repair_result = {\n362:                 \"view_name\": canon_view,\n363:                 \"source_view\": source_view,\n364:                 \"repair_attempted\": False,\n365:                 \"repair_successful\": False",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 368,
              "context": "366:             }\n367:             \n368:             if not canon_data[\"has_data\"] and source_data[\"has_data\"]:\n369:                 logger.info(f\"创建标准化视图: {canon_view}\")\n370:                 repair_result[\"repair_attempted\"] = True",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 372,
              "context": "370:                 repair_result[\"repair_attempted\"] = True\n371:                 \n372:                 success = self.create_missing_canonical_view(canon_view, source_view)\n373:                 repair_result[\"repair_successful\"] = success\n374:                 ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 428,
              "context": "426:         # 保存结果\n427:         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n428:         report_file = f\"/Users/a606/cloud_function_source/test_suite/smart_repair_report_{timestamp}.json\"\n429:         \n430:         with open(report_file, 'w', encoding='utf-8') as f:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/comprehensive_repair_system.py",
              "line": 196,
              "context": "194:             return {\"view_name\": view_name, \"dependencies\": [], \"sql\": None, \"error\": str(e)}\n195:     \n196:     def create_missing_source_table(self, table_name: str) -> bool:\n197:         \"\"\"创建缺失的源表\"\"\"\n198:         try:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/comprehensive_repair_system.py",
              "line": 267,
              "context": "265:                 else:\n266:                     # 创建缺失的表\n267:                     if self.create_missing_source_table(missing_dep):\n268:                         logger.info(f\"创建了缺失的表: {missing_dep}\")\n269:                     else:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/comprehensive_repair_system.py",
              "line": 437,
              "context": "435:         # 保存结果\n436:         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n437:         report_file = f\"/Users/a606/cloud_function_source/test_suite/comprehensive_repair_report_{timestamp}.json\"\n438:         \n439:         with open(report_file, 'w', encoding='utf-8') as f:",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/intelligent_auto_repair.py",
              "line": 65,
              "context": "63:                         {\"name\": \"ts_utc\", \"type\": \"TIMESTAMP\", \"mode\": \"REQUIRED\"},\n64:                         {\"name\": \"p_even\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\"},\n65:                         {\"name\": \"source\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"}\n66:                     ]\n67:                 },",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/intelligent_auto_repair.py",
              "line": 77,
              "context": "75:                         {\"name\": \"pick\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n76:                         {\"name\": \"p_win\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\"},\n77:                         {\"name\": \"source\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"}\n78:                     ]\n79:                 }",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/intelligent_auto_repair.py",
              "line": 96,
              "context": "94:         try:\n95:             import glob\n96:             pattern = \"/Users/a606/cloud_function_source/test_suite/table_logic_export_*.json\"\n97:             files = glob.glob(pattern)\n98:             ",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/intelligent_auto_repair.py",
              "line": 197,
              "context": "195:                                     \"table\": table_name,\n196:                                     \"missing_field\": ref_field,\n197:                                     \"source_table\": dep_table,\n198:                                     \"suggested_field\": suggested_field,\n199:                                     \"available_fields\": list(available_fields)",
              "pattern": "['\\\"]?source['\\\"]?"
            },
            {
              "file": "test_suite/intelligent_auto_repair.py",
              "line": 552,
              "context": "550:         # 保存结果\n551:         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n552:         report_file = f\"/Users/a606/cloud_function_source/test_suite/intelligent_repair_report_{timestamp}.json\"\n553:         \n554:         with open(report_file, 'w', encoding='utf-8') as f:",
              "pattern": "['\\\"]?source['\\\"]?"
            }
          ],
          "usage_patterns": [],
          "last_used": null
        }
      },
      "draws_14w_dedup_v": {
        "ts_utc": {
          "references": [
            {
              "file": "database_table_optimizer.py",
              "line": 81,
              "context": "79:                     {'name': 'numbers', 'type': 'REPEATED INTEGER', 'nullable': False, 'usage': 'high'},\n80:                     {'name': 'result_sum', 'type': 'INTEGER', 'nullable': False, 'usage': 'high'},\n81:                     {'name': 'ts_utc', 'type': 'TIMESTAMP', 'nullable': True, 'usage': 'medium'},  # 与timestamp重复\n82:                     {'name': 'legacy_format', 'type': 'STRING', 'nullable': True, 'usage': 'low'},  # 遗留字段\n83:                     {'name': 'data_source', 'type': 'STRING', 'nullable': True, 'usage': 'low'},",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 213,
              "context": "211:         redundant_patterns = {\n212:             'result_digits': 'numbers',  # result_digits与numbers重复\n213:             'ts_utc': 'timestamp',       # ts_utc与timestamp重复\n214:             'data_source': 'source',     # 类似功能字段\n215:         }",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 213,
              "context": "211:         redundant_patterns = {\n212:             'result_digits': 'numbers',  # result_digits与numbers重复\n213:             'ts_utc': 'timestamp',       # ts_utc与timestamp重复\n214:             'data_source': 'source',     # 类似功能字段\n215:         }",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 222,
              "context": "220:         \"\"\"检查是否有相似字段\"\"\"\n221:         similar_patterns = [\n222:             ('timestamp', 'ts_utc', 'created_at'),\n223:             ('source', 'data_source'),\n224:             ('numbers', 'result_digits')",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "field_cleanup_sql_generator.py",
              "line": 22,
              "context": "20:                 {\n21:                     'table': 'draws_14w_dedup_v',\n22:                     'field': 'ts_utc',\n23:                     'reason': '与timestamp字段重复',\n24:                     'risk': 'low',",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 32,
              "context": "30:         self.target_fields = {\n31:             'score_ledger': ['result_digits', 'source'],\n32:             'draws_14w_dedup_v': ['ts_utc', 'legacy_format', 'data_source'],\n33:             'p_size_clean_merged_dedup_v': ['model_version', 'raw_features', 'processing_time']\n34:         }",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 195,
              "context": "193:                     deps['transformation_logic'] = ['numbers数组的副本']\n194:                 \n195:                 elif field == 'ts_utc':\n196:                     deps['upstream_sources'] = ['timestamp字段的UTC转换']\n197:                     deps['downstream_targets'] = ['时区相关查询']",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_database_optimizer.py",
              "line": 50,
              "context": "48:         # 测试已知冗余字段\n49:         assert optimizer._is_redundant_field('result_digits')\n50:         assert optimizer._is_redundant_field('ts_utc')\n51:         \n52:         # 测试非冗余字段",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_database_optimizer.py",
              "line": 60,
              "context": "58:         # 测试时间相关字段\n59:         assert optimizer._has_similar_field('timestamp')\n60:         assert optimizer._has_similar_field('ts_utc')\n61:         assert optimizer._has_similar_field('created_at')\n62:         ",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_database_optimizer.py",
              "line": 353,
              "context": "351:         redundant_pairs = [\n352:             ('result_digits', 'numbers'),\n353:             ('ts_utc', 'timestamp'),\n354:             ('data_source', 'source')\n355:         ]",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "local_system/local_database.py",
              "line": 123,
              "context": "121:                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n122:                     draw_id TEXT NOT NULL,\n123:                     ts_utc TEXT NOT NULL,\n124:                     period TEXT,\n125:                     market TEXT,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "local_system/local_database.py",
              "line": 142,
              "context": "140:                     id TEXT PRIMARY KEY,\n141:                     created_at TEXT NOT NULL,\n142:                     ts_utc TEXT NOT NULL,\n143:                     period TEXT,\n144:                     market TEXT,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 219,
              "context": "217:                 self.db.execute_update(\"\"\"\n218:                     INSERT OR REPLACE INTO signal_pool_union_v3 \n219:                     (draw_id, ts_utc, period, market, pick, p_win, source, vote_ratio, pick_zh, day_id_cst)\n220:                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n221:                 \"\"\", (",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "local_system/demo_data_generator.py",
              "line": 275,
              "context": "273:                 self.db.execute_update(\"\"\"\n274:                     INSERT OR REPLACE INTO lab_push_candidates_v2 \n275:                     (id, created_at, ts_utc, period, market, pick, p_win, ev, kelly_frac, \n276:                      source, vote_ratio, pick_zh, day_id_cst, draw_id)\n277:                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 98,
              "context": "96:                 SELECT \n97:                     draw_id,\n98:                     datetime('now') as ts_utc,\n99:                     period,\n100:                     market,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 120,
              "context": "118:                 SELECT \n119:                     draw_id,\n120:                     datetime('now') as ts_utc,\n121:                     period,\n122:                     market,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 144,
              "context": "142:                     SELECT \n143:                         draw_id,\n144:                         ts_utc,\n145:                         period,\n146:                         market,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 161,
              "context": "159:                     SELECT \n160:                         draw_id,\n161:                         ts_utc,\n162:                         period,\n163:                         market,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 182,
              "context": "180:                     printf('%s_%s_%s_%s', s.draw_id, s.market, s.pick, strftime('%s', 'now')) as id,\n181:                     datetime('now') as created_at,\n182:                     s.ts_utc,\n183:                     s.period,\n184:                     s.market,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 268,
              "context": "266:                 SELECT \n267:                     draw_id,\n268:                     ts_utc,\n269:                     period,\n270:                     market,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "local_system/local_sql_engine.py",
              "line": 306,
              "context": "304:                     id,\n305:                     created_at,\n306:                     ts_utc,\n307:                     period,\n308:                     market,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 115,
              "context": "113:             query = \"\"\"\n114:             SELECT \n115:                 DATE(ts_utc,'Asia/Shanghai') as data_date,\n116:                 CURRENT_DATE('Asia/Shanghai') as today,\n117:                 COUNT(*) as count",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 216,
              "context": "214:             CREATE OR REPLACE VIEW `{self.project_id}.{self.dataset_lab}.p_cloud_today_v` AS\n215:             WITH params AS (\n216:                 SELECT MAX(DATE(ts_utc,'Asia/Shanghai')) AS day_id\n217:                 FROM `{self.project_id}.{self.dataset_lab}.p_cloud_clean_merged_dedup_v`\n218:             )",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 219,
              "context": "217:                 FROM `{self.project_id}.{self.dataset_lab}.p_cloud_clean_merged_dedup_v`\n218:             )\n219:             SELECT period, ts_utc,\n220:                    GREATEST(LEAST(CAST(p_even AS FLOAT64), 1-1e-6), 1e-6) AS p_even,\n221:                    'cloud' AS src,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 224,
              "context": "222:                    999 AS n_src\n223:             FROM `{self.project_id}.{self.dataset_lab}.p_cloud_clean_merged_dedup_v`, params\n224:             WHERE DATE(ts_utc,'Asia/Shanghai')=params.day_id\n225:             \"\"\"\n226:             ",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 249,
              "context": "247:             SELECT \n248:                 period,\n249:                 ts_utc,\n250:                 p_even,\n251:                 'map' as src,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 278,
              "context": "276:             SELECT \n277:                 period,\n278:                 ts_utc,\n279:                 'oe' as market,\n280:                 CASE WHEN p_even >= 0.5 THEN 'even' ELSE 'odd' END as pick,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 307,
              "context": "305:             insert_query = f\"\"\"\n306:             INSERT INTO `{self.project_id}.{self.dataset_lab}.signal_pool`\n307:             (id, created_at, ts_utc, period, market, pick, p_win, source, vote_ratio, params, features, notes)\n308:             SELECT \n309:                 id, created_at, ts_utc, period, market, pick, p_win, source, vote_ratio, params, features, notes",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 309,
              "context": "307:             (id, created_at, ts_utc, period, market, pick, p_win, source, vote_ratio, params, features, notes)\n308:             SELECT \n309:                 id, created_at, ts_utc, period, market, pick, p_win, source, vote_ratio, params, features, notes\n310:             FROM `{self.project_id}.{self.dataset_lab}.signal_pool_union_v3`\n311:             WHERE DATE(ts_utc, 'Asia/Shanghai') = CURRENT_DATE('Asia/Shanghai')",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 311,
              "context": "309:                 id, created_at, ts_utc, period, market, pick, p_win, source, vote_ratio, params, features, notes\n310:             FROM `{self.project_id}.{self.dataset_lab}.signal_pool_union_v3`\n311:             WHERE DATE(ts_utc, 'Asia/Shanghai') = CURRENT_DATE('Asia/Shanghai')\n312:             \"\"\"\n313:             ",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 216,
              "context": "214:             CREATE OR REPLACE VIEW `{self.project_id}.{self.dataset_lab}.p_cloud_today_v` AS\n215:             WITH params AS (\n216:                 SELECT MAX(DATE(ts_utc,'Asia/Shanghai')) AS day_id\n217:                 FROM `{self.project_id}.{self.dataset_lab}.p_cloud_clean_merged_dedup_v`\n218:             )",
              "pattern": "SELECT.*ts_utc"
            },
            {
              "file": "test_suite/data_repair_system.py",
              "line": 219,
              "context": "217:                 FROM `{self.project_id}.{self.dataset_lab}.p_cloud_clean_merged_dedup_v`\n218:             )\n219:             SELECT period, ts_utc,\n220:                    GREATEST(LEAST(CAST(p_even AS FLOAT64), 1-1e-6), 1e-6) AS p_even,\n221:                    'cloud' AS src,",
              "pattern": "SELECT.*ts_utc"
            },
            {
              "file": "test_suite/pc28_data_flow_diagram.py",
              "line": 217,
              "context": "215:         query = f\"\"\"\n216:         SELECT \n217:             MAX(DATE(ts_utc, 'Asia/Shanghai')) as latest_date,\n218:             MIN(DATE(ts_utc, 'Asia/Shanghai')) as earliest_date\n219:         FROM `{self.project_id}.{self.dataset_id}.{table_name}`",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/pc28_data_flow_diagram.py",
              "line": 218,
              "context": "216:         SELECT \n217:             MAX(DATE(ts_utc, 'Asia/Shanghai')) as latest_date,\n218:             MIN(DATE(ts_utc, 'Asia/Shanghai')) as earliest_date\n219:         FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n220:         WHERE ts_utc IS NOT NULL",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/pc28_data_flow_diagram.py",
              "line": 220,
              "context": "218:             MIN(DATE(ts_utc, 'Asia/Shanghai')) as earliest_date\n219:         FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n220:         WHERE ts_utc IS NOT NULL\n221:         \"\"\"\n222:         ",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 32,
              "context": "30:             \"p_cloud_clean_merged_dedup_v\": {\n31:                 \"base_table\": \"cloud_pred_today_norm\",\n32:                 \"fields\": [\"period\", \"ts_utc\", \"p_even\", \"src\", \"n_src\"],\n33:                 \"prediction_type\": \"cloud\"\n34:             },",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 37,
              "context": "35:             \"p_map_clean_merged_dedup_v\": {\n36:                 \"base_table\": \"cloud_pred_today_norm\", \n37:                 \"fields\": [\"period\", \"ts_utc\", \"p_even\", \"src\", \"n_src\"],\n38:                 \"prediction_type\": \"map\"\n39:             },",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 42,
              "context": "40:             \"p_size_clean_merged_dedup_v\": {\n41:                 \"base_table\": \"cloud_pred_today_norm\",\n42:                 \"fields\": [\"period\", \"ts_utc\", \"p_even\", \"src\", \"n_src\"], \n43:                 \"prediction_type\": \"size\"\n44:             }",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 72,
              "context": "70:                     latest_query = f\"\"\"\n71:                         SELECT \n72:                             MAX(DATE(ts_utc, 'Asia/Shanghai')) as latest_date,\n73:                             COUNT(*) as total_rows\n74:                         FROM `{self.project_id}.{self.dataset_id}.{table_name}`",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 137,
              "context": "135:                 second = random.randint(0, 59)\n136:                 \n137:                 ts_utc = current_dt.replace(hour=hour, minute=minute, second=second)\n138:                 \n139:                 # 生成PC28期号 (假设每5分钟一期)",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 140,
              "context": "138:                 \n139:                 # 生成PC28期号 (假设每5分钟一期)\n140:                 period_base = int(ts_utc.timestamp() / 300)  # 5分钟间隔\n141:                 period = period_base % 1000000  # 保持合理范围\n142:                 ",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 146,
              "context": "144:                 record = {\n145:                     \"period\": period,\n146:                     \"ts_utc\": ts_utc.isoformat() + \"Z\",\n147:                     \"p_even\": round(random.uniform(0.3, 0.7), 4),  # 偶数概率\n148:                     \"src\": random.choice([\"model_a\", \"model_b\", \"model_c\", \"ensemble\"]),",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 146,
              "context": "144:                 record = {\n145:                     \"period\": period,\n146:                     \"ts_utc\": ts_utc.isoformat() + \"Z\",\n147:                     \"p_even\": round(random.uniform(0.3, 0.7), 4),  # 偶数概率\n148:                     \"src\": random.choice([\"model_a\", \"model_b\", \"model_c\", \"ensemble\"]),",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 198,
              "context": "196:                 schema = [\n197:                     bigquery.SchemaField(\"period\", \"INTEGER\"),\n198:                     bigquery.SchemaField(\"ts_utc\", \"TIMESTAMP\"),\n199:                     bigquery.SchemaField(\"p_even\", \"FLOAT\"),\n200:                     bigquery.SchemaField(\"src\", \"STRING\"),",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/data_collection_repair.py",
              "line": 209,
              "context": "207:                 table.time_partitioning = bigquery.TimePartitioning(\n208:                     type_=bigquery.TimePartitioningType.DAY,\n209:                     field=\"ts_utc\"\n210:                 )\n211:                 ",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/cloud_data_pipeline_test.py",
              "line": 69,
              "context": "67:                 \"id\": f\"test_{int(time.time())}_{i}\",\n68:                 \"period\": f\"{(base_time + timedelta(minutes=i)).strftime('%Y%m%d%H%M')}\",\n69:                 \"ts_utc\": (base_time + timedelta(minutes=i)).isoformat(),\n70:                 \"created_at\": base_time.isoformat(),\n71:                 \"p_even\": round(random.uniform(0.3, 0.7), 4),",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/cloud_data_pipeline_test.py",
              "line": 109,
              "context": "107:             \n108:             # 验证必要字段\n109:             required_fields = [\"id\", \"period\", \"ts_utc\", \"p_even\", \"source\"]\n110:             for record in mock_data[:5]:  # 检查前5条记录\n111:                 for field in required_fields:",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 33,
              "context": "31:                 \"type\": \"date_filter_view\",\n32:                 \"source_table\": \"p_cloud_clean_merged_dedup_v\",\n33:                 \"expected_fields\": [\"period\", \"ts_utc\", \"p_even\", \"src\", \"n_src\"]\n34:             },\n35:             \"p_map_today_v\": {",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 38,
              "context": "36:                 \"type\": \"prediction_view\", \n37:                 \"source_table\": \"p_map_clean_merged_dedup_v\",\n38:                 \"expected_fields\": [\"period\", \"ts_utc\", \"p_even\", \"src\", \"n_src\"]\n39:             },\n40:             \"p_size_today_v\": {",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 43,
              "context": "41:                 \"type\": \"prediction_view\",\n42:                 \"source_table\": \"p_size_clean_merged_dedup_v\", \n43:                 \"expected_fields\": [\"period\", \"ts_utc\", \"p_even\", \"src\", \"n_src\"]\n44:             },\n45:             \"signal_pool_union_v3\": {",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 48,
              "context": "46:                 \"type\": \"union_view\",\n47:                 \"source_views\": [\"p_ensemble_today_canon_v\", \"p_map_today_canon_v\", \"p_size_today_canon_v\"],\n48:                 \"expected_fields\": [\"id\", \"created_at\", \"ts_utc\", \"period\", \"market\", \"pick\", \"p_win\", \"source\"]\n49:             }\n50:         }",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 77,
              "context": "75:             # 检查最新数据时间\n76:             latest_query = f\"\"\"\n77:                 SELECT MAX(ts_utc) as latest_time \n78:                 FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n79:                 WHERE ts_utc IS NOT NULL",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 79,
              "context": "77:                 SELECT MAX(ts_utc) as latest_time \n78:                 FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n79:                 WHERE ts_utc IS NOT NULL\n80:             \"\"\"\n81:             ",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 91,
              "context": "89:             date_query = f\"\"\"\n90:                 SELECT \n91:                     DATE(ts_utc, 'Asia/Shanghai') as date,\n92:                     COUNT(*) as count\n93:                 FROM `{self.project_id}.{self.dataset_id}.{table_name}`",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 94,
              "context": "92:                     COUNT(*) as count\n93:                 FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n94:                 WHERE ts_utc IS NOT NULL\n95:                 GROUP BY DATE(ts_utc, 'Asia/Shanghai')\n96:                 ORDER BY date DESC",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 95,
              "context": "93:                 FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n94:                 WHERE ts_utc IS NOT NULL\n95:                 GROUP BY DATE(ts_utc, 'Asia/Shanghai')\n96:                 ORDER BY date DESC\n97:                 LIMIT 10",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 219,
              "context": "217:         # 查找日期过滤条件并替换\n218:         patterns_and_replacements = [\n219:             # 模式1: WHERE DATE(ts_utc,'Asia/Shanghai')=params.day_id\n220:             (\n221:                 r\"WHERE\\s+DATE\\(ts_utc,\\s*'Asia/Shanghai'\\)\\s*=\\s*params\\.day_id\",",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 221,
              "context": "219:             # 模式1: WHERE DATE(ts_utc,'Asia/Shanghai')=params.day_id\n220:             (\n221:                 r\"WHERE\\s+DATE\\(ts_utc,\\s*'Asia/Shanghai'\\)\\s*=\\s*params\\.day_id\",\n222:                 \"WHERE DATE(ts_utc,'Asia/Shanghai') >= DATE_SUB(params.day_id, INTERVAL 7 DAY)\"\n223:             ),",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 222,
              "context": "220:             (\n221:                 r\"WHERE\\s+DATE\\(ts_utc,\\s*'Asia/Shanghai'\\)\\s*=\\s*params\\.day_id\",\n222:                 \"WHERE DATE(ts_utc,'Asia/Shanghai') >= DATE_SUB(params.day_id, INTERVAL 7 DAY)\"\n223:             ),\n224:             # 模式2: WHERE DATE(ts_utc,'Asia/Shanghai') = CURRENT_DATE('Asia/Shanghai')",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 224,
              "context": "222:                 \"WHERE DATE(ts_utc,'Asia/Shanghai') >= DATE_SUB(params.day_id, INTERVAL 7 DAY)\"\n223:             ),\n224:             # 模式2: WHERE DATE(ts_utc,'Asia/Shanghai') = CURRENT_DATE('Asia/Shanghai')\n225:             (\n226:                 r\"WHERE\\s+DATE\\(ts_utc,\\s*'Asia/Shanghai'\\)\\s*=\\s*CURRENT_DATE\\('Asia/Shanghai'\\)\",",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 226,
              "context": "224:             # 模式2: WHERE DATE(ts_utc,'Asia/Shanghai') = CURRENT_DATE('Asia/Shanghai')\n225:             (\n226:                 r\"WHERE\\s+DATE\\(ts_utc,\\s*'Asia/Shanghai'\\)\\s*=\\s*CURRENT_DATE\\('Asia/Shanghai'\\)\",\n227:                 \"WHERE DATE(ts_utc,'Asia/Shanghai') >= DATE_SUB(CURRENT_DATE('Asia/Shanghai'), INTERVAL 7 DAY)\"\n228:             ),",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 227,
              "context": "225:             (\n226:                 r\"WHERE\\s+DATE\\(ts_utc,\\s*'Asia/Shanghai'\\)\\s*=\\s*CURRENT_DATE\\('Asia/Shanghai'\\)\",\n227:                 \"WHERE DATE(ts_utc,'Asia/Shanghai') >= DATE_SUB(CURRENT_DATE('Asia/Shanghai'), INTERVAL 7 DAY)\"\n228:             ),\n229:             # 模式3: WHERE DATE(ts_utc) = CURRENT_DATE()",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 229,
              "context": "227:                 \"WHERE DATE(ts_utc,'Asia/Shanghai') >= DATE_SUB(CURRENT_DATE('Asia/Shanghai'), INTERVAL 7 DAY)\"\n228:             ),\n229:             # 模式3: WHERE DATE(ts_utc) = CURRENT_DATE()\n230:             (\n231:                 r\"WHERE\\s+DATE\\(ts_utc\\)\\s*=\\s*CURRENT_DATE\\(\\)\",",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 231,
              "context": "229:             # 模式3: WHERE DATE(ts_utc) = CURRENT_DATE()\n230:             (\n231:                 r\"WHERE\\s+DATE\\(ts_utc\\)\\s*=\\s*CURRENT_DATE\\(\\)\",\n232:                 \"WHERE DATE(ts_utc) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)\"\n233:             )",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 232,
              "context": "230:             (\n231:                 r\"WHERE\\s+DATE\\(ts_utc\\)\\s*=\\s*CURRENT_DATE\\(\\)\",\n232:                 \"WHERE DATE(ts_utc) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)\"\n233:             )\n234:         ]",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 273,
              "context": "271:                     CONCAT(period, '_', src) as id,\n272:                     CURRENT_TIMESTAMP() as created_at,\n273:                     ts_utc,\n274:                     period,\n275:                     'pc28' as market,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/smart_data_repair.py",
              "line": 77,
              "context": "75:             # 检查最新数据时间\n76:             latest_query = f\"\"\"\n77:                 SELECT MAX(ts_utc) as latest_time \n78:                 FROM `{self.project_id}.{self.dataset_id}.{table_name}`\n79:                 WHERE ts_utc IS NOT NULL",
              "pattern": "SELECT.*ts_utc"
            },
            {
              "file": "test_suite/unified_repair_workflow.py",
              "line": 131,
              "context": "129:             query = f\"\"\"\n130:             SELECT \n131:                 MAX(DATE(ts_utc, 'Asia/Shanghai')) as latest_date,\n132:                 COUNT(*) as total_rows,\n133:                 CURRENT_DATE('Asia/Shanghai') as today",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/comprehensive_repair_system.py",
              "line": 95,
              "context": "93:             \n94:             # 检查时间字段\n95:             time_fields = [\"ts_utc\", \"timestamp\", \"created_at\", \"updated_at\"]\n96:             time_field = None\n97:             ",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/comprehensive_repair_system.py",
              "line": 203,
              "context": "201:                 schema = [\n202:                     bigquery.SchemaField(\"period\", \"INTEGER\"),\n203:                     bigquery.SchemaField(\"ts_utc\", \"TIMESTAMP\"),\n204:                     bigquery.SchemaField(\"p_even\", \"FLOAT\"),\n205:                     bigquery.SchemaField(\"src\", \"STRING\"),",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/comprehensive_repair_system.py",
              "line": 211,
              "context": "209:                 schema = [\n210:                     bigquery.SchemaField(\"period\", \"INTEGER\"),\n211:                     bigquery.SchemaField(\"ts_utc\", \"TIMESTAMP\"),\n212:                     bigquery.SchemaField(\"p_even\", \"FLOAT\"),\n213:                     bigquery.SchemaField(\"src\", \"STRING\"),",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/comprehensive_repair_system.py",
              "line": 228,
              "context": "226:             \n227:             # 设置分区\n228:             if any(field.name == \"ts_utc\" for field in schema):\n229:                 table.time_partitioning = bigquery.TimePartitioning(\n230:                     type_=bigquery.TimePartitioningType.DAY,",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/comprehensive_repair_system.py",
              "line": 231,
              "context": "229:                 table.time_partitioning = bigquery.TimePartitioning(\n230:                     type_=bigquery.TimePartitioningType.DAY,\n231:                     field=\"ts_utc\"\n232:                 )\n233:             ",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/intelligent_auto_repair.py",
              "line": 53,
              "context": "51:                 \"p_big\": [\"prob_big\", \"big_prob\"],\n52:                 \"p_small\": [\"prob_small\", \"small_prob\"],\n53:                 \"ts_utc\": [\"timestamp\", \"time_utc\", \"created_at\"],\n54:                 \"period\": [\"period_id\", \"game_period\", \"issue\"],\n55:                 \"id\": [\"record_id\", \"row_id\", \"pk\"]",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/intelligent_auto_repair.py",
              "line": 63,
              "context": "61:                         {\"name\": \"id\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n62:                         {\"name\": \"period\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n63:                         {\"name\": \"ts_utc\", \"type\": \"TIMESTAMP\", \"mode\": \"REQUIRED\"},\n64:                         {\"name\": \"p_even\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\"},\n65:                         {\"name\": \"source\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"}",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/intelligent_auto_repair.py",
              "line": 72,
              "context": "70:                         {\"name\": \"id\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n71:                         {\"name\": \"created_at\", \"type\": \"TIMESTAMP\", \"mode\": \"REQUIRED\"},\n72:                         {\"name\": \"ts_utc\", \"type\": \"TIMESTAMP\", \"mode\": \"REQUIRED\"},\n73:                         {\"name\": \"period\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n74:                         {\"name\": \"market\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/intelligent_auto_repair.py",
              "line": 393,
              "context": "391:         # 替换为最近3天的数据\n392:         replacement = \"\"\"DATE(DATETIME(TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), DAY, 'Asia/Shanghai'))) \n393:                         OR DATE(ts_utc, 'Asia/Shanghai') >= DATE_SUB(DATE(DATETIME(TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), DAY, 'Asia/Shanghai'))), INTERVAL 3 DAY)\"\"\"\n394:         \n395:         fixed_sql = re.sub(today_pattern, replacement, view_definition)",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/intelligent_auto_repair.py",
              "line": 401,
              "context": "399:             # 查找其他日期过滤模式并修复\n400:             date_patterns = [\n401:                 r\"DATE\\\\(ts_utc, 'Asia/Shanghai'\\\\) = CURRENT_DATE\\\\('Asia/Shanghai'\\\\)\",\n402:                 r\"DATE\\\\(ts_utc\\\\) = CURRENT_DATE\\\\(\\\\)\"\n403:             ]",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/intelligent_auto_repair.py",
              "line": 402,
              "context": "400:             date_patterns = [\n401:                 r\"DATE\\\\(ts_utc, 'Asia/Shanghai'\\\\) = CURRENT_DATE\\\\('Asia/Shanghai'\\\\)\",\n402:                 r\"DATE\\\\(ts_utc\\\\) = CURRENT_DATE\\\\(\\\\)\"\n403:             ]\n404:             ",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            },
            {
              "file": "test_suite/intelligent_auto_repair.py",
              "line": 408,
              "context": "406:                 if re.search(pattern, view_definition):\n407:                     fixed_sql = re.sub(pattern, \n408:                                      \"DATE(ts_utc, 'Asia/Shanghai') >= DATE_SUB(CURRENT_DATE('Asia/Shanghai'), INTERVAL 3 DAY)\", \n409:                                      view_definition)\n410:                     break",
              "pattern": "['\\\"]?ts_utc['\\\"]?"
            }
          ],
          "usage_patterns": [],
          "last_used": null
        },
        "legacy_format": {
          "references": [
            {
              "file": "database_table_optimizer.py",
              "line": 82,
              "context": "80:                     {'name': 'result_sum', 'type': 'INTEGER', 'nullable': False, 'usage': 'high'},\n81:                     {'name': 'ts_utc', 'type': 'TIMESTAMP', 'nullable': True, 'usage': 'medium'},  # 与timestamp重复\n82:                     {'name': 'legacy_format', 'type': 'STRING', 'nullable': True, 'usage': 'low'},  # 遗留字段\n83:                     {'name': 'data_source', 'type': 'STRING', 'nullable': True, 'usage': 'low'},\n84:                     {'name': 'validation_status', 'type': 'STRING', 'nullable': True, 'usage': 'medium'}",
              "pattern": "['\\\"]?legacy_format['\\\"]?"
            },
            {
              "file": "field_cleanup_sql_generator.py",
              "line": 58,
              "context": "56:                 {\n57:                     'table': 'draws_14w_dedup_v',\n58:                     'field': 'legacy_format',\n59:                     'reason': '遗留字段，不再使用',\n60:                     'risk': 'low',",
              "pattern": "['\\\"]?legacy_format['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 32,
              "context": "30:         self.target_fields = {\n31:             'score_ledger': ['result_digits', 'source'],\n32:             'draws_14w_dedup_v': ['ts_utc', 'legacy_format', 'data_source'],\n33:             'p_size_clean_merged_dedup_v': ['model_version', 'raw_features', 'processing_time']\n34:         }",
              "pattern": "['\\\"]?legacy_format['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 205,
              "context": "203:                     deps['transformation_logic'] = ['特征工程输出']\n204:                 \n205:                 elif field == 'legacy_format':\n206:                     deps['upstream_sources'] = ['历史数据格式']\n207:                     deps['downstream_targets'] = ['兼容性处理']",
              "pattern": "['\\\"]?legacy_format['\\\"]?"
            }
          ],
          "usage_patterns": [],
          "last_used": null
        },
        "data_source": {
          "references": [
            {
              "file": "database_table_optimizer.py",
              "line": 83,
              "context": "81:                     {'name': 'ts_utc', 'type': 'TIMESTAMP', 'nullable': True, 'usage': 'medium'},  # 与timestamp重复\n82:                     {'name': 'legacy_format', 'type': 'STRING', 'nullable': True, 'usage': 'low'},  # 遗留字段\n83:                     {'name': 'data_source', 'type': 'STRING', 'nullable': True, 'usage': 'low'},\n84:                     {'name': 'validation_status', 'type': 'STRING', 'nullable': True, 'usage': 'medium'}\n85:                 ],",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 214,
              "context": "212:             'result_digits': 'numbers',  # result_digits与numbers重复\n213:             'ts_utc': 'timestamp',       # ts_utc与timestamp重复\n214:             'data_source': 'source',     # 类似功能字段\n215:         }\n216:         ",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 223,
              "context": "221:         similar_patterns = [\n222:             ('timestamp', 'ts_utc', 'created_at'),\n223:             ('source', 'data_source'),\n224:             ('numbers', 'result_digits')\n225:         ]",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "field_cleanup_sql_generator.py",
              "line": 65,
              "context": "63:                 {\n64:                     'table': 'draws_14w_dedup_v',\n65:                     'field': 'data_source',\n66:                     'reason': '使用率极低',\n67:                     'risk': 'low',",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "historical_data_api.py",
              "line": 251,
              "context": "249:         \n250:         # 尝试多个数据源\n251:         data_sources = [\n252:             (\"BigQuery历史表\", self.fetch_from_bigquery_history),\n253:             (\"外部API\", self.fetch_from_external_api),",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "historical_data_api.py",
              "line": 257,
              "context": "255:         ]\n256:         \n257:         for source_name, fetch_func in data_sources:\n258:             try:\n259:                 logger.info(f\"尝试从 {source_name} 获取数据\")",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 32,
              "context": "30:         self.target_fields = {\n31:             'score_ledger': ['result_digits', 'source'],\n32:             'draws_14w_dedup_v': ['ts_utc', 'legacy_format', 'data_source'],\n33:             'p_size_clean_merged_dedup_v': ['model_version', 'raw_features', 'processing_time']\n34:         }",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 84,
              "context": "82:                     timestamp TEXT NOT NULL,\n83:                     record_hash TEXT NOT NULL,\n84:                     data_source TEXT NOT NULL,\n85:                     randomness_score REAL NOT NULL,\n86:                 is_blocked BOOLEAN NOT NULL,",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 443,
              "context": "441:         return len(critical_issues) == 0, randomness_tests, integrity_issues\n442:     \n443:     def protect_batch_write(self, records: List[Dict[str, Any]], data_source: str = \"unknown\") -> ProtectionReport:\n444:         \"\"\"保护批量写入\"\"\"\n445:         timestamp = datetime.now().isoformat()",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 471,
              "context": "469:                 cursor.execute(\"\"\"\n470:                     INSERT INTO protection_records \n471:                     (timestamp, record_hash, data_source, randomness_score, is_blocked, block_reason, created_at)\n472:                     VALUES (?, ?, ?, ?, ?, ?, ?)\n473:                 \"\"\", (timestamp, record_hash, data_source, randomness_score, not is_valid, block_reason, datetime.now().isoformat()))",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 473,
              "context": "471:                     (timestamp, record_hash, data_source, randomness_score, is_blocked, block_reason, created_at)\n472:                     VALUES (?, ?, ?, ?, ?, ?, ?)\n473:                 \"\"\", (timestamp, record_hash, data_source, randomness_score, not is_valid, block_reason, datetime.now().isoformat()))\n474:                 \n475:                 # 记录随机性测试结果",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 533,
              "context": "531:             # 按数据源统计\n532:             cursor.execute(\"\"\"\n533:                 SELECT data_source, COUNT(*) as total, \n534:                        SUM(CASE WHEN is_blocked = 1 THEN 1 ELSE 0 END) as blocked\n535:                 FROM protection_records ",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 536,
              "context": "534:                        SUM(CASE WHEN is_blocked = 1 THEN 1 ELSE 0 END) as blocked\n535:                 FROM protection_records \n536:                 GROUP BY data_source\n537:             \"\"\")\n538:             source_stats = {row[0]: {\"total\": row[1], \"blocked\": row[2]} for row in cursor.fetchall()}",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "historical_data_integrity_protector.py",
              "line": 533,
              "context": "531:             # 按数据源统计\n532:             cursor.execute(\"\"\"\n533:                 SELECT data_source, COUNT(*) as total, \n534:                        SUM(CASE WHEN is_blocked = 1 THEN 1 ELSE 0 END) as blocked\n535:                 FROM protection_records ",
              "pattern": "SELECT.*data_source"
            },
            {
              "file": "test_database_optimizer.py",
              "line": 65,
              "context": "63:         # 测试数据源相关字段\n64:         assert optimizer._has_similar_field('source')\n65:         assert optimizer._has_similar_field('data_source')\n66:         \n67:         # 测试独特字段",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "test_database_optimizer.py",
              "line": 354,
              "context": "352:             ('result_digits', 'numbers'),\n353:             ('ts_utc', 'timestamp'),\n354:             ('data_source', 'source')\n355:         ]\n356:         ",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 66,
              "context": "64:     \n65:     # 元数据\n66:     data_source: str = \"realtime\"  # 数据来源\n67:     fetch_timestamp: Optional[str] = None  # 获取时间戳\n68:     data_hash: Optional[str] = None  # 数据哈希",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 196,
              "context": "194:                         odd_even TEXT,\n195:                         dragon_tiger TEXT,\n196:                         data_source TEXT,\n197:                         fetch_timestamp TEXT,\n198:                         data_hash TEXT,",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 377,
              "context": "375:                 next_time=next_data.get('next_time'),\n376:                 \n377:                 data_source=\"realtime_enhanced\"\n378:             )\n379:             ",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 499,
              "context": "497:                     (draw_id, timestamp, date, result_numbers, result_sum, result_digits,\n498:                      current_time, short_issue, award_time, next_issue, next_time,\n499:                      big_small, odd_even, dragon_tiger, data_source, fetch_timestamp,\n500:                      data_hash, validation_status, created_at)\n501:                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/enhanced_realtime_service.py",
              "line": 508,
              "context": "506:                     draw_data.short_issue, draw_data.award_time, draw_data.next_issue,\n507:                     draw_data.next_time, draw_data.big_small, draw_data.odd_even,\n508:                     draw_data.dragon_tiger, draw_data.data_source, draw_data.fetch_timestamp,\n509:                     draw_data.data_hash, draw_data.validation_status,\n510:                     datetime.now(timezone.utc).isoformat()",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/test_api_integration.py",
              "line": 52,
              "context": "50:                 \"max_retries\": 3\n51:             },\n52:             \"data_source\": {\n53:                 \"use_upstream_api\": True,\n54:                 \"fallback_to_bigquery\": True,",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/test_api_integration.py",
              "line": 231,
              "context": "229:             },\n230:             'upstream_api': config['upstream_api'],\n231:             'data_source': {\n232:                 'use_upstream_api': True,\n233:                 'fallback_to_bigquery': False,",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter_client.py",
              "line": 57,
              "context": "55:         \n56:         # 数据源优先级配置\n57:         self.use_upstream_api = config.get('data_source', {}).get('use_upstream_api', True)\n58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter_client.py",
              "line": 58,
              "context": "56:         # 数据源优先级配置\n57:         self.use_upstream_api = config.get('data_source', {}).get('use_upstream_api', True)\n58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)\n60:         self.validation_enabled = config.get('data_source', {}).get('validation_enabled', True)",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter_client.py",
              "line": 59,
              "context": "57:         self.use_upstream_api = config.get('data_source', {}).get('use_upstream_api', True)\n58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)\n60:         self.validation_enabled = config.get('data_source', {}).get('validation_enabled', True)\n61:     ",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter_client.py",
              "line": 60,
              "context": "58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)\n60:         self.validation_enabled = config.get('data_source', {}).get('validation_enabled', True)\n61:     \n62:     def get_current_draw_info(self) -> Optional[Dict[str, Any]]:",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_sync_validator.py",
              "line": 449,
              "context": "447:         }\n448:     \n449:     def compare_data_sources(self, upstream_data: List[Dict[str, Any]], \n450:                            bigquery_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n451:         \"\"\"",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 19,
              "context": "17:     \"\"\"数据质量指标\"\"\"\n18:     timestamp: datetime\n19:     data_source: str\n20:     completeness_score: float  # 完整性评分 (0-1)\n21:     consistency_score: float   # 一致性评分 (0-1)",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 62,
              "context": "60:             return DataQualityMetrics(\n61:                 timestamp=datetime.now(),\n62:                 data_source='historical',\n63:                 completeness_score=0.0,\n64:                 consistency_score=0.0,",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 103,
              "context": "101:         return DataQualityMetrics(\n102:             timestamp=datetime.now(),\n103:             data_source='historical',\n104:             completeness_score=avg_completeness,\n105:             consistency_score=avg_consistency,",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 138,
              "context": "136:         return DataQualityMetrics(\n137:             timestamp=datetime.now(),\n138:             data_source=data_type,\n139:             completeness_score=completeness_score,\n140:             consistency_score=consistency_score,",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 259,
              "context": "257:     def record_quality_metrics(self, metrics: DataQualityMetrics):\n258:         \"\"\"记录数据质量指标\"\"\"\n259:         self.quality_history[metrics.data_source].append(metrics)\n260:         \n261:         # 保持历史记录在合理范围内",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 263,
              "context": "261:         # 保持历史记录在合理范围内\n262:         max_history = 100\n263:         if len(self.quality_history[metrics.data_source]) > max_history:\n264:             self.quality_history[metrics.data_source] = self.quality_history[metrics.data_source][-max_history:]\n265:         ",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 264,
              "context": "262:         max_history = 100\n263:         if len(self.quality_history[metrics.data_source]) > max_history:\n264:             self.quality_history[metrics.data_source] = self.quality_history[metrics.data_source][-max_history:]\n265:         \n266:         # 记录日志",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 264,
              "context": "262:         max_history = 100\n263:         if len(self.quality_history[metrics.data_source]) > max_history:\n264:             self.quality_history[metrics.data_source] = self.quality_history[metrics.data_source][-max_history:]\n265:         \n266:         # 记录日志",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 267,
              "context": "265:         \n266:         # 记录日志\n267:         self.logger.info(f\"数据质量评估 - {metrics.data_source}: 总分={metrics.overall_score:.3f}, \"\n268:                         f\"完整性={metrics.completeness_score:.3f}, 一致性={metrics.consistency_score:.3f}, \"\n269:                         f\"时效性={metrics.timeliness_score:.3f}, 准确性={metrics.accuracy_score:.3f}\")",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 272,
              "context": "270:         \n271:         if metrics.issues:\n272:             self.logger.warning(f\"数据质量问题 - {metrics.data_source}: {', '.join(metrics.issues)}\")\n273:     \n274:     def get_quality_summary(self, hours: int = 24) -> Dict[str, Any]:",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 279,
              "context": "277:         summary = {}\n278:         \n279:         for data_source, metrics_list in self.quality_history.items():\n280:             recent_metrics = [m for m in metrics_list if m.timestamp >= cutoff_time]\n281:             ",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/data_quality_monitor.py",
              "line": 312,
              "context": "310:                 status = 'critical'\n311:             \n312:             summary[data_source] = {\n313:                 'status': status,\n314:                 'avg_scores': avg_scores,",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter.py",
              "line": 57,
              "context": "55:         \n56:         # 数据源优先级配置\n57:         self.use_upstream_api = config.get('data_source', {}).get('use_upstream_api', True)\n58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter.py",
              "line": 58,
              "context": "56:         # 数据源优先级配置\n57:         self.use_upstream_api = config.get('data_source', {}).get('use_upstream_api', True)\n58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)\n60:         self.validation_enabled = config.get('data_source', {}).get('validation_enabled', True)",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter.py",
              "line": 59,
              "context": "57:         self.use_upstream_api = config.get('data_source', {}).get('use_upstream_api', True)\n58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)\n60:         self.validation_enabled = config.get('data_source', {}).get('validation_enabled', True)\n61:     ",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "python/integrated_data_adapter.py",
              "line": 60,
              "context": "58:         self.fallback_to_bigquery = config.get('data_source', {}).get('fallback_to_bigquery', True)\n59:         self.sync_to_bigquery = config.get('data_source', {}).get('sync_to_bigquery', True)\n60:         self.validation_enabled = config.get('data_source', {}).get('validation_enabled', True)\n61:     \n62:     def get_current_draw_info(self) -> Optional[Dict[str, Any]]:",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "test_suite/pc28_data_flow_diagram.py",
              "line": 74,
              "context": "72:             \"tables\": {},\n73:             \"total_rows\": 0,\n74:             \"data_sources\": [\"Cloud API\", \"Map API\", \"Size API\"],\n75:             \"update_frequency\": \"实时\"\n76:         }",
              "pattern": "['\\\"]?data_source['\\\"]?"
            },
            {
              "file": "test_suite/pc28_data_flow_diagram.py",
              "line": 567,
              "context": "565: {layers[\"1_raw_data\"][\"description\"]}\n566: \n567: **数据源**: {\", \".join(layers[\"1_raw_data\"][\"data_sources\"])}\n568: **更新频率**: {layers[\"1_raw_data\"][\"update_frequency\"]}\n569: **总行数**: {layers[\"1_raw_data\"][\"total_rows\"]:,}",
              "pattern": "['\\\"]?data_source['\\\"]?"
            }
          ],
          "usage_patterns": [],
          "last_used": null
        }
      },
      "p_size_clean_merged_dedup_v": {
        "model_version": {
          "references": [
            {
              "file": "database_table_optimizer.py",
              "line": 97,
              "context": "95:                     {'name': 'size_prediction', 'type': 'FLOAT64', 'nullable': True, 'usage': 'high'},\n96:                     {'name': 'confidence_score', 'type': 'FLOAT64', 'nullable': True, 'usage': 'medium'},\n97:                     {'name': 'model_version', 'type': 'STRING', 'nullable': True, 'usage': 'low'},  # 很少查询\n98:                     {'name': 'raw_features', 'type': 'JSON', 'nullable': True, 'usage': 'low'},  # 调试用，很少使用\n99:                     {'name': 'processing_time', 'type': 'FLOAT64', 'nullable': True, 'usage': 'low'},",
              "pattern": "['\\\"]?model_version['\\\"]?"
            },
            {
              "file": "field_cleanup_sql_generator.py",
              "line": 72,
              "context": "70:                 {\n71:                     'table': 'p_size_clean_merged_dedup_v',\n72:                     'field': 'model_version',\n73:                     'reason': '使用率极低',\n74:                     'risk': 'low',",
              "pattern": "['\\\"]?model_version['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 33,
              "context": "31:             'score_ledger': ['result_digits', 'source'],\n32:             'draws_14w_dedup_v': ['ts_utc', 'legacy_format', 'data_source'],\n33:             'p_size_clean_merged_dedup_v': ['model_version', 'raw_features', 'processing_time']\n34:         }\n35:     ",
              "pattern": "['\\\"]?model_version['\\\"]?"
            },
            {
              "file": "test_suite/cloud_data_pipeline_test.py",
              "line": 75,
              "context": "73:                 \"p_small\": round(random.uniform(0.3, 0.7), 4),\n74:                 \"confidence\": round(random.uniform(0.6, 0.95), 3),\n75:                 \"model_version\": \"test_v1.0\",\n76:                 \"source\": \"api_test\",\n77:                 \"features\": json.dumps({",
              "pattern": "['\\\"]?model_version['\\\"]?"
            }
          ],
          "usage_patterns": [],
          "last_used": null
        },
        "raw_features": {
          "references": [
            {
              "file": "database_table_optimizer.py",
              "line": 98,
              "context": "96:                     {'name': 'confidence_score', 'type': 'FLOAT64', 'nullable': True, 'usage': 'medium'},\n97:                     {'name': 'model_version', 'type': 'STRING', 'nullable': True, 'usage': 'low'},  # 很少查询\n98:                     {'name': 'raw_features', 'type': 'JSON', 'nullable': True, 'usage': 'low'},  # 调试用，很少使用\n99:                     {'name': 'processing_time', 'type': 'FLOAT64', 'nullable': True, 'usage': 'low'},\n100:                     {'name': 'created_at', 'type': 'TIMESTAMP', 'nullable': True, 'usage': 'medium'}",
              "pattern": "['\\\"]?raw_features['\\\"]?"
            },
            {
              "file": "field_cleanup_sql_generator.py",
              "line": 47,
              "context": "45:                 {\n46:                     'table': 'p_size_clean_merged_dedup_v',\n47:                     'field': 'raw_features',\n48:                     'reason': '大型JSON字段，使用率极低',\n49:                     'risk': 'medium',",
              "pattern": "['\\\"]?raw_features['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 33,
              "context": "31:             'score_ledger': ['result_digits', 'source'],\n32:             'draws_14w_dedup_v': ['ts_utc', 'legacy_format', 'data_source'],\n33:             'p_size_clean_merged_dedup_v': ['model_version', 'raw_features', 'processing_time']\n34:         }\n35:     ",
              "pattern": "['\\\"]?raw_features['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 200,
              "context": "198:                     deps['transformation_logic'] = ['timestamp -> UTC转换']\n199:                 \n200:                 elif field == 'raw_features':\n201:                     deps['upstream_sources'] = ['机器学习特征提取']\n202:                     deps['downstream_targets'] = ['模型调试和分析']",
              "pattern": "['\\\"]?raw_features['\\\"]?"
            }
          ],
          "usage_patterns": [],
          "last_used": null
        },
        "processing_time": {
          "references": [
            {
              "file": "data_deduplication_system.py",
              "line": 41,
              "context": "39:     unique_records: int = 0\n40:     deduplication_rate: float = 0.0\n41:     processing_time: float = 0.0\n42: \n43: @dataclass",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "data_deduplication_system.py",
              "line": 275,
              "context": "273:             return False, None\n274:         finally:\n275:             self.stats.processing_time += time.time() - start_time\n276:     \n277:     def _record_unique(self, record: Dict, record_id: str):",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "data_deduplication_system.py",
              "line": 519,
              "context": "517:     print(f\"唯一记录: {stats.unique_records}\")\n518:     print(f\"去重率: {stats.deduplication_rate:.2f}%\")\n519:     print(f\"处理时间: {stats.processing_time:.4f}秒\")\n520:     \n521:     print(\"\\n4. 重复报告:\")",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "test_ops_system.py",
              "line": 157,
              "context": "155:             'performance_metrics': {\n156:                 'avg_api_response_time': 150,\n157:                 'avg_data_processing_time': 80,\n158:                 'requests_per_hour': 1200\n159:             },",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "database_table_optimizer.py",
              "line": 99,
              "context": "97:                     {'name': 'model_version', 'type': 'STRING', 'nullable': True, 'usage': 'low'},  # 很少查询\n98:                     {'name': 'raw_features', 'type': 'JSON', 'nullable': True, 'usage': 'low'},  # 调试用，很少使用\n99:                     {'name': 'processing_time', 'type': 'FLOAT64', 'nullable': True, 'usage': 'low'},\n100:                     {'name': 'created_at', 'type': 'TIMESTAMP', 'nullable': True, 'usage': 'medium'}\n101:                 ],",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "field_cleanup_sql_generator.py",
              "line": 79,
              "context": "77:                 {\n78:                     'table': 'p_size_clean_merged_dedup_v',\n79:                     'field': 'processing_time',\n80:                     'reason': '使用率极低',\n81:                     'risk': 'low',",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "data_flow_analyzer.py",
              "line": 33,
              "context": "31:             'score_ledger': ['result_digits', 'source'],\n32:             'draws_14w_dedup_v': ['ts_utc', 'legacy_format', 'data_source'],\n33:             'p_size_clean_merged_dedup_v': ['model_version', 'raw_features', 'processing_time']\n34:         }\n35:     ",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "real_api_data_system.py",
              "line": 78,
              "context": "76:     message: str\n77:     is_duplicate: bool = False\n78:     processing_time: float = 0.0\n79: \n80: class RealAPIDataSystem:",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "real_api_data_system.py",
              "line": 438,
              "context": "436:                     message=\"记录重复，已跳过\",\n437:                     is_duplicate=True,\n438:                     processing_time=time.time() - start_time\n439:                 )\n440:             ",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "real_api_data_system.py",
              "line": 474,
              "context": "472:                 record_id=record.draw_id,\n473:                 message=\"记录写入成功\",\n474:                 processing_time=time.time() - start_time\n475:             )\n476:             ",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "real_api_data_system.py",
              "line": 485,
              "context": "483:                 record_id=record.draw_id,\n484:                 message=f\"写入失败: {str(e)}\",\n485:                 processing_time=time.time() - start_time\n486:             )\n487:     ",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "real_api_data_system.py",
              "line": 521,
              "context": "519:                 'duplicate_records': duplicate_count,\n520:                 'failed_records': len(results) - success_count,\n521:                 'processing_time': sum(r.processing_time for r in results)\n522:             }\n523:             ",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "real_api_data_system.py",
              "line": 521,
              "context": "519:                 'duplicate_records': duplicate_count,\n520:                 'failed_records': len(results) - success_count,\n521:                 'processing_time': sum(r.processing_time for r in results)\n522:             }\n523:             ",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "ops_manager.py",
              "line": 161,
              "context": "159:         metrics = health_status.performance_metrics\n160:         print(f\"  平均响应时间: {metrics['avg_api_response_time']:.0f}ms\")\n161:         print(f\"  数据处理时间: {metrics['avg_data_processing_time']:.0f}ms\")\n162:         print(f\"  每小时请求数: {metrics['requests_per_hour']:.0f}\")\n163:     ",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 50,
              "context": "48:         self.performance_metrics = {\n49:             'api_response_times': defaultdict(list),\n50:             'data_processing_times': [],\n51:             'error_counts': defaultdict(int),\n52:             'success_counts': defaultdict(int)",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 110,
              "context": "108:         performance_metrics = {\n109:             'avg_api_response_time': self._calculate_avg_response_time(),\n110:             'avg_data_processing_time': sum(self.performance_metrics['data_processing_times'][-10:]) / min(10, len(self.performance_metrics['data_processing_times'])) if self.performance_metrics['data_processing_times'] else 0,\n111:             'requests_per_hour': self._calculate_requests_per_hour(),\n112:             'error_rate': error_rate",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 110,
              "context": "108:         performance_metrics = {\n109:             'avg_api_response_time': self._calculate_avg_response_time(),\n110:             'avg_data_processing_time': sum(self.performance_metrics['data_processing_times'][-10:]) / min(10, len(self.performance_metrics['data_processing_times'])) if self.performance_metrics['data_processing_times'] else 0,\n111:             'requests_per_hour': self._calculate_requests_per_hour(),\n112:             'error_rate': error_rate",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 110,
              "context": "108:         performance_metrics = {\n109:             'avg_api_response_time': self._calculate_avg_response_time(),\n110:             'avg_data_processing_time': sum(self.performance_metrics['data_processing_times'][-10:]) / min(10, len(self.performance_metrics['data_processing_times'])) if self.performance_metrics['data_processing_times'] else 0,\n111:             'requests_per_hour': self._calculate_requests_per_hour(),\n112:             'error_rate': error_rate",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 110,
              "context": "108:         performance_metrics = {\n109:             'avg_api_response_time': self._calculate_avg_response_time(),\n110:             'avg_data_processing_time': sum(self.performance_metrics['data_processing_times'][-10:]) / min(10, len(self.performance_metrics['data_processing_times'])) if self.performance_metrics['data_processing_times'] else 0,\n111:             'requests_per_hour': self._calculate_requests_per_hour(),\n112:             'error_rate': error_rate",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 215,
              "context": "213:             self.performance_metrics['error_counts'][api_type] += 1\n214:     \n215:     def record_data_processing_time(self, processing_time_ms: float):\n216:         \"\"\"记录数据处理时间\"\"\"\n217:         self.performance_metrics['data_processing_times'].append(processing_time_ms)",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 215,
              "context": "213:             self.performance_metrics['error_counts'][api_type] += 1\n214:     \n215:     def record_data_processing_time(self, processing_time_ms: float):\n216:         \"\"\"记录数据处理时间\"\"\"\n217:         self.performance_metrics['data_processing_times'].append(processing_time_ms)",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 217,
              "context": "215:     def record_data_processing_time(self, processing_time_ms: float):\n216:         \"\"\"记录数据处理时间\"\"\"\n217:         self.performance_metrics['data_processing_times'].append(processing_time_ms)\n218:         \n219:         # 保持最近的记录",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 217,
              "context": "215:     def record_data_processing_time(self, processing_time_ms: float):\n216:         \"\"\"记录数据处理时间\"\"\"\n217:         self.performance_metrics['data_processing_times'].append(processing_time_ms)\n218:         \n219:         # 保持最近的记录",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 220,
              "context": "218:         \n219:         # 保持最近的记录\n220:         if len(self.performance_metrics['data_processing_times']) > 100:\n221:             self.performance_metrics['data_processing_times'] = self.performance_metrics['data_processing_times'][-100:]\n222:     ",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 221,
              "context": "219:         # 保持最近的记录\n220:         if len(self.performance_metrics['data_processing_times']) > 100:\n221:             self.performance_metrics['data_processing_times'] = self.performance_metrics['data_processing_times'][-100:]\n222:     \n223:     def get_dashboard_data(self, hours: int = 24) -> Dict[str, Any]:",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 221,
              "context": "219:         # 保持最近的记录\n220:         if len(self.performance_metrics['data_processing_times']) > 100:\n221:             self.performance_metrics['data_processing_times'] = self.performance_metrics['data_processing_times'][-100:]\n222:     \n223:     def get_dashboard_data(self, hours: int = 24) -> Dict[str, Any]:",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            },
            {
              "file": "python/monitoring_dashboard.py",
              "line": 322,
              "context": "320: 【性能指标】\n321: - 平均API响应时间: {current_status['performance_metrics']['avg_api_response_time']:.0f}ms\n322: - 平均数据处理时间: {current_status['performance_metrics']['avg_data_processing_time']:.0f}ms\n323: - 每小时请求数: {current_status['performance_metrics']['requests_per_hour']:.0f}\n324: ",
              "pattern": "['\\\"]?processing_time['\\\"]?"
            }
          ],
          "usage_patterns": [],
          "last_used": null
        }
      }
    },
    "api_usage": {
      "upstream_api": {
        "realtime": {
          "api_endpoint": "259 (实时开奖)",
          "total_fields": 13,
          "used_fields": [
            "codeid",
            "retdata.curent.kjtime",
            "retdata",
            "retdata.curent",
            "retdata.curent.long_issue",
            "retdata.curent.number",
            "message"
          ],
          "unused_fields": [
            "retdata.next.next_time",
            "retdata.next.next_issue",
            "curtime",
            "retdata.next",
            "retdata.curent.short_issue",
            "retdata.next.award_time"
          ],
          "usage_rate": 53.84615384615385,
          "sample_data": {
            "codeid": 10000,
            "message": "操作成功!",
            "retdata": {
              "curent": {
                "kjtime": "2025-09-29 05:59:00",
                "long_issue": "3341015",
                "short_issue": null,
                "number": [
                  "3",
                  "6",
                  "0"
                ]
              },
              "next": {
                "next_issue": 3341016,
                "next_time": "2025-09-29 06:02:30",
                "award_time": 68
              }
            },
            "curtime": 1759096882
          }
        },
        "history": {
          "api_endpoint": "260 (历史开奖)",
          "total_fields": 7,
          "used_fields": [
            "retdata[].number",
            "codeid",
            "retdata[].long_issue",
            "retdata",
            "message",
            "retdata[].kjtime"
          ],
          "unused_fields": [
            "curtime"
          ],
          "usage_rate": 85.71428571428571,
          "sample_data": {
            "codeid": 10000,
            "message": "操作成功!",
            "retdata": [
              {
                "kjtime": "2025-09-29 05:59:00",
                "number": [
                  "3",
                  "6",
                  "0"
                ],
                "long_issue": "3341015"
              },
              {
                "kjtime": "2025-09-29 05:56:30",
                "number": [
                  "3",
                  "7",
                  "8"
                ],
                "long_issue": "3341014"
              },
              {
                "kjtime": "2025-09-29 05:52:00",
                "number": [
                  "1",
                  "0",
                  "4"
                ],
                "long_issue": "3341013"
              },
              {
                "kjtime": "2025-09-29 05:49:30",
                "number": [
                  "0",
                  "8",
                  "0"
                ],
                "long_issue": "3341012"
              },
              {
                "kjtime": "2025-09-29 05:45:00",
                "number": [
                  "7",
                  "8",
                  "5"
                ],
                "long_issue": "3341011"
              }
            ],
            "curtime": 1759096882
          }
        }
      },
      "internal_processing": {},
      "downstream_usage": {}
    },
    "risk_assessment": {
      "score_ledger": {
        "result_digits": {
          "risk_factors": {
            "code_references": 100,
            "data_usage": 0,
            "api_dependency": 0,
            "business_logic": 0
          },
          "total_risk_score": 25.0,
          "risk_level": "low",
          "recommendation": "建议先归档，监控1个月后删除"
        },
        "source": {
          "risk_factors": {
            "code_references": 100,
            "data_usage": 0,
            "api_dependency": 0,
            "business_logic": 0
          },
          "total_risk_score": 25.0,
          "risk_level": "low",
          "recommendation": "建议先归档，监控1个月后删除"
        }
      },
      "draws_14w_dedup_v": {
        "ts_utc": {
          "risk_factors": {
            "code_references": 100,
            "data_usage": 0,
            "api_dependency": 0,
            "business_logic": 0
          },
          "total_risk_score": 25.0,
          "risk_level": "low",
          "recommendation": "建议先归档，监控1个月后删除"
        },
        "legacy_format": {
          "risk_factors": {
            "code_references": 40,
            "data_usage": 0,
            "api_dependency": 0,
            "business_logic": 0
          },
          "total_risk_score": 10.0,
          "risk_level": "low",
          "recommendation": "建议先归档，监控1个月后删除"
        },
        "data_source": {
          "risk_factors": {
            "code_references": 100,
            "data_usage": 0,
            "api_dependency": 0,
            "business_logic": 0
          },
          "total_risk_score": 25.0,
          "risk_level": "low",
          "recommendation": "建议先归档，监控1个月后删除"
        }
      },
      "p_size_clean_merged_dedup_v": {
        "model_version": {
          "risk_factors": {
            "code_references": 40,
            "data_usage": 0,
            "api_dependency": 0,
            "business_logic": 0
          },
          "total_risk_score": 10.0,
          "risk_level": "low",
          "recommendation": "建议先归档，监控1个月后删除"
        },
        "raw_features": {
          "risk_factors": {
            "code_references": 40,
            "data_usage": 0,
            "api_dependency": 0,
            "business_logic": 0
          },
          "total_risk_score": 10.0,
          "risk_level": "low",
          "recommendation": "建议先归档，监控1个月后删除"
        },
        "processing_time": {
          "risk_factors": {
            "code_references": 100,
            "data_usage": 0,
            "api_dependency": 0,
            "business_logic": 0
          },
          "total_risk_score": 25.0,
          "risk_level": "low",
          "recommendation": "建议先归档，监控1个月后删除"
        }
      }
    },
    "database_usage": {
      "p_size_clean_merged_dedup_v": {}
    },
    "data_dependencies": {
      "score_ledger": {
        "result_digits": {
          "upstream_sources": [
            "API响应中的numbers字段"
          ],
          "downstream_targets": [
            "可能用于结果验证"
          ],
          "transformation_logic": [
            "numbers数组的副本"
          ],
          "backup_references": []
        },
        "source": {
          "upstream_sources": [],
          "downstream_targets": [],
          "transformation_logic": [],
          "backup_references": []
        }
      },
      "draws_14w_dedup_v": {
        "ts_utc": {
          "upstream_sources": [
            "timestamp字段的UTC转换"
          ],
          "downstream_targets": [
            "时区相关查询"
          ],
          "transformation_logic": [
            "timestamp -> UTC转换"
          ],
          "backup_references": []
        },
        "legacy_format": {
          "upstream_sources": [
            "历史数据格式"
          ],
          "downstream_targets": [
            "兼容性处理"
          ],
          "transformation_logic": [
            "格式转换逻辑"
          ],
          "backup_references": []
        },
        "data_source": {
          "upstream_sources": [],
          "downstream_targets": [],
          "transformation_logic": [],
          "backup_references": []
        }
      },
      "p_size_clean_merged_dedup_v": {
        "model_version": {
          "upstream_sources": [],
          "downstream_targets": [],
          "transformation_logic": [],
          "backup_references": []
        },
        "raw_features": {
          "upstream_sources": [
            "机器学习特征提取"
          ],
          "downstream_targets": [
            "模型调试和分析"
          ],
          "transformation_logic": [
            "特征工程输出"
          ],
          "backup_references": []
        },
        "processing_time": {
          "upstream_sources": [],
          "downstream_targets": [],
          "transformation_logic": [],
          "backup_references": []
        }
      }
    },
    "cleanup_plan": {
      "immediate_safe": [],
      "archive_first": [
        {
          "table": "score_ledger",
          "field": "result_digits",
          "risk_level": "low",
          "risk_score": 25.0,
          "recommendation": "建议先归档，监控1个月后删除"
        },
        {
          "table": "score_ledger",
          "field": "source",
          "risk_level": "low",
          "risk_score": 25.0,
          "recommendation": "建议先归档，监控1个月后删除"
        },
        {
          "table": "draws_14w_dedup_v",
          "field": "ts_utc",
          "risk_level": "low",
          "risk_score": 25.0,
          "recommendation": "建议先归档，监控1个月后删除"
        },
        {
          "table": "draws_14w_dedup_v",
          "field": "legacy_format",
          "risk_level": "low",
          "risk_score": 10.0,
          "recommendation": "建议先归档，监控1个月后删除"
        },
        {
          "table": "draws_14w_dedup_v",
          "field": "data_source",
          "risk_level": "low",
          "risk_score": 25.0,
          "recommendation": "建议先归档，监控1个月后删除"
        },
        {
          "table": "p_size_clean_merged_dedup_v",
          "field": "model_version",
          "risk_level": "low",
          "risk_score": 10.0,
          "recommendation": "建议先归档，监控1个月后删除"
        },
        {
          "table": "p_size_clean_merged_dedup_v",
          "field": "raw_features",
          "risk_level": "low",
          "risk_score": 10.0,
          "recommendation": "建议先归档，监控1个月后删除"
        },
        {
          "table": "p_size_clean_merged_dedup_v",
          "field": "processing_time",
          "risk_level": "low",
          "risk_score": 25.0,
          "recommendation": "建议先归档，监控1个月后删除"
        }
      ],
      "monitor_period": [],
      "keep_indefinitely": []
    }
  }
}