#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PC28ä¼˜åŒ–åŸºçº¿åˆ›å»ºå™¨
åŸºäºæå–çš„ä¸šåŠ¡é€»è¾‘åˆ›å»ºä¼˜åŒ–åŸºçº¿å’Œæµ‹è¯•ä¿éšœ
ç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹çš„å®‰å…¨æ€§å’Œå¯è¿½æº¯æ€§
"""

import os
import json
import logging
import subprocess
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import shutil

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OptimizationBaselineCreator:
    """ä¼˜åŒ–åŸºçº¿åˆ›å»ºå™¨"""
    
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åŸºçº¿æ•°æ®
        self.baseline_data = {
            "business_logic_baseline": {},
            "test_baseline": {},
            "performance_baseline": {},
            "data_baseline": {},
            "optimization_checkpoints": []
        }
        
        # å¤‡ä»½ç›®å½•
        self.backup_dir = self.base_path / "optimization_baseline_backups" / self.timestamp
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
    def create_optimization_baseline(self) -> Dict[str, Any]:
        """åˆ›å»ºä¼˜åŒ–åŸºçº¿"""
        logger.info("ğŸ¯ å¼€å§‹åˆ›å»ºPC28ç³»ç»Ÿä¼˜åŒ–åŸºçº¿...")
        
        # 1. åŠ è½½ä¸šåŠ¡é€»è¾‘æå–æŠ¥å‘Š
        business_logic_data = self._load_business_logic_data()
        
        # 2. åˆ›å»ºä¸šåŠ¡é€»è¾‘åŸºçº¿
        self._create_business_logic_baseline(business_logic_data)
        
        # 3. åˆ›å»ºæµ‹è¯•åŸºçº¿
        self._create_test_baseline()
        
        # 4. åˆ›å»ºæ€§èƒ½åŸºçº¿
        self._create_performance_baseline()
        
        # 5. åˆ›å»ºæ•°æ®åŸºçº¿
        self._create_data_baseline()
        
        # 6. è®¾ç½®ä¼˜åŒ–æ£€æŸ¥ç‚¹
        self._setup_optimization_checkpoints()
        
        # 7. åˆ›å»ºå¤‡ä»½
        self._create_system_backup()
        
        # 8. ç”ŸæˆåŸºçº¿æŠ¥å‘Š
        baseline_report = {
            "baseline_metadata": {
                "timestamp": self.timestamp,
                "base_path": str(self.base_path),
                "backup_location": str(self.backup_dir)
            },
            "baseline_data": self.baseline_data,
            "optimization_readiness": self._assess_optimization_readiness(),
            "safety_measures": self._get_safety_measures()
        }
        
        return baseline_report
    
    def _load_business_logic_data(self) -> Dict[str, Any]:
        """åŠ è½½ä¸šåŠ¡é€»è¾‘æå–æ•°æ®"""
        logger.info("ğŸ“Š åŠ è½½ä¸šåŠ¡é€»è¾‘æå–æ•°æ®...")
        
        # æŸ¥æ‰¾æœ€æ–°çš„ä¸šåŠ¡é€»è¾‘æå–æŠ¥å‘Š
        json_files = list(self.base_path.glob("pc28_business_logic_extraction_report_*.json"))
        
        if not json_files:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä¸šåŠ¡é€»è¾‘æå–æŠ¥å‘Šï¼Œå°†åˆ›å»ºç©ºåŸºçº¿")
            return {}
        
        # é€‰æ‹©æœ€æ–°çš„æŠ¥å‘Š
        latest_file = max(json_files, key=lambda f: f.stat().st_mtime)
        logger.info(f"ğŸ“„ ä½¿ç”¨æŠ¥å‘Š: {latest_file.name}")
        
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½ä¸šåŠ¡é€»è¾‘æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def _create_business_logic_baseline(self, business_logic_data: Dict[str, Any]):
        """åˆ›å»ºä¸šåŠ¡é€»è¾‘åŸºçº¿"""
        logger.info("ğŸ’¼ åˆ›å»ºä¸šåŠ¡é€»è¾‘åŸºçº¿...")
        
        if not business_logic_data:
            logger.warning("âš ï¸ ä¸šåŠ¡é€»è¾‘æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åŸºçº¿åˆ›å»º")
            return
        
        code_logic = business_logic_data.get("code_business_logic", {})
        db_logic = business_logic_data.get("database_business_logic", {})
        
        # ç»Ÿè®¡ä¸šåŠ¡é€»è¾‘åŸºçº¿
        self.baseline_data["business_logic_baseline"] = {
            "code_logic_counts": {
                category: len(items) for category, items in code_logic.items()
            },
            "total_code_logic": sum(len(items) for items in code_logic.values()),
            "database_tables": len(db_logic.get("table_relationships", [])),
            "calculated_fields": len(db_logic.get("calculated_fields", [])),
            "critical_logic_items": self._identify_critical_logic(code_logic),
            "optimization_targets": business_logic_data.get("optimization_opportunities", {})
        }
        
        logger.info(f"âœ… ä¸šåŠ¡é€»è¾‘åŸºçº¿åˆ›å»ºå®Œæˆ: {self.baseline_data['business_logic_baseline']['total_code_logic']} ä¸ªä»£ç é€»è¾‘é¡¹")
    
    def _identify_critical_logic(self, code_logic: Dict[str, List[Dict]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«å…³é”®ä¸šåŠ¡é€»è¾‘"""
        critical_items = []
        
        # å…³é”®ä¸šåŠ¡é€»è¾‘ç±»åˆ«
        critical_categories = ["lottery_logic", "betting_logic", "payout_logic", "risk_management"]
        
        for category in critical_categories:
            items = code_logic.get(category, [])
            for item in items:
                if item.get("confidence", 0) > 0.7:  # é«˜ç½®ä¿¡åº¦çš„é€»è¾‘
                    critical_items.append({
                        "category": category,
                        "name": item.get("name", "unknown"),
                        "file_path": item.get("file_path", ""),
                        "line_number": item.get("line_number", 0),
                        "confidence": item.get("confidence", 0),
                        "type": item.get("type", "unknown")
                    })
        
        return critical_items[:50]  # é™åˆ¶ä¸ºå‰50ä¸ªæœ€å…³é”®çš„
    
    def _create_test_baseline(self):
        """åˆ›å»ºæµ‹è¯•åŸºçº¿"""
        logger.info("ğŸ§ª åˆ›å»ºæµ‹è¯•åŸºçº¿...")
        
        # æŸ¥æ‰¾æµ‹è¯•æŠ¥å‘Š
        test_reports = []
        
        # æŸ¥æ‰¾é€»è¾‘æµ‹è¯•æå–æŠ¥å‘Š
        logic_test_files = list(self.base_path.glob("pc28_logic_test_extraction_report_*.json"))
        if logic_test_files:
            latest_logic_test = max(logic_test_files, key=lambda f: f.stat().st_mtime)
            try:
                with open(latest_logic_test, 'r', encoding='utf-8') as f:
                    logic_test_data = json.load(f)
                    test_reports.append({
                        "type": "logic_tests",
                        "file": latest_logic_test.name,
                        "data": logic_test_data
                    })
            except Exception as e:
                logger.warning(f"åŠ è½½é€»è¾‘æµ‹è¯•æŠ¥å‘Šå¤±è´¥: {e}")
        
        # æŸ¥æ‰¾å…¶ä»–æµ‹è¯•ç»“æœ
        test_result_files = list(self.base_path.glob("test_results_*.json"))
        test_result_files.extend(list(self.base_path.glob("test_suite/test_results_*.json")))
        
        for test_file in test_result_files:
            try:
                with open(test_file, 'r', encoding='utf-8') as f:
                    test_data = json.load(f)
                    test_reports.append({
                        "type": "test_results",
                        "file": test_file.name,
                        "data": test_data
                    })
            except Exception as e:
                logger.warning(f"åŠ è½½æµ‹è¯•ç»“æœå¤±è´¥ {test_file}: {e}")
        
        # ç»Ÿè®¡æµ‹è¯•åŸºçº¿
        total_tests = 0
        passed_tests = 0
        test_categories = {}
        
        for report in test_reports:
            if report["type"] == "logic_tests":
                data = report["data"]
                logic_tests = data.get("logic_test_classification", {})
                for category, tests in logic_tests.items():
                    test_categories[category] = len(tests)
                    total_tests += len(tests)
            elif report["type"] == "test_results":
                data = report["data"]
                if isinstance(data, dict) and "tests" in data:
                    tests = data["tests"]
                    for test in tests:
                        total_tests += 1
                        if test.get("status") == "passed":
                            passed_tests += 1
        
        self.baseline_data["test_baseline"] = {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "test_success_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0,
            "test_categories": test_categories,
            "test_reports": [{"type": r["type"], "file": r["file"]} for r in test_reports]
        }
        
        logger.info(f"âœ… æµ‹è¯•åŸºçº¿åˆ›å»ºå®Œæˆ: {total_tests} ä¸ªæµ‹è¯•ï¼ŒæˆåŠŸç‡ {self.baseline_data['test_baseline']['test_success_rate']:.1f}%")
    
    def _create_performance_baseline(self):
        """åˆ›å»ºæ€§èƒ½åŸºçº¿"""
        logger.info("âš¡ åˆ›å»ºæ€§èƒ½åŸºçº¿...")
        
        # è¿è¡Œæ€§èƒ½æµ‹è¯•è·å–åŸºçº¿æ•°æ®
        performance_data = {}
        
        try:
            # è¿è¡Œç®€å•çš„æ€§èƒ½æµ‹è¯•
            result = subprocess.run([
                "python", "-c", 
                "import time; start=time.time(); import main; print(f'Import time: {time.time()-start:.3f}s')"
            ], capture_output=True, text=True, timeout=30, cwd=self.base_path)
            
            if result.returncode == 0:
                output = result.stdout.strip()
                if "Import time:" in output:
                    import_time = float(output.split(":")[1].replace("s", "").strip())
                    performance_data["main_import_time"] = import_time
            
        except Exception as e:
            logger.warning(f"æ€§èƒ½åŸºçº¿æµ‹è¯•å¤±è´¥: {e}")
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
        try:
            import psutil
            performance_data.update({
                "cpu_percent": psutil.cpu_percent(interval=1),
                "memory_percent": psutil.virtual_memory().percent,
                "disk_usage_percent": psutil.disk_usage('.').percent
            })
        except ImportError:
            logger.warning("psutilæœªå®‰è£…ï¼Œè·³è¿‡ç³»ç»Ÿèµ„æºç›‘æ§")
        
        self.baseline_data["performance_baseline"] = {
            "timestamp": datetime.now().isoformat(),
            "metrics": performance_data,
            "benchmark_status": "baseline_established" if performance_data else "no_data"
        }
        
        logger.info(f"âœ… æ€§èƒ½åŸºçº¿åˆ›å»ºå®Œæˆ: {len(performance_data)} ä¸ªæŒ‡æ ‡")
    
    def _create_data_baseline(self):
        """åˆ›å»ºæ•°æ®åŸºçº¿"""
        logger.info("ğŸ—„ï¸ åˆ›å»ºæ•°æ®åŸºçº¿...")
        
        # ç»Ÿè®¡æ–‡ä»¶å’Œç›®å½•ä¿¡æ¯
        file_stats = {
            "python_files": len(list(self.base_path.rglob("*.py"))),
            "json_files": len(list(self.base_path.rglob("*.json"))),
            "sql_files": len(list(self.base_path.rglob("*.sql"))),
            "md_files": len(list(self.base_path.rglob("*.md"))),
            "total_files": len(list(self.base_path.rglob("*"))),
        }
        
        # è®¡ç®—ä»£ç è¡Œæ•°
        total_lines = 0
        python_lines = 0
        
        for py_file in self.base_path.rglob("*.py"):
            # è·³è¿‡è™šæ‹Ÿç¯å¢ƒå’Œç¼“å­˜ç›®å½•
            if any(part in str(py_file) for part in ["venv", "__pycache__", ".git"]):
                continue
            
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    lines = len(f.readlines())
                    python_lines += lines
                    total_lines += lines
            except Exception:
                continue
        
        self.baseline_data["data_baseline"] = {
            "file_statistics": file_stats,
            "code_statistics": {
                "total_lines": total_lines,
                "python_lines": python_lines
            },
            "directory_structure": self._get_directory_structure()
        }
        
        logger.info(f"âœ… æ•°æ®åŸºçº¿åˆ›å»ºå®Œæˆ: {file_stats['python_files']} ä¸ªPythonæ–‡ä»¶ï¼Œ{python_lines} è¡Œä»£ç ")
    
    def _get_directory_structure(self) -> Dict[str, int]:
        """è·å–ç›®å½•ç»“æ„ç»Ÿè®¡"""
        structure = {}
        
        for item in self.base_path.iterdir():
            if item.is_dir() and not item.name.startswith('.'):
                file_count = len(list(item.rglob("*")))
                structure[item.name] = file_count
        
        return structure
    
    def _setup_optimization_checkpoints(self):
        """è®¾ç½®ä¼˜åŒ–æ£€æŸ¥ç‚¹"""
        logger.info("ğŸ¯ è®¾ç½®ä¼˜åŒ–æ£€æŸ¥ç‚¹...")
        
        checkpoints = [
            {
                "checkpoint_id": "pre_optimization",
                "name": "ä¼˜åŒ–å‰æ£€æŸ¥ç‚¹",
                "description": "ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿç¨³å®š",
                "criteria": [
                    "æ‰€æœ‰å…³é”®ä¸šåŠ¡æµ‹è¯•å¿…é¡»é€šè¿‡",
                    "ç³»ç»Ÿæ€§èƒ½åŸºçº¿å·²å»ºç«‹",
                    "å®Œæ•´å¤‡ä»½å·²åˆ›å»º",
                    "å›æ»šæœºåˆ¶å·²å‡†å¤‡"
                ],
                "status": "pending"
            },
            {
                "checkpoint_id": "phase1_validation",
                "name": "é˜¶æ®µ1éªŒè¯æ£€æŸ¥ç‚¹",
                "description": "ä»£ç é€»è¾‘ä¼˜åŒ–åçš„éªŒè¯",
                "criteria": [
                    "å†—ä½™ä»£ç æ¸…ç†å®Œæˆ",
                    "æ‰€æœ‰æµ‹è¯•ä»ç„¶é€šè¿‡",
                    "æ€§èƒ½æ²¡æœ‰é€€åŒ–",
                    "ä¸šåŠ¡é€»è¾‘å®Œæ•´æ€§ä¿æŒ"
                ],
                "status": "pending"
            },
            {
                "checkpoint_id": "phase2_validation",
                "name": "é˜¶æ®µ2éªŒè¯æ£€æŸ¥ç‚¹",
                "description": "æ•°æ®åº“ä¼˜åŒ–åçš„éªŒè¯",
                "criteria": [
                    "æ•°æ®åº“ç»“æ„ä¼˜åŒ–å®Œæˆ",
                    "æŸ¥è¯¢æ€§èƒ½æå‡éªŒè¯",
                    "æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡",
                    "ä¸šåŠ¡åŠŸèƒ½æ­£å¸¸è¿è¡Œ"
                ],
                "status": "pending"
            },
            {
                "checkpoint_id": "final_validation",
                "name": "æœ€ç»ˆéªŒè¯æ£€æŸ¥ç‚¹",
                "description": "å…¨é¢ä¼˜åŒ–å®Œæˆåçš„æœ€ç»ˆéªŒè¯",
                "criteria": [
                    "æ‰€æœ‰ä¼˜åŒ–ç›®æ ‡è¾¾æˆ",
                    "ç³»ç»Ÿæ€§èƒ½æ˜¾è‘—æå‡",
                    "ä¸šåŠ¡é€»è¾‘å®Œå…¨æ­£å¸¸",
                    "ç›‘æ§ç³»ç»Ÿæ­£å¸¸è¿è¡Œ"
                ],
                "status": "pending"
            }
        ]
        
        self.baseline_data["optimization_checkpoints"] = checkpoints
        
        logger.info(f"âœ… ä¼˜åŒ–æ£€æŸ¥ç‚¹è®¾ç½®å®Œæˆ: {len(checkpoints)} ä¸ªæ£€æŸ¥ç‚¹")
    
    def _create_system_backup(self):
        """åˆ›å»ºç³»ç»Ÿå¤‡ä»½"""
        logger.info("ğŸ’¾ åˆ›å»ºç³»ç»Ÿå¤‡ä»½...")
        
        # å¤‡ä»½å…³é”®æ–‡ä»¶
        critical_files = [
            "main.py",
            "models.py", 
            "requirements.txt",
            "app.yaml"
        ]
        
        # å¤‡ä»½å…³é”®ç›®å½•
        critical_dirs = [
            "sql",
            "config",
            "test_suite"
        ]
        
        backup_summary = {
            "files_backed_up": [],
            "dirs_backed_up": [],
            "backup_size": 0
        }
        
        # å¤‡ä»½æ–‡ä»¶
        for file_name in critical_files:
            file_path = self.base_path / file_name
            if file_path.exists():
                backup_path = self.backup_dir / file_name
                try:
                    shutil.copy2(file_path, backup_path)
                    backup_summary["files_backed_up"].append(file_name)
                    backup_summary["backup_size"] += file_path.stat().st_size
                except Exception as e:
                    logger.warning(f"å¤‡ä»½æ–‡ä»¶å¤±è´¥ {file_name}: {e}")
        
        # å¤‡ä»½ç›®å½•
        for dir_name in critical_dirs:
            dir_path = self.base_path / dir_name
            if dir_path.exists() and dir_path.is_dir():
                backup_path = self.backup_dir / dir_name
                try:
                    shutil.copytree(dir_path, backup_path, dirs_exist_ok=True)
                    backup_summary["dirs_backed_up"].append(dir_name)
                    # è®¡ç®—ç›®å½•å¤§å°
                    for file_path in backup_path.rglob("*"):
                        if file_path.is_file():
                            backup_summary["backup_size"] += file_path.stat().st_size
                except Exception as e:
                    logger.warning(f"å¤‡ä»½ç›®å½•å¤±è´¥ {dir_name}: {e}")
        
        # ä¿å­˜å¤‡ä»½æ¸…å•
        backup_manifest = {
            "backup_timestamp": self.timestamp,
            "backup_location": str(self.backup_dir),
            "backup_summary": backup_summary
        }
        
        manifest_path = self.backup_dir / "backup_manifest.json"
        with open(manifest_path, 'w', encoding='utf-8') as f:
            json.dump(backup_manifest, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ç³»ç»Ÿå¤‡ä»½å®Œæˆ: {len(backup_summary['files_backed_up'])} ä¸ªæ–‡ä»¶ï¼Œ{len(backup_summary['dirs_backed_up'])} ä¸ªç›®å½•")
    
    def _assess_optimization_readiness(self) -> Dict[str, Any]:
        """è¯„ä¼°ä¼˜åŒ–å‡†å¤‡æƒ…å†µ"""
        logger.info("ğŸ“‹ è¯„ä¼°ä¼˜åŒ–å‡†å¤‡æƒ…å†µ...")
        
        readiness_score = 0
        max_score = 100
        
        readiness_factors = []
        
        # ä¸šåŠ¡é€»è¾‘åŸºçº¿ (30åˆ†)
        if self.baseline_data["business_logic_baseline"]:
            logic_score = min(30, self.baseline_data["business_logic_baseline"]["total_code_logic"] / 100 * 30)
            readiness_score += logic_score
            readiness_factors.append({
                "factor": "ä¸šåŠ¡é€»è¾‘åŸºçº¿",
                "score": logic_score,
                "max_score": 30,
                "status": "å®Œæˆ" if logic_score > 20 else "éƒ¨åˆ†å®Œæˆ"
            })
        
        # æµ‹è¯•åŸºçº¿ (25åˆ†)
        test_baseline = self.baseline_data["test_baseline"]
        if test_baseline["total_tests"] > 0:
            test_score = min(25, test_baseline["test_success_rate"] / 100 * 25)
            readiness_score += test_score
            readiness_factors.append({
                "factor": "æµ‹è¯•åŸºçº¿",
                "score": test_score,
                "max_score": 25,
                "status": "å®Œæˆ" if test_score > 20 else "éœ€è¦æ”¹è¿›"
            })
        
        # æ€§èƒ½åŸºçº¿ (20åˆ†)
        perf_baseline = self.baseline_data["performance_baseline"]
        if perf_baseline["benchmark_status"] == "baseline_established":
            perf_score = 20
            readiness_score += perf_score
            readiness_factors.append({
                "factor": "æ€§èƒ½åŸºçº¿",
                "score": perf_score,
                "max_score": 20,
                "status": "å®Œæˆ"
            })
        
        # æ•°æ®åŸºçº¿ (15åˆ†)
        data_baseline = self.baseline_data["data_baseline"]
        if data_baseline["code_statistics"]["python_lines"] > 0:
            data_score = 15
            readiness_score += data_score
            readiness_factors.append({
                "factor": "æ•°æ®åŸºçº¿",
                "score": data_score,
                "max_score": 15,
                "status": "å®Œæˆ"
            })
        
        # æ£€æŸ¥ç‚¹è®¾ç½® (10åˆ†)
        if self.baseline_data["optimization_checkpoints"]:
            checkpoint_score = 10
            readiness_score += checkpoint_score
            readiness_factors.append({
                "factor": "æ£€æŸ¥ç‚¹è®¾ç½®",
                "score": checkpoint_score,
                "max_score": 10,
                "status": "å®Œæˆ"
            })
        
        # ç¡®å®šå‡†å¤‡çŠ¶æ€
        if readiness_score >= 80:
            readiness_status = "ready"
        elif readiness_score >= 60:
            readiness_status = "mostly_ready"
        elif readiness_score >= 40:
            readiness_status = "partially_ready"
        else:
            readiness_status = "not_ready"
        
        return {
            "readiness_score": readiness_score,
            "max_score": max_score,
            "readiness_percentage": (readiness_score / max_score) * 100,
            "readiness_status": readiness_status,
            "readiness_factors": readiness_factors,
            "recommendations": self._get_readiness_recommendations(readiness_status, readiness_factors)
        }
    
    def _get_readiness_recommendations(self, status: str, factors: List[Dict]) -> List[str]:
        """è·å–å‡†å¤‡æƒ…å†µå»ºè®®"""
        recommendations = []
        
        if status == "ready":
            recommendations.append("âœ… ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œä¼˜åŒ–")
            recommendations.append("å»ºè®®æŒ‰è®¡åˆ’æ‰§è¡Œé˜¶æ®µ1ä»£ç é€»è¾‘ä¼˜åŒ–")
        elif status == "mostly_ready":
            recommendations.append("âš ï¸ ç³»ç»ŸåŸºæœ¬å‡†å¤‡å°±ç»ªï¼Œå»ºè®®å®Œå–„ä»¥ä¸‹æ–¹é¢åå¼€å§‹ä¼˜åŒ–")
            for factor in factors:
                if factor["score"] < factor["max_score"] * 0.8:
                    recommendations.append(f"- æ”¹è¿›{factor['factor']}ï¼ˆå½“å‰å¾—åˆ†: {factor['score']:.1f}/{factor['max_score']}ï¼‰")
        else:
            recommendations.append("âŒ ç³»ç»Ÿå°šæœªå‡†å¤‡å¥½è¿›è¡Œä¼˜åŒ–")
            recommendations.append("å¿…é¡»å®Œæˆä»¥ä¸‹å‡†å¤‡å·¥ä½œ:")
            for factor in factors:
                if factor["score"] < factor["max_score"] * 0.5:
                    recommendations.append(f"- å®Œå–„{factor['factor']}ï¼ˆå½“å‰å¾—åˆ†: {factor['score']:.1f}/{factor['max_score']}ï¼‰")
        
        return recommendations
    
    def _get_safety_measures(self) -> List[Dict[str, str]]:
        """è·å–å®‰å…¨æªæ–½"""
        return [
            {
                "measure": "å®Œæ•´å¤‡ä»½",
                "description": "æ‰€æœ‰å…³é”®æ–‡ä»¶å’Œç›®å½•å·²å¤‡ä»½",
                "location": str(self.backup_dir)
            },
            {
                "measure": "æµ‹è¯•ä¿éšœ",
                "description": "å»ºç«‹äº†å®Œæ•´çš„æµ‹è¯•åŸºçº¿ï¼Œç¡®ä¿ä¼˜åŒ–ååŠŸèƒ½æ­£å¸¸",
                "coverage": f"{self.baseline_data['test_baseline']['total_tests']} ä¸ªæµ‹è¯•"
            },
            {
                "measure": "æ€§èƒ½ç›‘æ§",
                "description": "å»ºç«‹äº†æ€§èƒ½åŸºçº¿ï¼Œå¯ç›‘æ§ä¼˜åŒ–æ•ˆæœ",
                "metrics": f"{len(self.baseline_data['performance_baseline']['metrics'])} ä¸ªæŒ‡æ ‡"
            },
            {
                "measure": "æ£€æŸ¥ç‚¹éªŒè¯",
                "description": "è®¾ç½®äº†å¤šä¸ªéªŒè¯æ£€æŸ¥ç‚¹ï¼Œç¡®ä¿æ¯ä¸ªé˜¶æ®µçš„å®‰å…¨æ€§",
                "checkpoints": f"{len(self.baseline_data['optimization_checkpoints'])} ä¸ªæ£€æŸ¥ç‚¹"
            },
            {
                "measure": "å›æ»šæœºåˆ¶",
                "description": "å¯ä»¥å¿«é€Ÿå›æ»šåˆ°ä¼˜åŒ–å‰çš„çŠ¶æ€",
                "method": "åŸºäºå¤‡ä»½çš„å®Œæ•´å›æ»š"
            }
        ]
    
    def save_baseline_report(self, report: Dict[str, Any]):
        """ä¿å­˜åŸºçº¿æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_file = f"pc28_optimization_baseline_report_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_file = f"pc28_optimization_baseline_report_{timestamp}.md"
        self._generate_markdown_report(report, md_file)
        
        logger.info(f"ğŸ“„ ä¼˜åŒ–åŸºçº¿æŠ¥å‘Šå·²ä¿å­˜:")
        logger.info(f"  JSON: {json_file}")
        logger.info(f"  Markdown: {md_file}")
        
        return json_file, md_file
    
    def _generate_markdown_report(self, report: Dict[str, Any], file_path: str):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        
        metadata = report["baseline_metadata"]
        baseline_data = report["baseline_data"]
        readiness = report["optimization_readiness"]
        safety_measures = report["safety_measures"]
        
        content = f"""# PC28ä¼˜åŒ–åŸºçº¿æŠ¥å‘Š

## ğŸ“Š åŸºçº¿æ¦‚è§ˆ

**åˆ›å»ºæ—¶é—´**: {metadata['timestamp']}
**åŸºç¡€è·¯å¾„**: {metadata['base_path']}
**å¤‡ä»½ä½ç½®**: {metadata['backup_location']}

## ğŸ’¼ ä¸šåŠ¡é€»è¾‘åŸºçº¿

"""
        
        logic_baseline = baseline_data["business_logic_baseline"]
        if logic_baseline:
            content += f"""
**ä»£ç é€»è¾‘æ€»æ•°**: {logic_baseline['total_code_logic']} ä¸ª
**æ•°æ®åº“è¡¨æ•°**: {logic_baseline['database_tables']} ä¸ª
**è®¡ç®—å­—æ®µæ•°**: {logic_baseline['calculated_fields']} ä¸ª

### ä»£ç é€»è¾‘åˆ†å¸ƒ
"""
            for category, count in logic_baseline["code_logic_counts"].items():
                category_name = category.replace("_", " ").title()
                content += f"- **{category_name}**: {count} ä¸ª\n"
            
            content += f"""
### å…³é”®ä¸šåŠ¡é€»è¾‘
**è¯†åˆ«æ•°é‡**: {len(logic_baseline['critical_logic_items'])} ä¸ª

"""
            for item in logic_baseline["critical_logic_items"][:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                content += f"- `{item['name']}` ({item['category']}, ç½®ä¿¡åº¦: {item['confidence']:.2f})\n"
        
        content += f"""
## ğŸ§ª æµ‹è¯•åŸºçº¿

"""
        
        test_baseline = baseline_data["test_baseline"]
        content += f"""
**æµ‹è¯•æ€»æ•°**: {test_baseline['total_tests']} ä¸ª
**é€šè¿‡æµ‹è¯•**: {test_baseline['passed_tests']} ä¸ª
**æˆåŠŸç‡**: {test_baseline['test_success_rate']:.1f}%

### æµ‹è¯•åˆ†ç±»
"""
        
        for category, count in test_baseline["test_categories"].items():
            category_name = category.replace("_", " ").title()
            content += f"- **{category_name}**: {count} ä¸ª\n"
        
        content += f"""
## âš¡ æ€§èƒ½åŸºçº¿

"""
        
        perf_baseline = baseline_data["performance_baseline"]
        content += f"""
**åŸºçº¿çŠ¶æ€**: {perf_baseline['benchmark_status']}
**æµ‹è¯•æ—¶é—´**: {perf_baseline['timestamp']}

### æ€§èƒ½æŒ‡æ ‡
"""
        
        for metric, value in perf_baseline["metrics"].items():
            content += f"- **{metric}**: {value}\n"
        
        content += f"""
## ğŸ—„ï¸ æ•°æ®åŸºçº¿

"""
        
        data_baseline = baseline_data["data_baseline"]
        file_stats = data_baseline["file_statistics"]
        code_stats = data_baseline["code_statistics"]
        
        content += f"""
### æ–‡ä»¶ç»Ÿè®¡
- **Pythonæ–‡ä»¶**: {file_stats['python_files']} ä¸ª
- **JSONæ–‡ä»¶**: {file_stats['json_files']} ä¸ª
- **SQLæ–‡ä»¶**: {file_stats['sql_files']} ä¸ª
- **Markdownæ–‡ä»¶**: {file_stats['md_files']} ä¸ª
- **æ€»æ–‡ä»¶æ•°**: {file_stats['total_files']} ä¸ª

### ä»£ç ç»Ÿè®¡
- **æ€»ä»£ç è¡Œæ•°**: {code_stats['total_lines']:,} è¡Œ
- **Pythonä»£ç è¡Œæ•°**: {code_stats['python_lines']:,} è¡Œ

### ç›®å½•ç»“æ„
"""
        
        for dir_name, file_count in data_baseline["directory_structure"].items():
            content += f"- **{dir_name}**: {file_count} ä¸ªæ–‡ä»¶\n"
        
        content += f"""
## ğŸ¯ ä¼˜åŒ–æ£€æŸ¥ç‚¹

"""
        
        for checkpoint in baseline_data["optimization_checkpoints"]:
            status_emoji = {"pending": "â³", "completed": "âœ…", "failed": "âŒ"}.get(checkpoint["status"], "â“")
            content += f"""
### {checkpoint['name']} {status_emoji}
**ID**: {checkpoint['checkpoint_id']}
**æè¿°**: {checkpoint['description']}
**çŠ¶æ€**: {checkpoint['status']}

**éªŒè¯æ ‡å‡†**:
"""
            for criterion in checkpoint["criteria"]:
                content += f"- {criterion}\n"
        
        content += f"""
## ğŸ“‹ ä¼˜åŒ–å‡†å¤‡æƒ…å†µ

**å‡†å¤‡å¾—åˆ†**: {readiness['readiness_score']:.1f}/{readiness['max_score']} ({readiness['readiness_percentage']:.1f}%)
**å‡†å¤‡çŠ¶æ€**: {readiness['readiness_status']}

### å‡†å¤‡å› ç´ è¯„ä¼°
"""
        
        for factor in readiness["readiness_factors"]:
            status_emoji = {"å®Œæˆ": "âœ…", "éƒ¨åˆ†å®Œæˆ": "âš ï¸", "éœ€è¦æ”¹è¿›": "âŒ"}.get(factor["status"], "â“")
            content += f"""
#### {factor['factor']} {status_emoji}
- **å¾—åˆ†**: {factor['score']:.1f}/{factor['max_score']}
- **çŠ¶æ€**: {factor['status']}
"""
        
        content += f"""
### å»ºè®®
"""
        
        for recommendation in readiness["recommendations"]:
            content += f"{recommendation}\n"
        
        content += f"""
## ğŸ›¡ï¸ å®‰å…¨æªæ–½

"""
        
        for measure in safety_measures:
            content += f"""
### {measure['measure']}
**æè¿°**: {measure['description']}
"""
            if 'location' in measure:
                content += f"**ä½ç½®**: {measure['location']}\n"
            if 'coverage' in measure:
                content += f"**è¦†ç›–**: {measure['coverage']}\n"
            if 'metrics' in measure:
                content += f"**æŒ‡æ ‡**: {measure['metrics']}\n"
            if 'checkpoints' in measure:
                content += f"**æ£€æŸ¥ç‚¹**: {measure['checkpoints']}\n"
            if 'method' in measure:
                content += f"**æ–¹æ³•**: {measure['method']}\n"
        
        content += f"""
## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **éªŒè¯å‡†å¤‡æƒ…å†µ** - ç¡®è®¤æ‰€æœ‰åŸºçº¿æ•°æ®å®Œæ•´ä¸”å‡†ç¡®
2. **è¿è¡ŒåŸºçº¿æµ‹è¯•** - æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼Œç¡®ä¿100%é€šè¿‡
3. **å¼€å§‹é˜¶æ®µ1ä¼˜åŒ–** - å¤„ç†429ä¸ªå†—ä½™é€»è¾‘é¡¹
4. **æŒç»­ç›‘æ§** - åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æŒç»­ç›‘æ§ç³»ç»ŸçŠ¶æ€
5. **æ£€æŸ¥ç‚¹éªŒè¯** - åœ¨æ¯ä¸ªé˜¶æ®µå®Œæˆåè¿›è¡Œæ£€æŸ¥ç‚¹éªŒè¯

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}
**ç‰ˆæœ¬**: 1.0
"""
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ PC28ä¼˜åŒ–åŸºçº¿åˆ›å»ºå™¨")
    print("=" * 60)
    print("ğŸ“‹ ç›®æ ‡ï¼šä¸ºç³»ç»Ÿä¼˜åŒ–åˆ›å»ºå®Œæ•´çš„åŸºçº¿å’Œå®‰å…¨ä¿éšœ")
    print("ğŸ›¡ï¸ èŒƒå›´ï¼šä¸šåŠ¡é€»è¾‘ã€æµ‹è¯•ã€æ€§èƒ½ã€æ•°æ®åŸºçº¿ + å®‰å…¨æªæ–½")
    print("=" * 60)
    
    creator = OptimizationBaselineCreator()
    
    try:
        # åˆ›å»ºä¼˜åŒ–åŸºçº¿
        baseline_report = creator.create_optimization_baseline()
        
        # ä¿å­˜æŠ¥å‘Š
        json_file, md_file = creator.save_baseline_report(baseline_report)
        
        # æ˜¾ç¤ºæ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“Š ä¼˜åŒ–åŸºçº¿åˆ›å»ºæ‘˜è¦")
        print("=" * 60)
        
        baseline_data = baseline_report["baseline_data"]
        readiness = baseline_report["optimization_readiness"]
        
        logic_baseline = baseline_data["business_logic_baseline"]
        test_baseline = baseline_data["test_baseline"]
        
        print(f"\nğŸ’¼ ä¸šåŠ¡é€»è¾‘åŸºçº¿:")
        if logic_baseline:
            print(f"   ä»£ç é€»è¾‘: {logic_baseline['total_code_logic']} ä¸ª")
            print(f"   æ•°æ®åº“è¡¨: {logic_baseline['database_tables']} ä¸ª")
            print(f"   å…³é”®é€»è¾‘: {len(logic_baseline['critical_logic_items'])} ä¸ª")
        else:
            print("   æœªå»ºç«‹ä¸šåŠ¡é€»è¾‘åŸºçº¿")
        
        print(f"\nğŸ§ª æµ‹è¯•åŸºçº¿:")
        print(f"   æ€»æµ‹è¯•æ•°: {test_baseline['total_tests']} ä¸ª")
        print(f"   æˆåŠŸç‡: {test_baseline['test_success_rate']:.1f}%")
        
        print(f"\nğŸ“‹ ä¼˜åŒ–å‡†å¤‡æƒ…å†µ:")
        print(f"   å‡†å¤‡å¾—åˆ†: {readiness['readiness_score']:.1f}/{readiness['max_score']} ({readiness['readiness_percentage']:.1f}%)")
        print(f"   å‡†å¤‡çŠ¶æ€: {readiness['readiness_status']}")
        
        print(f"\nğŸ›¡ï¸ å®‰å…¨æªæ–½:")
        safety_measures = baseline_report["safety_measures"]
        for measure in safety_measures:
            print(f"   âœ“ {measure['measure']}")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {md_file}")
        
        if readiness['readiness_status'] == 'ready':
            print("\nğŸ‰ ä¼˜åŒ–åŸºçº¿åˆ›å»ºå®Œæˆï¼ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œå®‰å…¨ä¼˜åŒ–ã€‚")
        else:
            print(f"\nâš ï¸ ä¼˜åŒ–åŸºçº¿åˆ›å»ºå®Œæˆï¼Œä½†ç³»ç»Ÿå‡†å¤‡æƒ…å†µä¸º: {readiness['readiness_status']}")
            print("å»ºè®®å®Œå–„å‡†å¤‡å·¥ä½œåå†å¼€å§‹ä¼˜åŒ–ã€‚")
        
    except Exception as e:
        logger.error(f"ä¼˜åŒ–åŸºçº¿åˆ›å»ºå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()