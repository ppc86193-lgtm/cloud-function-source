"""
æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å’Œå®¡è®¡ç³»ç»Ÿ
ç”¨äºå®šæœŸéªŒè¯ PC28 ç³»ç»Ÿå’Œ Supabase ä¹‹é—´çš„æ•°æ®ä¸€è‡´æ€§
"""

import os
import logging
import sqlite3
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import json
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics

from supabase_config import get_supabase_client
from supabase_sync_manager import sync_manager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataAuditSystem:
    """æ•°æ®å®¡è®¡ç³»ç»Ÿ"""
    
    def __init__(self, sqlite_db_path: Optional[str] = None):
        self.sqlite_db_path = sqlite_db_path or os.getenv('SQLITE_DB_PATH', 'pc28_data.db')
        self.supabase_client = get_supabase_client(use_service_role=True)
        
        # å®¡è®¡é…ç½®
        self.audit_tables = [
            'lab_push_candidates_v2',
            'cloud_pred_today_norm', 
            'signal_pool_union_v3',
            'p_size_clean_merged_dedup_v',
            'draws_14w_dedup_v',
            'score_ledger'
        ]
        
        self.audit_history = []
        self.max_history_records = 50
        
        # å®¡è®¡é˜ˆå€¼
        self.integrity_threshold = 0.95  # å®Œæ•´æ€§å¾—åˆ†é˜ˆå€¼
        self.consistency_threshold = 0.90  # ä¸€è‡´æ€§å¾—åˆ†é˜ˆå€¼
        
    def get_sqlite_connection(self) -> sqlite3.Connection:
        """è·å– SQLite æ•°æ®åº“è¿æ¥"""
        try:
            conn = sqlite3.connect(self.sqlite_db_path)
            conn.row_factory = sqlite3.Row
            return conn
        except Exception as e:
            logger.error(f"Failed to connect to SQLite database: {e}")
            raise
    
    def count_records(self, table_name: str, source: str = 'both') -> Dict[str, int]:
        """
        ç»Ÿè®¡è¡¨è®°å½•æ•°
        
        Args:
            table_name: è¡¨å
            source: æ•°æ®æº ('sqlite', 'supabase', 'both')
            
        Returns:
            è®°å½•æ•°ç»Ÿè®¡
        """
        counts = {}
        
        try:
            if source in ['sqlite', 'both']:
                with self.get_sqlite_connection() as conn:
                    cursor = conn.execute(f"SELECT COUNT(*) FROM {table_name}")
                    counts['sqlite'] = cursor.fetchone()[0]
            
            if source in ['supabase', 'both']:
                response = self.supabase_client.table(table_name).select('id', count='exact').execute()
                counts['supabase'] = response.count or 0
                
        except Exception as e:
            logger.error(f"Failed to count records for {table_name}: {e}")
            if source == 'sqlite' or source == 'both':
                counts['sqlite'] = -1
            if source == 'supabase' or source == 'both':
                counts['supabase'] = -1
        
        return counts
    
    def check_data_integrity(self, table_name: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        
        Args:
            table_name: è¡¨å
            
        Returns:
            å®Œæ•´æ€§æ£€æŸ¥ç»“æœ
        """
        start_time = datetime.now()
        
        try:
            # ç»Ÿè®¡è®°å½•æ•°
            counts = self.count_records(table_name)
            
            if counts.get('sqlite', -1) == -1 or counts.get('supabase', -1) == -1:
                return {
                    'table_name': table_name,
                    'status': 'error',
                    'error_message': 'Failed to retrieve record counts',
                    'timestamp': start_time.isoformat()
                }
            
            sqlite_count = counts['sqlite']
            supabase_count = counts['supabase']
            
            # è®¡ç®—å®Œæ•´æ€§å¾—åˆ†
            if sqlite_count == 0 and supabase_count == 0:
                integrity_score = 1.0
                missing_records = 0
            elif sqlite_count == 0:
                integrity_score = 0.0
                missing_records = supabase_count
            else:
                missing_records = abs(sqlite_count - supabase_count)
                integrity_score = 1.0 - (missing_records / sqlite_count)
            
            # æ£€æŸ¥æ•°æ®è´¨é‡
            quality_issues = self._check_data_quality(table_name)
            
            result = {
                'table_name': table_name,
                'sqlite_count': sqlite_count,
                'supabase_count': supabase_count,
                'missing_records': missing_records,
                'integrity_score': round(integrity_score, 4),
                'quality_issues': quality_issues,
                'status': 'passed' if integrity_score >= self.integrity_threshold else 'failed',
                'timestamp': start_time.isoformat(),
                'duration_seconds': (datetime.now() - start_time).total_seconds()
            }
            
            logger.info(f"Integrity check for {table_name}: {result['status']} "
                       f"(score: {integrity_score:.3f}, missing: {missing_records})")
            
            return result
            
        except Exception as e:
            error_msg = f"Integrity check failed for {table_name}: {str(e)}"
            logger.error(error_msg)
            
            return {
                'table_name': table_name,
                'status': 'error',
                'error_message': error_msg,
                'timestamp': start_time.isoformat(),
                'duration_seconds': (datetime.now() - start_time).total_seconds()
            }
    
    def _check_data_quality(self, table_name: str) -> List[Dict[str, Any]]:
        """
        æ£€æŸ¥æ•°æ®è´¨é‡é—®é¢˜
        
        Args:
            table_name: è¡¨å
            
        Returns:
            è´¨é‡é—®é¢˜åˆ—è¡¨
        """
        issues = []
        
        try:
            # æ£€æŸ¥ Supabase ä¸­çš„æ•°æ®è´¨é‡
            response = self.supabase_client.table(table_name).select('*').limit(1000).execute()
            
            if not response.data:
                return issues
            
            data = response.data
            total_records = len(data)
            
            # æ£€æŸ¥ç©ºå€¼
            null_counts = {}
            for record in data:
                for field, value in record.items():
                    if value is None or value == '':
                        null_counts[field] = null_counts.get(field, 0) + 1
            
            # æŠ¥å‘Šé«˜ç©ºå€¼ç‡çš„å­—æ®µ
            for field, null_count in null_counts.items():
                null_rate = null_count / total_records
                if null_rate > 0.1:  # è¶…è¿‡10%çš„ç©ºå€¼ç‡
                    issues.append({
                        'type': 'high_null_rate',
                        'field': field,
                        'null_count': null_count,
                        'null_rate': round(null_rate, 3),
                        'severity': 'warning' if null_rate < 0.3 else 'error'
                    })
            
            # æ£€æŸ¥é‡å¤è®°å½•ï¼ˆåŸºäºä¸»é”®å­—æ®µï¼‰
            if table_name in sync_manager.CORE_TABLES:
                primary_key = sync_manager.CORE_TABLES[table_name].get('primary_key')
                if primary_key:
                    key_values = [record.get(primary_key) for record in data if record.get(primary_key)]
                    unique_keys = set(key_values)
                    
                    if len(key_values) != len(unique_keys):
                        duplicate_count = len(key_values) - len(unique_keys)
                        issues.append({
                            'type': 'duplicate_keys',
                            'field': primary_key,
                            'duplicate_count': duplicate_count,
                            'severity': 'error'
                        })
            
        except Exception as e:
            logger.error(f"Data quality check failed for {table_name}: {e}")
            issues.append({
                'type': 'quality_check_error',
                'error_message': str(e),
                'severity': 'error'
            })
        
        return issues
    
    def check_data_consistency(self, table_name: str, sample_size: int = 100) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§ï¼ˆæ¯”è¾ƒ SQLite å’Œ Supabase ä¸­çš„å®é™…æ•°æ®ï¼‰
        
        Args:
            table_name: è¡¨å
            sample_size: é‡‡æ ·å¤§å°
            
        Returns:
            ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
        """
        start_time = datetime.now()
        
        try:
            # ä» SQLite è·å–æ ·æœ¬æ•°æ®
            with self.get_sqlite_connection() as conn:
                query = f"SELECT * FROM {table_name} ORDER BY RANDOM() LIMIT {sample_size}"
                sqlite_df = pd.read_sql_query(query, conn)
            
            if sqlite_df.empty:
                return {
                    'table_name': table_name,
                    'status': 'skipped',
                    'message': 'No data in SQLite table',
                    'timestamp': start_time.isoformat()
                }
            
            # è·å–ä¸»é”®å­—æ®µ
            primary_key = sync_manager.CORE_TABLES.get(table_name, {}).get('primary_key', 'id')
            
            # ä» Supabase è·å–å¯¹åº”æ•°æ®
            key_values = sqlite_df[primary_key].tolist()
            response = self.supabase_client.table(table_name).select('*').in_(primary_key, key_values).execute()
            
            if not response.data:
                return {
                    'table_name': table_name,
                    'status': 'failed',
                    'consistency_score': 0.0,
                    'message': 'No matching data found in Supabase',
                    'timestamp': start_time.isoformat()
                }
            
            supabase_df = pd.DataFrame(response.data)
            
            # æ¯”è¾ƒæ•°æ®ä¸€è‡´æ€§
            consistency_results = self._compare_dataframes(sqlite_df, supabase_df, primary_key)
            
            result = {
                'table_name': table_name,
                'sample_size': len(sqlite_df),
                'matched_records': consistency_results['matched_records'],
                'consistency_score': consistency_results['consistency_score'],
                'field_differences': consistency_results['field_differences'],
                'status': 'passed' if consistency_results['consistency_score'] >= self.consistency_threshold else 'failed',
                'timestamp': start_time.isoformat(),
                'duration_seconds': (datetime.now() - start_time).total_seconds()
            }
            
            logger.info(f"Consistency check for {table_name}: {result['status']} "
                       f"(score: {consistency_results['consistency_score']:.3f})")
            
            return result
            
        except Exception as e:
            error_msg = f"Consistency check failed for {table_name}: {str(e)}"
            logger.error(error_msg)
            
            return {
                'table_name': table_name,
                'status': 'error',
                'error_message': error_msg,
                'timestamp': start_time.isoformat(),
                'duration_seconds': (datetime.now() - start_time).total_seconds()
            }
    
    def _compare_dataframes(self, df1: pd.DataFrame, df2: pd.DataFrame, 
                           primary_key: str) -> Dict[str, Any]:
        """
        æ¯”è¾ƒä¸¤ä¸ª DataFrame çš„ä¸€è‡´æ€§
        
        Args:
            df1: ç¬¬ä¸€ä¸ª DataFrame (SQLite)
            df2: ç¬¬äºŒä¸ª DataFrame (Supabase)
            primary_key: ä¸»é”®å­—æ®µ
            
        Returns:
            æ¯”è¾ƒç»“æœ
        """
        # æŒ‰ä¸»é”®åˆå¹¶æ•°æ®
        merged = df1.set_index(primary_key).join(
            df2.set_index(primary_key), 
            how='inner', 
            rsuffix='_supabase'
        )
        
        matched_records = len(merged)
        total_records = len(df1)
        
        if matched_records == 0:
            return {
                'matched_records': 0,
                'consistency_score': 0.0,
                'field_differences': {}
            }
        
        # æ¯”è¾ƒå­—æ®µå€¼
        field_differences = {}
        consistent_fields = 0
        total_fields = 0
        
        for column in df1.columns:
            if column == primary_key:
                continue
                
            supabase_column = f"{column}_supabase"
            if supabase_column not in merged.columns:
                continue
            
            total_fields += 1
            
            # æ¯”è¾ƒå­—æ®µå€¼ï¼ˆå¤„ç†ç±»å‹å·®å¼‚ï¼‰
            sqlite_values = merged[column].astype(str).fillna('')
            supabase_values = merged[supabase_column].astype(str).fillna('')
            
            differences = (sqlite_values != supabase_values).sum()
            consistency_rate = 1.0 - (differences / matched_records)
            
            if consistency_rate < 1.0:
                field_differences[column] = {
                    'differences': int(differences),
                    'consistency_rate': round(consistency_rate, 4),
                    'severity': 'warning' if consistency_rate >= 0.9 else 'error'
                }
            else:
                consistent_fields += 1
        
        # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§å¾—åˆ†
        if total_fields > 0:
            field_consistency_score = consistent_fields / total_fields
        else:
            field_consistency_score = 1.0
        
        record_consistency_score = matched_records / total_records
        overall_consistency_score = (field_consistency_score + record_consistency_score) / 2
        
        return {
            'matched_records': matched_records,
            'consistency_score': round(overall_consistency_score, 4),
            'field_differences': field_differences
        }
    
    def run_full_audit(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å®¡è®¡
        
        Returns:
            å®¡è®¡ç»“æœæ‘˜è¦
        """
        start_time = datetime.now()
        
        audit_result = {
            'audit_id': hashlib.md5(start_time.isoformat().encode()).hexdigest()[:8],
            'start_time': start_time.isoformat(),
            'audit_type': 'full_audit',
            'table_results': {},
            'summary': {
                'total_tables': len(self.audit_tables),
                'passed_integrity': 0,
                'failed_integrity': 0,
                'passed_consistency': 0,
                'failed_consistency': 0,
                'total_issues': 0
            }
        }
        
        logger.info(f"Starting full audit for {len(self.audit_tables)} tables")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œå®¡è®¡
        with ThreadPoolExecutor(max_workers=4) as executor:
            # æäº¤å®Œæ•´æ€§æ£€æŸ¥ä»»åŠ¡
            integrity_futures = {
                executor.submit(self.check_data_integrity, table): table 
                for table in self.audit_tables
            }
            
            # æäº¤ä¸€è‡´æ€§æ£€æŸ¥ä»»åŠ¡
            consistency_futures = {
                executor.submit(self.check_data_consistency, table): table 
                for table in self.audit_tables
            }
            
            # æ”¶é›†å®Œæ•´æ€§æ£€æŸ¥ç»“æœ
            for future in as_completed(integrity_futures):
                table_name = integrity_futures[future]
                try:
                    result = future.result()
                    audit_result['table_results'][table_name] = {'integrity': result}
                    
                    if result['status'] == 'passed':
                        audit_result['summary']['passed_integrity'] += 1
                    else:
                        audit_result['summary']['failed_integrity'] += 1
                    
                    # ç»Ÿè®¡è´¨é‡é—®é¢˜
                    if 'quality_issues' in result:
                        audit_result['summary']['total_issues'] += len(result['quality_issues'])
                        
                except Exception as e:
                    logger.error(f"Integrity check failed for {table_name}: {e}")
                    audit_result['table_results'][table_name] = {
                        'integrity': {
                            'status': 'error',
                            'error_message': str(e)
                        }
                    }
                    audit_result['summary']['failed_integrity'] += 1
            
            # æ”¶é›†ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
            for future in as_completed(consistency_futures):
                table_name = consistency_futures[future]
                try:
                    result = future.result()
                    if table_name not in audit_result['table_results']:
                        audit_result['table_results'][table_name] = {}
                    
                    audit_result['table_results'][table_name]['consistency'] = result
                    
                    if result['status'] == 'passed':
                        audit_result['summary']['passed_consistency'] += 1
                    else:
                        audit_result['summary']['failed_consistency'] += 1
                        
                except Exception as e:
                    logger.error(f"Consistency check failed for {table_name}: {e}")
                    if table_name not in audit_result['table_results']:
                        audit_result['table_results'][table_name] = {}
                    
                    audit_result['table_results'][table_name]['consistency'] = {
                        'status': 'error',
                        'error_message': str(e)
                    }
                    audit_result['summary']['failed_consistency'] += 1
        
        # å®Œæˆå®¡è®¡
        audit_result['end_time'] = datetime.now().isoformat()
        audit_result['duration_seconds'] = (datetime.now() - start_time).total_seconds()
        
        # è®¡ç®—æ€»ä½“çŠ¶æ€
        total_checks = audit_result['summary']['total_tables'] * 2  # å®Œæ•´æ€§ + ä¸€è‡´æ€§
        passed_checks = (audit_result['summary']['passed_integrity'] + 
                        audit_result['summary']['passed_consistency'])
        
        audit_result['summary']['overall_pass_rate'] = passed_checks / total_checks if total_checks > 0 else 0
        audit_result['summary']['status'] = 'passed' if audit_result['summary']['overall_pass_rate'] >= 0.8 else 'failed'
        
        # ä¿å­˜å®¡è®¡å†å²
        self._save_audit_result(audit_result)
        
        logger.info(f"Full audit completed: {audit_result['summary']['status']} "
                   f"(pass rate: {audit_result['summary']['overall_pass_rate']:.2%})")
        
        return audit_result
    
    def _save_audit_result(self, audit_result: Dict[str, Any]):
        """
        ä¿å­˜å®¡è®¡ç»“æœåˆ° Supabase
        
        Args:
            audit_result: å®¡è®¡ç»“æœ
        """
        try:
            # ä¿å­˜åˆ°å®¡è®¡æ—¥å¿—è¡¨
            audit_log = {
                'audit_type': 'full_audit',
                'details': audit_result,
                'status': audit_result['summary']['status'],
                'created_by': 'audit_system'
            }
            
            self.supabase_client.table('audit_logs').insert(audit_log).execute()
            
            # ä¿å­˜åˆ°æœ¬åœ°å†å²
            self.audit_history.append(audit_result)
            
            # ä¿æŒå†å²è®°å½•æ•°é‡é™åˆ¶
            if len(self.audit_history) > self.max_history_records:
                self.audit_history = self.audit_history[-self.max_history_records:]
            
            logger.info(f"Audit result saved: {audit_result['audit_id']}")
            
        except Exception as e:
            logger.error(f"Failed to save audit result: {e}")
    
    def get_audit_summary(self, days: int = 7) -> Dict[str, Any]:
        """
        è·å–å®¡è®¡æ‘˜è¦æŠ¥å‘Š
        
        Args:
            days: æŸ¥è¯¢å¤©æ•°
            
        Returns:
            å®¡è®¡æ‘˜è¦
        """
        try:
            # ä» Supabase è·å–æœ€è¿‘çš„å®¡è®¡è®°å½•
            since_date = (datetime.now() - timedelta(days=days)).isoformat()
            
            response = self.supabase_client.table('audit_logs') \
                .select('*') \
                .eq('audit_type', 'full_audit') \
                .gte('created_at', since_date) \
                .order('created_at', desc=True) \
                .execute()
            
            if not response.data:
                return {
                    'period_days': days,
                    'total_audits': 0,
                    'message': 'No audit records found'
                }
            
            audits = response.data
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_audits = len(audits)
            passed_audits = sum(1 for audit in audits if audit.get('status') == 'passed')
            
            # è®¡ç®—å¹³å‡é€šè¿‡ç‡
            pass_rates = []
            for audit in audits:
                details = audit.get('details', {})
                summary = details.get('summary', {})
                if 'overall_pass_rate' in summary:
                    pass_rates.append(summary['overall_pass_rate'])
            
            avg_pass_rate = statistics.mean(pass_rates) if pass_rates else 0
            
            # æœ€è¿‘å®¡è®¡çŠ¶æ€
            latest_audit = audits[0] if audits else None
            
            summary = {
                'period_days': days,
                'total_audits': total_audits,
                'passed_audits': passed_audits,
                'failed_audits': total_audits - passed_audits,
                'success_rate': passed_audits / total_audits if total_audits > 0 else 0,
                'average_pass_rate': avg_pass_rate,
                'latest_audit': {
                    'timestamp': latest_audit['created_at'] if latest_audit else None,
                    'status': latest_audit['status'] if latest_audit else None
                },
                'generated_at': datetime.now().isoformat()
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"Failed to generate audit summary: {e}")
            return {
                'period_days': days,
                'error_message': str(e),
                'generated_at': datetime.now().isoformat()
            }
    
    def generate_audit_report(self, audit_result: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆå®¡è®¡æŠ¥å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰
        
        Args:
            audit_result: å®¡è®¡ç»“æœ
            
        Returns:
            Markdown æ ¼å¼çš„æŠ¥å‘Š
        """
        report_lines = [
            f"# æ•°æ®å®¡è®¡æŠ¥å‘Š",
            f"",
            f"**å®¡è®¡ID**: {audit_result.get('audit_id', 'N/A')}",
            f"**å®¡è®¡æ—¶é—´**: {audit_result.get('start_time', 'N/A')}",
            f"**å®¡è®¡ç±»å‹**: {audit_result.get('audit_type', 'N/A')}",
            f"**æ€»ä½“çŠ¶æ€**: {audit_result.get('summary', {}).get('status', 'N/A')}",
            f"**æŒç»­æ—¶é—´**: {audit_result.get('duration_seconds', 0):.2f} ç§’",
            f"",
            f"## å®¡è®¡æ‘˜è¦",
            f"",
            f"- **å®¡è®¡è¡¨æ•°é‡**: {audit_result.get('summary', {}).get('total_tables', 0)}",
            f"- **å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡**: {audit_result.get('summary', {}).get('passed_integrity', 0)}",
            f"- **å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥**: {audit_result.get('summary', {}).get('failed_integrity', 0)}",
            f"- **ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡**: {audit_result.get('summary', {}).get('passed_consistency', 0)}",
            f"- **ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥**: {audit_result.get('summary', {}).get('failed_consistency', 0)}",
            f"- **æ€»ä½“é€šè¿‡ç‡**: {audit_result.get('summary', {}).get('overall_pass_rate', 0):.2%}",
            f"- **å‘ç°é—®é¢˜æ€»æ•°**: {audit_result.get('summary', {}).get('total_issues', 0)}",
            f"",
            f"## è¯¦ç»†ç»“æœ",
            f""
        ]
        
        # æ·»åŠ æ¯ä¸ªè¡¨çš„è¯¦ç»†ç»“æœ
        table_results = audit_result.get('table_results', {})
        for table_name, results in table_results.items():
            report_lines.extend([
                f"### {table_name}",
                f""
            ])
            
            # å®Œæ•´æ€§æ£€æŸ¥ç»“æœ
            integrity = results.get('integrity', {})
            if integrity:
                status_icon = "âœ…" if integrity.get('status') == 'passed' else "âŒ"
                report_lines.extend([
                    f"**å®Œæ•´æ€§æ£€æŸ¥**: {status_icon} {integrity.get('status', 'N/A')}",
                    f"- SQLite è®°å½•æ•°: {integrity.get('sqlite_count', 'N/A')}",
                    f"- Supabase è®°å½•æ•°: {integrity.get('supabase_count', 'N/A')}",
                    f"- ç¼ºå¤±è®°å½•æ•°: {integrity.get('missing_records', 'N/A')}",
                    f"- å®Œæ•´æ€§å¾—åˆ†: {integrity.get('integrity_score', 'N/A')}",
                    f""
                ])
                
                # è´¨é‡é—®é¢˜
                quality_issues = integrity.get('quality_issues', [])
                if quality_issues:
                    report_lines.append("**è´¨é‡é—®é¢˜**:")
                    for issue in quality_issues:
                        severity_icon = "âš ï¸" if issue.get('severity') == 'warning' else "ğŸš¨"
                        report_lines.append(f"- {severity_icon} {issue.get('type', 'N/A')}: {issue}")
                    report_lines.append("")
            
            # ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
            consistency = results.get('consistency', {})
            if consistency:
                status_icon = "âœ…" if consistency.get('status') == 'passed' else "âŒ"
                report_lines.extend([
                    f"**ä¸€è‡´æ€§æ£€æŸ¥**: {status_icon} {consistency.get('status', 'N/A')}",
                    f"- æ ·æœ¬å¤§å°: {consistency.get('sample_size', 'N/A')}",
                    f"- åŒ¹é…è®°å½•æ•°: {consistency.get('matched_records', 'N/A')}",
                    f"- ä¸€è‡´æ€§å¾—åˆ†: {consistency.get('consistency_score', 'N/A')}",
                    f""
                ])
                
                # å­—æ®µå·®å¼‚
                field_differences = consistency.get('field_differences', {})
                if field_differences:
                    report_lines.append("**å­—æ®µå·®å¼‚**:")
                    for field, diff in field_differences.items():
                        severity_icon = "âš ï¸" if diff.get('severity') == 'warning' else "ğŸš¨"
                        report_lines.append(f"- {severity_icon} {field}: {diff.get('differences')} å·®å¼‚ "
                                          f"(ä¸€è‡´æ€§: {diff.get('consistency_rate', 0):.2%})")
                    report_lines.append("")
            
            report_lines.append("---")
            report_lines.append("")
        
        return "\n".join(report_lines)

# å…¨å±€å®¡è®¡ç³»ç»Ÿå®ä¾‹
audit_system = DataAuditSystem()

def run_data_audit() -> Dict[str, Any]:
    """
    è¿è¡Œæ•°æ®å®¡è®¡çš„ä¾¿æ·å‡½æ•°
    
    Returns:
        å®¡è®¡ç»“æœ
    """
    return audit_system.run_full_audit()

def check_table_integrity(table_name: str) -> Dict[str, Any]:
    """
    æ£€æŸ¥å•ä¸ªè¡¨å®Œæ•´æ€§çš„ä¾¿æ·å‡½æ•°
    
    Args:
        table_name: è¡¨å
        
    Returns:
        å®Œæ•´æ€§æ£€æŸ¥ç»“æœ
    """
    return audit_system.check_data_integrity(table_name)

if __name__ == "__main__":
    # æµ‹è¯•å®¡è®¡ç³»ç»Ÿ
    print("Testing Data Audit System...")
    
    try:
        system = DataAuditSystem()
        
        # è¿è¡Œå®Œæ•´å®¡è®¡
        print("ğŸ” Running full audit...")
        audit_result = system.run_full_audit()
        
        print(f"Audit completed:")
        print(f"  Status: {audit_result['summary']['status']}")
        print(f"  Pass rate: {audit_result['summary']['overall_pass_rate']:.2%}")
        print(f"  Duration: {audit_result['duration_seconds']:.2f}s")
        
        # ç”ŸæˆæŠ¥å‘Š
        report = system.generate_audit_report(audit_result)
        print(f"\nGenerated audit report ({len(report)} characters)")
        
        # è·å–å®¡è®¡æ‘˜è¦
        summary = system.get_audit_summary(7)
        print(f"\nAudit summary: {summary}")
        
    except Exception as e:
        print(f"âŒ Audit system test failed: {e}")