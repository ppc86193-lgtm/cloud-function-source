#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PC28ç³»ç»Ÿè‡ªåŠ¨åŒ–pytestæµ‹è¯•æ‰§è¡Œå™¨
è‡ªåŠ¨å‘ç°ã€è¿è¡Œå’Œæ•è·pytestæµ‹è¯•çš„è¯¦ç»†æ‰§è¡Œç»“æœ
"""

import os
import sys
import json
import time
import logging
import subprocess
import pytest
import coverage
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, asdict
from collections import defaultdict
import traceback
import argparse
import threading
import queue
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class PytestTestResult:
    """Pytestæµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_id: str
    test_name: str
    test_file: str
    test_class: Optional[str]
    test_method: str
    status: str  # passed, failed, error, skipped, xfail, xpass
    duration: float
    error_message: Optional[str] = None
    error_traceback: Optional[str] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    test_type: str = "unit"
    markers: List[str] = None
    setup_duration: float = 0.0
    teardown_duration: float = 0.0
    
    def __post_init__(self):
        if self.markers is None:
            self.markers = []

@dataclass
class PytestSuiteResult:
    """Pytestæµ‹è¯•å¥—ä»¶ç»“æœ"""
    suite_name: str
    test_file: str
    test_type: str
    tests: List[PytestTestResult]
    total_duration: float
    passed_count: int
    failed_count: int
    error_count: int
    skipped_count: int
    xfail_count: int
    xpass_count: int
    success_rate: float
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    coverage_data: Optional[Dict] = None

@dataclass
class PytestExecutionReport:
    """Pytestæ‰§è¡ŒæŠ¥å‘Š"""
    report_id: str
    generation_time: datetime
    total_tests: int
    total_suites: int
    passed_tests: int
    failed_tests: int
    error_tests: int
    skipped_tests: int
    xfail_tests: int
    xpass_tests: int
    total_duration: float
    success_rate: float
    test_suites: List[PytestSuiteResult]
    coverage_data: Optional[Dict] = None
    performance_metrics: Optional[Dict] = None
    environment_info: Optional[Dict] = None
    command_line_args: Optional[List[str]] = None

class PytestResultCollector:
    """Pytestç»“æœæ”¶é›†å™¨"""
    
    def __init__(self):
        self.results = []
        self.current_suite = None
        self.start_time = None
        self.end_time = None
        
    def pytest_runtest_setup(self, item):
        """æµ‹è¯•è®¾ç½®é˜¶æ®µ"""
        test_info = self._extract_test_info(item)
        test_info['setup_start'] = datetime.now()
        item.test_info = test_info
        
    def pytest_runtest_call(self, item):
        """æµ‹è¯•æ‰§è¡Œé˜¶æ®µ"""
        if hasattr(item, 'test_info'):
            item.test_info['call_start'] = datetime.now()
            
    def pytest_runtest_teardown(self, item):
        """æµ‹è¯•æ¸…ç†é˜¶æ®µ"""
        if hasattr(item, 'test_info'):
            item.test_info['teardown_start'] = datetime.now()
            
    def pytest_runtest_logreport(self, report):
        """æ”¶é›†æµ‹è¯•æŠ¥å‘Š"""
        if report.when == 'call':
            test_result = self._create_test_result(report)
            self.results.append(test_result)
            
    def pytest_sessionstart(self, session):
        """æµ‹è¯•ä¼šè¯å¼€å§‹"""
        self.start_time = datetime.now()
        logger.info(f"Pytestä¼šè¯å¼€å§‹: {self.start_time}")
        
    def pytest_sessionfinish(self, session, exitstatus):
        """æµ‹è¯•ä¼šè¯ç»“æŸ"""
        self.end_time = datetime.now()
        logger.info(f"Pytestä¼šè¯ç»“æŸ: {self.end_time}, é€€å‡ºçŠ¶æ€: {exitstatus}")
        
    def _extract_test_info(self, item):
        """æå–æµ‹è¯•ä¿¡æ¯"""
        return {
            'test_id': item.nodeid,
            'test_name': item.name,
            'test_file': str(item.fspath),
            'test_class': item.cls.__name__ if item.cls else None,
            'test_method': item.function.__name__,
            'markers': [marker.name for marker in item.iter_markers()],
            'test_type': self._determine_test_type(item)
        }
        
    def _determine_test_type(self, item):
        """ç¡®å®šæµ‹è¯•ç±»å‹"""
        markers = [marker.name for marker in item.iter_markers()]
        if 'integration' in markers:
            return 'integration'
        elif 'performance' in markers:
            return 'performance'
        elif 'e2e' in markers:
            return 'e2e'
        else:
            return 'unit'
            
    def _create_test_result(self, report):
        """åˆ›å»ºæµ‹è¯•ç»“æœå¯¹è±¡"""
        item = report.item
        test_info = getattr(item, 'test_info', {})
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        setup_duration = 0.0
        call_duration = report.duration
        teardown_duration = 0.0
        
        if 'setup_start' in test_info and 'call_start' in test_info:
            setup_duration = (test_info['call_start'] - test_info['setup_start']).total_seconds()
        if 'teardown_start' in test_info:
            teardown_duration = 0.1  # ä¼°ç®—å€¼
            
        # å¤„ç†é”™è¯¯ä¿¡æ¯
        error_message = None
        error_traceback = None
        if report.failed or report.outcome == 'error':
            if hasattr(report, 'longrepr') and report.longrepr:
                error_message = str(report.longrepr).split('\n')[-1] if report.longrepr else None
                error_traceback = str(report.longrepr)
                
        return PytestTestResult(
            test_id=test_info.get('test_id', item.nodeid),
            test_name=test_info.get('test_name', item.name),
            test_file=test_info.get('test_file', str(item.fspath)),
            test_class=test_info.get('test_class'),
            test_method=test_info.get('test_method', item.function.__name__),
            status=report.outcome,
            duration=call_duration,
            error_message=error_message,
            error_traceback=error_traceback,
            start_time=test_info.get('call_start'),
            end_time=datetime.now(),
            test_type=test_info.get('test_type', 'unit'),
            markers=test_info.get('markers', []),
            setup_duration=setup_duration,
            teardown_duration=teardown_duration
        )

class AutomatedPytestExecutor:
    """è‡ªåŠ¨åŒ–Pytestæ‰§è¡Œå™¨"""
    
    def __init__(self, root_path: str = ".", output_dir: str = "pytest_reports"):
        self.root_path = Path(root_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.collector = PytestResultCollector()
        self.coverage_analyzer = None
        
    def discover_test_files(self, test_patterns: List[str] = None) -> List[Path]:
        """å‘ç°æµ‹è¯•æ–‡ä»¶"""
        if test_patterns is None:
            test_patterns = ["test_*.py", "*_test.py"]
            
        test_files = []
        
        # æ‰«æpc28_business_logic_testsç›®å½•
        business_logic_dir = self.root_path / "pc28_business_logic_tests"
        if business_logic_dir.exists():
            for pattern in test_patterns:
                test_files.extend(business_logic_dir.rglob(pattern))
                
        # æ‰«ææ ¹ç›®å½•ä¸‹çš„æµ‹è¯•æ–‡ä»¶
        for pattern in test_patterns:
            test_files.extend(self.root_path.glob(pattern))
            
        # å»é‡å¹¶æ’åº
        test_files = sorted(list(set(test_files)))
        
        logger.info(f"å‘ç° {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
        for test_file in test_files:
            logger.info(f"  - {test_file}")
            
        return test_files
        
    def run_pytest_with_collection(self, 
                                 test_files: List[Path] = None,
                                 markers: List[str] = None,
                                 enable_coverage: bool = True,
                                 verbose: bool = True,
                                 capture: str = "no",
                                 extra_args: List[str] = None) -> PytestExecutionReport:
        """è¿è¡Œpytestå¹¶æ”¶é›†ç»“æœ"""
        
        # å‡†å¤‡pytestå‚æ•°
        pytest_args = []
        
        # æ·»åŠ æµ‹è¯•æ–‡ä»¶
        if test_files:
            pytest_args.extend([str(f) for f in test_files])
        else:
            # ä½¿ç”¨é»˜è®¤æµ‹è¯•è·¯å¾„
            pytest_args.extend([
                "pc28_business_logic_tests/",
                "test_*.py"
            ])
            
        # æ·»åŠ æ ‡è®°è¿‡æ»¤
        if markers:
            for marker in markers:
                pytest_args.extend(["-m", marker])
                
        # æ·»åŠ è¯¦ç»†è¾“å‡º
        if verbose:
            pytest_args.append("-v")
            
        # è®¾ç½®è¾“å‡ºæ•è·
        pytest_args.extend(["-s" if capture == "no" else f"--capture={capture}"])
        
        # æ·»åŠ é¢å¤–å‚æ•°
        if extra_args:
            pytest_args.extend(extra_args)
            
        # å¯ç”¨è¦†ç›–ç‡
        if enable_coverage:
            self._setup_coverage()
            
        logger.info(f"æ‰§è¡Œpytestå‘½ä»¤: pytest {' '.join(pytest_args)}")
        
        # æ³¨å†Œæ’ä»¶
        pytest.main(pytest_args + ["--tb=short"], plugins=[self.collector])
        
        # åœæ­¢è¦†ç›–ç‡æ”¶é›†
        coverage_data = None
        if enable_coverage and self.coverage_analyzer:
            coverage_data = self._stop_coverage()
            
        # ç”ŸæˆæŠ¥å‘Š
        return self._create_execution_report(
            results=self.collector.results,
            coverage_data=coverage_data,
            command_args=pytest_args
        )
        
    def run_pytest_subprocess(self,
                            test_files: List[Path] = None,
                            markers: List[str] = None,
                            enable_coverage: bool = True,
                            verbose: bool = True,
                            output_format: str = "json") -> PytestExecutionReport:
        """é€šè¿‡å­è¿›ç¨‹è¿è¡Œpytest"""
        
        # æ„å»ºå‘½ä»¤
        cmd = [sys.executable, "-m", "pytest"]
        
        # æ·»åŠ æµ‹è¯•æ–‡ä»¶
        if test_files:
            cmd.extend([str(f) for f in test_files])
        else:
            cmd.extend(["pc28_business_logic_tests/", "test_*.py"])
            
        # æ·»åŠ æ ‡è®°è¿‡æ»¤
        if markers:
            for marker in markers:
                cmd.extend(["-m", marker])
                
        # æ·»åŠ è¾“å‡ºæ ¼å¼
        if output_format == "json":
            json_report = self.output_dir / f"pytest_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            cmd.extend(["--json-report", f"--json-report-file={json_report}"])
            
        # æ·»åŠ è¯¦ç»†è¾“å‡º
        if verbose:
            cmd.append("-v")
            
        # æ·»åŠ è¦†ç›–ç‡
        if enable_coverage:
            cmd.extend([
                "--cov=.",
                "--cov-report=json",
                f"--cov-report=json:{self.output_dir}/coverage.json"
            ])
            
        logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        # æ‰§è¡Œå‘½ä»¤
        start_time = datetime.now()
        try:
            result = subprocess.run(
                cmd,
                cwd=self.root_path,
                capture_output=True,
                text=True,
                timeout=3600  # 1å°æ—¶è¶…æ—¶
            )
            end_time = datetime.now()
            
            # è§£æç»“æœ
            return self._parse_subprocess_result(
                result, start_time, end_time, json_report if output_format == "json" else None
            )
            
        except subprocess.TimeoutExpired:
            logger.error("Pytestæ‰§è¡Œè¶…æ—¶")
            return self._create_empty_report("æ‰§è¡Œè¶…æ—¶")
        except Exception as e:
            logger.error(f"Pytestæ‰§è¡Œå¤±è´¥: {e}")
            return self._create_empty_report(f"æ‰§è¡Œå¤±è´¥: {e}")
            
    def run_tests_by_category(self, enable_coverage: bool = True) -> Dict[str, PytestExecutionReport]:
        """æŒ‰ç±»åˆ«è¿è¡Œæµ‹è¯•"""
        categories = {
            "lottery": ["lottery"],
            "betting": ["betting"],
            "payout": ["payout"],
            "risk": ["risk"],
            "data": ["data"],
            "integration": ["integration"],
            "performance": ["performance"]
        }
        
        results = {}
        
        for category, markers in categories.items():
            logger.info(f"è¿è¡Œ {category} ç±»åˆ«çš„æµ‹è¯•...")
            try:
                report = self.run_pytest_with_collection(
                    markers=markers,
                    enable_coverage=enable_coverage
                )
                results[category] = report
                logger.info(f"{category} æµ‹è¯•å®Œæˆ: {report.passed_tests}/{report.total_tests} é€šè¿‡")
            except Exception as e:
                logger.error(f"{category} æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
                results[category] = self._create_empty_report(f"æ‰§è¡Œå¤±è´¥: {e}")
                
        return results
        
    def _setup_coverage(self):
        """è®¾ç½®è¦†ç›–ç‡åˆ†æ"""
        try:
            self.coverage_analyzer = coverage.Coverage(
                source=[str(self.root_path)],
                omit=[
                    "*/venv/*",
                    "*/test*",
                    "*/__pycache__/*",
                    "*/migrations/*"
                ]
            )
            self.coverage_analyzer.start()
            logger.info("è¦†ç›–ç‡åˆ†æå·²å¯åŠ¨")
        except Exception as e:
            logger.warning(f"æ— æ³•å¯åŠ¨è¦†ç›–ç‡åˆ†æ: {e}")
            
    def _stop_coverage(self) -> Optional[Dict]:
        """åœæ­¢è¦†ç›–ç‡åˆ†æ"""
        if not self.coverage_analyzer:
            return None
            
        try:
            self.coverage_analyzer.stop()
            self.coverage_analyzer.save()
            
            # ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
            coverage_file = self.output_dir / "coverage.json"
            self.coverage_analyzer.json_report(outfile=str(coverage_file))
            
            # è¯»å–è¦†ç›–ç‡æ•°æ®
            if coverage_file.exists():
                with open(coverage_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
                    
        except Exception as e:
            logger.warning(f"è¦†ç›–ç‡åˆ†æå¤±è´¥: {e}")
            
        return None
        
    def _create_execution_report(self, 
                               results: List[PytestTestResult],
                               coverage_data: Optional[Dict] = None,
                               command_args: Optional[List[str]] = None) -> PytestExecutionReport:
        """åˆ›å»ºæ‰§è¡ŒæŠ¥å‘Š"""
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„æµ‹è¯•ç»“æœ
        suites_by_file = defaultdict(list)
        for result in results:
            suites_by_file[result.test_file].append(result)
            
        # åˆ›å»ºæµ‹è¯•å¥—ä»¶
        test_suites = []
        for test_file, file_results in suites_by_file.items():
            suite = self._create_suite_result(test_file, file_results)
            test_suites.append(suite)
            
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.status == 'passed')
        failed_tests = sum(1 for r in results if r.status == 'failed')
        error_tests = sum(1 for r in results if r.status == 'error')
        skipped_tests = sum(1 for r in results if r.status == 'skipped')
        xfail_tests = sum(1 for r in results if r.status == 'xfail')
        xpass_tests = sum(1 for r in results if r.status == 'xpass')
        
        total_duration = sum(r.duration for r in results)
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        # ç¯å¢ƒä¿¡æ¯
        env_info = {
            'python_version': sys.version,
            'pytest_version': pytest.__version__,
            'working_directory': str(self.root_path),
            'timestamp': datetime.now().isoformat()
        }
        
        # æ€§èƒ½æŒ‡æ ‡
        performance_metrics = {
            'average_test_duration': total_duration / total_tests if total_tests > 0 else 0,
            'slowest_tests': sorted(results, key=lambda x: x.duration, reverse=True)[:5],
            'fastest_tests': sorted(results, key=lambda x: x.duration)[:5]
        }
        
        return PytestExecutionReport(
            report_id=f"pytest_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            generation_time=datetime.now(),
            total_tests=total_tests,
            total_suites=len(test_suites),
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            error_tests=error_tests,
            skipped_tests=skipped_tests,
            xfail_tests=xfail_tests,
            xpass_tests=xpass_tests,
            total_duration=total_duration,
            success_rate=success_rate,
            test_suites=test_suites,
            coverage_data=coverage_data,
            performance_metrics=performance_metrics,
            environment_info=env_info,
            command_line_args=command_args
        )
        
    def _create_suite_result(self, test_file: str, results: List[PytestTestResult]) -> PytestSuiteResult:
        """åˆ›å»ºæµ‹è¯•å¥—ä»¶ç»“æœ"""
        
        passed_count = sum(1 for r in results if r.status == 'passed')
        failed_count = sum(1 for r in results if r.status == 'failed')
        error_count = sum(1 for r in results if r.status == 'error')
        skipped_count = sum(1 for r in results if r.status == 'skipped')
        xfail_count = sum(1 for r in results if r.status == 'xfail')
        xpass_count = sum(1 for r in results if r.status == 'xpass')
        
        total_duration = sum(r.duration for r in results)
        success_rate = (passed_count / len(results) * 100) if results else 0
        
        # ç¡®å®šæµ‹è¯•ç±»å‹
        test_type = "unit"
        if any("integration" in r.markers for r in results):
            test_type = "integration"
        elif any("performance" in r.markers for r in results):
            test_type = "performance"
            
        return PytestSuiteResult(
            suite_name=Path(test_file).stem,
            test_file=test_file,
            test_type=test_type,
            tests=results,
            total_duration=total_duration,
            passed_count=passed_count,
            failed_count=failed_count,
            error_count=error_count,
            skipped_count=skipped_count,
            xfail_count=xfail_count,
            xpass_count=xpass_count,
            success_rate=success_rate,
            start_time=min(r.start_time for r in results if r.start_time) if results else None,
            end_time=max(r.end_time for r in results if r.end_time) if results else None
        )
        
    def _parse_subprocess_result(self, 
                               result: subprocess.CompletedProcess,
                               start_time: datetime,
                               end_time: datetime,
                               json_report_file: Optional[Path] = None) -> PytestExecutionReport:
        """è§£æå­è¿›ç¨‹æ‰§è¡Œç»“æœ"""
        
        # å°è¯•ä»JSONæŠ¥å‘Šæ–‡ä»¶è¯»å–ç»“æœ
        if json_report_file and json_report_file.exists():
            try:
                with open(json_report_file, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                return self._parse_json_report(json_data)
            except Exception as e:
                logger.warning(f"æ— æ³•è§£æJSONæŠ¥å‘Š: {e}")
                
        # è§£ææ ‡å‡†è¾“å‡º
        return self._parse_text_output(result.stdout, result.stderr, start_time, end_time)
        
    def _parse_json_report(self, json_data: Dict) -> PytestExecutionReport:
        """è§£æJSONæ ¼å¼çš„pytestæŠ¥å‘Š"""
        # è¿™é‡Œéœ€è¦æ ¹æ®pytest-json-reportæ’ä»¶çš„è¾“å‡ºæ ¼å¼æ¥è§£æ
        # ç”±äºæ ¼å¼å¯èƒ½å˜åŒ–ï¼Œè¿™é‡Œæä¾›åŸºæœ¬å®ç°
        
        summary = json_data.get('summary', {})
        tests = json_data.get('tests', [])
        
        # è½¬æ¢æµ‹è¯•ç»“æœ
        pytest_results = []
        for test in tests:
            pytest_results.append(PytestTestResult(
                test_id=test.get('nodeid', ''),
                test_name=test.get('name', ''),
                test_file=test.get('file', ''),
                test_class=test.get('class'),
                test_method=test.get('function', ''),
                status=test.get('outcome', 'unknown'),
                duration=test.get('duration', 0.0),
                error_message=test.get('message'),
                error_traceback=test.get('traceback'),
                markers=test.get('markers', [])
            ))
            
        return self._create_execution_report(pytest_results)
        
    def _parse_text_output(self, 
                         stdout: str, 
                         stderr: str,
                         start_time: datetime,
                         end_time: datetime) -> PytestExecutionReport:
        """è§£ææ–‡æœ¬è¾“å‡º"""
        
        # ç®€å•çš„æ–‡æœ¬è§£æå®ç°
        lines = stdout.split('\n')
        
        # æŸ¥æ‰¾æµ‹è¯•ç»“æœæ‘˜è¦
        passed = failed = error = skipped = 0
        duration = (end_time - start_time).total_seconds()
        
        for line in lines:
            if 'passed' in line and 'failed' in line:
                # è§£æç±»ä¼¼ "5 passed, 2 failed in 10.5s" çš„è¡Œ
                parts = line.split()
                for i, part in enumerate(parts):
                    if part == 'passed' and i > 0:
                        passed = int(parts[i-1])
                    elif part == 'failed' and i > 0:
                        failed = int(parts[i-1])
                    elif part == 'error' and i > 0:
                        error = int(parts[i-1])
                    elif part == 'skipped' and i > 0:
                        skipped = int(parts[i-1])
                        
        total_tests = passed + failed + error + skipped
        success_rate = (passed / total_tests * 100) if total_tests > 0 else 0
        
        return PytestExecutionReport(
            report_id=f"pytest_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            generation_time=datetime.now(),
            total_tests=total_tests,
            total_suites=0,
            passed_tests=passed,
            failed_tests=failed,
            error_tests=error,
            skipped_tests=skipped,
            xfail_tests=0,
            xpass_tests=0,
            total_duration=duration,
            success_rate=success_rate,
            test_suites=[],
            environment_info={
                'stdout': stdout,
                'stderr': stderr,
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat()
            }
        )
        
    def _create_empty_report(self, error_message: str = "") -> PytestExecutionReport:
        """åˆ›å»ºç©ºæŠ¥å‘Š"""
        return PytestExecutionReport(
            report_id=f"pytest_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            generation_time=datetime.now(),
            total_tests=0,
            total_suites=0,
            passed_tests=0,
            failed_tests=0,
            error_tests=0,
            skipped_tests=0,
            xfail_tests=0,
            xpass_tests=0,
            total_duration=0.0,
            success_rate=0.0,
            test_suites=[],
            environment_info={'error': error_message}
        )
        
    def generate_detailed_report(self, report: PytestExecutionReport) -> Dict[str, str]:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        
        reports = {}
        
        # JSONæŠ¥å‘Š
        json_file = self.output_dir / f"{report.report_id}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(report), f, indent=2, ensure_ascii=False, default=str)
        reports['json'] = str(json_file)
        
        # HTMLæŠ¥å‘Š
        html_file = self.output_dir / f"{report.report_id}.html"
        html_content = self._generate_html_report(report)
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        reports['html'] = str(html_file)
        
        # MarkdownæŠ¥å‘Š
        md_file = self.output_dir / f"{report.report_id}.md"
        md_content = self._generate_markdown_report(report)
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        reports['markdown'] = str(md_file)
        
        return reports
        
    def _generate_html_report(self, report: PytestExecutionReport) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        
        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PC28 Pytestæ‰§è¡ŒæŠ¥å‘Š - {report.report_id}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f5f5f5; padding: 20px; border-radius: 5px; }}
        .summary {{ display: flex; gap: 20px; margin: 20px 0; }}
        .metric {{ background: white; padding: 15px; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .passed {{ color: #28a745; }}
        .failed {{ color: #dc3545; }}
        .error {{ color: #fd7e14; }}
        .skipped {{ color: #6c757d; }}
        .suite {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
        .test-result {{ margin: 10px 0; padding: 10px; border-left: 4px solid #ddd; }}
        .test-result.passed {{ border-left-color: #28a745; }}
        .test-result.failed {{ border-left-color: #dc3545; }}
        .test-result.error {{ border-left-color: #fd7e14; }}
        .test-result.skipped {{ border-left-color: #6c757d; }}
        .error-details {{ background: #f8f9fa; padding: 10px; margin: 10px 0; border-radius: 3px; }}
        pre {{ white-space: pre-wrap; word-wrap: break-word; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>PC28 Pytestæ‰§è¡ŒæŠ¥å‘Š</h1>
        <p><strong>æŠ¥å‘ŠID:</strong> {report.report_id}</p>
        <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {report.generation_time}</p>
        <p><strong>æ€»æ‰§è¡Œæ—¶é—´:</strong> {report.total_duration:.2f}ç§’</p>
    </div>
    
    <div class="summary">
        <div class="metric">
            <h3>æ€»æµ‹è¯•æ•°</h3>
            <div style="font-size: 2em; font-weight: bold;">{report.total_tests}</div>
        </div>
        <div class="metric">
            <h3 class="passed">é€šè¿‡</h3>
            <div style="font-size: 2em; font-weight: bold; color: #28a745;">{report.passed_tests}</div>
        </div>
        <div class="metric">
            <h3 class="failed">å¤±è´¥</h3>
            <div style="font-size: 2em; font-weight: bold; color: #dc3545;">{report.failed_tests}</div>
        </div>
        <div class="metric">
            <h3 class="error">é”™è¯¯</h3>
            <div style="font-size: 2em; font-weight: bold; color: #fd7e14;">{report.error_tests}</div>
        </div>
        <div class="metric">
            <h3 class="skipped">è·³è¿‡</h3>
            <div style="font-size: 2em; font-weight: bold; color: #6c757d;">{report.skipped_tests}</div>
        </div>
        <div class="metric">
            <h3>æˆåŠŸç‡</h3>
            <div style="font-size: 2em; font-weight: bold;">{report.success_rate:.1f}%</div>
        </div>
    </div>
"""
        
        # æ·»åŠ æµ‹è¯•å¥—ä»¶è¯¦æƒ…
        for suite in report.test_suites:
            html += f"""
    <div class="suite">
        <h2>{suite.suite_name}</h2>
        <p><strong>æ–‡ä»¶:</strong> {suite.test_file}</p>
        <p><strong>ç±»å‹:</strong> {suite.test_type}</p>
        <p><strong>æ‰§è¡Œæ—¶é—´:</strong> {suite.total_duration:.2f}ç§’</p>
        <p><strong>æˆåŠŸç‡:</strong> {suite.success_rate:.1f}%</p>
        
        <h3>æµ‹è¯•ç»“æœ</h3>
"""
            
            for test in suite.tests:
                html += f"""
        <div class="test-result {test.status}">
            <h4>{test.test_name}</h4>
            <p><strong>çŠ¶æ€:</strong> <span class="{test.status}">{test.status.upper()}</span></p>
            <p><strong>æ‰§è¡Œæ—¶é—´:</strong> {test.duration:.3f}ç§’</p>
            <p><strong>æ ‡è®°:</strong> {', '.join(test.markers) if test.markers else 'æ— '}</p>
"""
                
                if test.error_message:
                    html += f"""
            <div class="error-details">
                <strong>é”™è¯¯ä¿¡æ¯:</strong>
                <pre>{test.error_message}</pre>
"""
                    if test.error_traceback:
                        html += f"""
                <strong>é”™è¯¯å †æ ˆ:</strong>
                <pre>{test.error_traceback}</pre>
"""
                    html += "</div>"
                    
                html += "</div>"
                
            html += "</div>"
            
        html += """
</body>
</html>
"""
        return html
        
    def _generate_markdown_report(self, report: PytestExecutionReport) -> str:
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        
        md = f"""# PC28 Pytestæ‰§è¡ŒæŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

- **æŠ¥å‘ŠID**: {report.report_id}
- **ç”Ÿæˆæ—¶é—´**: {report.generation_time}
- **æ€»æ‰§è¡Œæ—¶é—´**: {report.total_duration:.2f}ç§’
- **æµ‹è¯•å¥—ä»¶æ•°**: {report.total_suites}

## æ‰§è¡Œæ‘˜è¦

| æŒ‡æ ‡ | æ•°é‡ | ç™¾åˆ†æ¯” |
|------|------|--------|
| æ€»æµ‹è¯•æ•° | {report.total_tests} | 100% |
| é€šè¿‡ | {report.passed_tests} | {(report.passed_tests/report.total_tests*100):.1f}% |
| å¤±è´¥ | {report.failed_tests} | {(report.failed_tests/report.total_tests*100):.1f}% |
| é”™è¯¯ | {report.error_tests} | {(report.error_tests/report.total_tests*100):.1f}% |
| è·³è¿‡ | {report.skipped_tests} | {(report.skipped_tests/report.total_tests*100):.1f}% |
| **æˆåŠŸç‡** | **{report.success_rate:.1f}%** | - |

## æµ‹è¯•å¥—ä»¶è¯¦æƒ…

"""
        
        for suite in report.test_suites:
            md += f"""### {suite.suite_name}

- **æ–‡ä»¶**: `{suite.test_file}`
- **ç±»å‹**: {suite.test_type}
- **æ‰§è¡Œæ—¶é—´**: {suite.total_duration:.2f}ç§’
- **æˆåŠŸç‡**: {suite.success_rate:.1f}%
- **æµ‹è¯•ç»Ÿè®¡**: é€šè¿‡ {suite.passed_count}, å¤±è´¥ {suite.failed_count}, é”™è¯¯ {suite.error_count}, è·³è¿‡ {suite.skipped_count}

#### æµ‹è¯•ç»“æœ

| æµ‹è¯•åç§° | çŠ¶æ€ | æ‰§è¡Œæ—¶é—´ | æ ‡è®° |
|----------|------|----------|------|
"""
            
            for test in suite.tests:
                status_emoji = {
                    'passed': 'âœ…',
                    'failed': 'âŒ',
                    'error': 'ğŸ’¥',
                    'skipped': 'â­ï¸',
                    'xfail': 'âš ï¸',
                    'xpass': 'ğŸ‰'
                }.get(test.status, 'â“')
                
                markers_str = ', '.join(test.markers) if test.markers else '-'
                md += f"| {test.test_name} | {status_emoji} {test.status} | {test.duration:.3f}s | {markers_str} |\n"
                
            # æ·»åŠ å¤±è´¥æµ‹è¯•çš„è¯¦ç»†ä¿¡æ¯
            failed_tests = [t for t in suite.tests if t.status in ['failed', 'error']]
            if failed_tests:
                md += f"\n#### å¤±è´¥æµ‹è¯•è¯¦æƒ…\n\n"
                for test in failed_tests:
                    md += f"##### {test.test_name}\n\n"
                    if test.error_message:
                        md += f"**é”™è¯¯ä¿¡æ¯**: {test.error_message}\n\n"
                    if test.error_traceback:
                        md += f"**é”™è¯¯å †æ ˆ**:\n```\n{test.error_traceback}\n```\n\n"
                        
            md += "\n"
            
        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
        if report.performance_metrics:
            md += "## æ€§èƒ½æŒ‡æ ‡\n\n"
            metrics = report.performance_metrics
            md += f"- **å¹³å‡æµ‹è¯•æ‰§è¡Œæ—¶é—´**: {metrics.get('average_test_duration', 0):.3f}ç§’\n\n"
            
            if 'slowest_tests' in metrics:
                md += "### æœ€æ…¢çš„æµ‹è¯•\n\n"
                for i, test in enumerate(metrics['slowest_tests'][:5], 1):
                    md += f"{i}. {test.test_name}: {test.duration:.3f}ç§’\n"
                md += "\n"
                
        # æ·»åŠ è¦†ç›–ç‡ä¿¡æ¯
        if report.coverage_data:
            md += "## ä»£ç è¦†ç›–ç‡\n\n"
            coverage = report.coverage_data
            if 'totals' in coverage:
                totals = coverage['totals']
                md += f"- **æ€»è¦†ç›–ç‡**: {totals.get('percent_covered', 0):.1f}%\n"
                md += f"- **è¦†ç›–è¡Œæ•°**: {totals.get('covered_lines', 0)}\n"
                md += f"- **æ€»è¡Œæ•°**: {totals.get('num_statements', 0)}\n\n"
                
        return md

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='PC28è‡ªåŠ¨åŒ–Pytestæ‰§è¡Œå™¨')
    parser.add_argument('--root', default='.', help='é¡¹ç›®æ ¹ç›®å½•')
    parser.add_argument('--output', default='pytest_reports', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--markers', nargs='*', help='æµ‹è¯•æ ‡è®°è¿‡æ»¤')
    parser.add_argument('--no-coverage', action='store_true', help='ç¦ç”¨è¦†ç›–ç‡')
    parser.add_argument('--category', help='æŒ‰ç±»åˆ«è¿è¡Œæµ‹è¯•')
    parser.add_argument('--subprocess', action='store_true', help='ä½¿ç”¨å­è¿›ç¨‹æ¨¡å¼')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ‰§è¡Œå™¨
    executor = AutomatedPytestExecutor(args.root, args.output)
    
    try:
        if args.category:
            # æŒ‰ç±»åˆ«è¿è¡Œ
            reports = executor.run_tests_by_category(enable_coverage=not args.no_coverage)
            for category, report in reports.items():
                logger.info(f"{category} æµ‹è¯•ç»“æœ: {report.passed_tests}/{report.total_tests} é€šè¿‡")
                executor.generate_detailed_report(report)
        else:
            # å‘ç°æµ‹è¯•æ–‡ä»¶
            test_files = executor.discover_test_files()
            
            if args.subprocess:
                # å­è¿›ç¨‹æ¨¡å¼
                report = executor.run_pytest_subprocess(
                    test_files=test_files,
                    markers=args.markers,
                    enable_coverage=not args.no_coverage,
                    verbose=args.verbose
                )
            else:
                # ç›´æ¥æ¨¡å¼
                report = executor.run_pytest_with_collection(
                    test_files=test_files,
                    markers=args.markers,
                    enable_coverage=not args.no_coverage,
                    verbose=args.verbose
                )
                
            # ç”ŸæˆæŠ¥å‘Š
            report_files = executor.generate_detailed_report(report)
            
            # è¾“å‡ºç»“æœ
            logger.info(f"æµ‹è¯•æ‰§è¡Œå®Œæˆ!")
            logger.info(f"æ€»æµ‹è¯•æ•°: {report.total_tests}")
            logger.info(f"é€šè¿‡: {report.passed_tests}")
            logger.info(f"å¤±è´¥: {report.failed_tests}")
            logger.info(f"é”™è¯¯: {report.error_tests}")
            logger.info(f"è·³è¿‡: {report.skipped_tests}")
            logger.info(f"æˆåŠŸç‡: {report.success_rate:.1f}%")
            logger.info(f"æ€»æ‰§è¡Œæ—¶é—´: {report.total_duration:.2f}ç§’")
            
            logger.info("ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶:")
            for format_type, file_path in report_files.items():
                logger.info(f"  {format_type.upper()}: {file_path}")
                
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()