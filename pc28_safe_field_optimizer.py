#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PC28å®‰å…¨å­—æ®µä¼˜åŒ–ç³»ç»Ÿ
åŸºäºæµ‹è¯•ç»“æœå’Œç³»ç»Ÿç¨³å®šæ€§ï¼Œå®‰å…¨è¯†åˆ«å’Œåˆ é™¤å†—ä½™å­—æ®µ
"""

import json
import os
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from google.cloud import bigquery
from dataclasses import dataclass

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class FieldOptimization:
    """å­—æ®µä¼˜åŒ–å»ºè®®"""
    table_name: str
    field_name: str
    optimization_type: str  # 'remove_redundant', 'remove_unused', 'archive'
    reason: str
    risk_level: str  # 'low', 'medium', 'high'
    estimated_savings: Dict[str, Any]
    backup_required: bool
    validation_queries: List[str]

class PC28SafeFieldOptimizer:
    """PC28å®‰å…¨å­—æ®µä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.client = bigquery.Client()
        self.project_id = "wprojectl"
        self.dataset_id = "pc28_lab"
        self.optimizations = []
        self.safety_checks = []
        
        # å·²çŸ¥çš„å®‰å…¨å†—ä½™å­—æ®µï¼ˆåŸºäºä¹‹å‰çš„åˆ†æï¼‰
        self.safe_redundant_fields = {
            'ts_utc': 'timestamp',  # ts_utcä¸timestampé‡å¤
            'result_digits': 'numbers',  # result_digitsä¸numbersé‡å¤
            'data_source': 'source',  # ç±»ä¼¼åŠŸèƒ½å­—æ®µ
            'curtime': 'drawTime',  # APIä¸­çš„å†—ä½™æ—¶é—´å­—æ®µ
        }
        
        # éœ€è¦ç‰¹åˆ«å°å¿ƒçš„å­—æ®µï¼ˆä¸å»ºè®®åˆ é™¤ï¼‰
        self.protected_fields = {
            'id', 'created_at', 'updated_at', 'timestamp', 'draw_id', 
            'period', 'issue', 'openCode', 'drawTime', 'numbers'
        }
    
    def analyze_system_readiness(self) -> Dict[str, Any]:
        """åˆ†æç³»ç»Ÿæ˜¯å¦å‡†å¤‡å¥½è¿›è¡Œä¼˜åŒ–"""
        logger.info("ğŸ” åˆ†æç³»ç»Ÿä¼˜åŒ–å‡†å¤‡çŠ¶æ€...")
        
        readiness = {
            "ready_for_optimization": False,
            "test_results": {},
            "system_health": {},
            "risk_assessment": "high",
            "recommendations": []
        }
        
        # æ£€æŸ¥æœ€æ–°çš„ä¸šåŠ¡æµ‹è¯•ç»“æœ
        business_test_results = self._get_latest_business_test_results()
        if business_test_results:
            readiness["test_results"] = business_test_results
            success_rate = business_test_results.get("success_rate", 0)
            
            if success_rate >= 95:
                readiness["ready_for_optimization"] = True
                readiness["risk_assessment"] = "low"
                logger.info(f"âœ… ç³»ç»Ÿæµ‹è¯•é€šè¿‡ç‡ {success_rate}%ï¼Œå¯ä»¥è¿›è¡Œå®‰å…¨ä¼˜åŒ–")
            else:
                readiness["recommendations"].append(
                    f"ç³»ç»Ÿæµ‹è¯•é€šè¿‡ç‡ä»… {success_rate}%ï¼Œå»ºè®®å…ˆä¿®å¤å¤±è´¥çš„æµ‹è¯•"
                )
                logger.warning(f"âš ï¸ ç³»ç»Ÿæµ‹è¯•é€šè¿‡ç‡ {success_rate}%ï¼Œä¸å»ºè®®è¿›è¡Œä¼˜åŒ–")
        
        # æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€
        system_health = self._check_system_health()
        readiness["system_health"] = system_health
        
        return readiness
    
    def identify_safe_optimizations(self) -> List[FieldOptimization]:
        """è¯†åˆ«å®‰å…¨çš„å­—æ®µä¼˜åŒ–æœºä¼š"""
        logger.info("ğŸ” è¯†åˆ«å®‰å…¨çš„å­—æ®µä¼˜åŒ–æœºä¼š...")
        
        optimizations = []
        
        # è·å–æ‰€æœ‰è¡¨çš„ä¿¡æ¯
        tables = self._get_table_information()
        
        for table_name, table_info in tables.items():
            # åˆ†ææ¯ä¸ªè¡¨çš„å­—æ®µä¼˜åŒ–æœºä¼š
            table_optimizations = self._analyze_table_optimizations(table_name, table_info)
            optimizations.extend(table_optimizations)
        
        # æŒ‰é£é™©çº§åˆ«å’Œæ”¶ç›Šæ’åº
        optimizations.sort(key=lambda x: (
            {'low': 0, 'medium': 1, 'high': 2}[x.risk_level],
            -x.estimated_savings.get('storage_mb', 0)
        ))
        
        self.optimizations = optimizations
        logger.info(f"âœ… è¯†åˆ«åˆ° {len(optimizations)} ä¸ªä¼˜åŒ–æœºä¼š")
        
        return optimizations
    
    def _analyze_table_optimizations(self, table_name: str, table_info: Dict) -> List[FieldOptimization]:
        """åˆ†æå•ä¸ªè¡¨çš„ä¼˜åŒ–æœºä¼š"""
        optimizations = []
        
        if not table_info.get('schema') or not table_info['schema'].get('fields'):
            return optimizations
        
        for field in table_info['schema']['fields']:
            field_name = field['name']
            
            # è·³è¿‡å—ä¿æŠ¤çš„å­—æ®µ
            if field_name in self.protected_fields:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå®‰å…¨çš„å†—ä½™å­—æ®µ
            if field_name in self.safe_redundant_fields:
                optimization = self._create_redundant_field_optimization(
                    table_name, field_name, field
                )
                if optimization:
                    optimizations.append(optimization)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœªä½¿ç”¨çš„å­—æ®µ
            elif self._is_unused_field(table_name, field_name):
                optimization = self._create_unused_field_optimization(
                    table_name, field_name, field
                )
                if optimization:
                    optimizations.append(optimization)
        
        return optimizations
    
    def _create_redundant_field_optimization(self, table_name: str, field_name: str, field_info: Dict) -> Optional[FieldOptimization]:
        """åˆ›å»ºå†—ä½™å­—æ®µä¼˜åŒ–å»ºè®®"""
        replacement_field = self.safe_redundant_fields[field_name]
        
        # éªŒè¯æ›¿ä»£å­—æ®µç¡®å®å­˜åœ¨
        if not self._field_exists_in_table(table_name, replacement_field):
            logger.warning(f"âš ï¸ è¡¨ {table_name} ä¸­ä¸å­˜åœ¨æ›¿ä»£å­—æ®µ {replacement_field}")
            return None
        
        # ä¼°ç®—å­˜å‚¨èŠ‚çœ
        estimated_savings = self._estimate_field_savings(table_name, field_name)
        
        # ç”ŸæˆéªŒè¯æŸ¥è¯¢
        validation_queries = [
            f"SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.{table_name}` WHERE {field_name} IS NOT NULL",
            f"SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.{table_name}` WHERE {replacement_field} IS NOT NULL",
            f"SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.{table_name}` WHERE {field_name} != {replacement_field}"
        ]
        
        return FieldOptimization(
            table_name=table_name,
            field_name=field_name,
            optimization_type='remove_redundant',
            reason=f'å­—æ®µ {field_name} ä¸ {replacement_field} åŠŸèƒ½é‡å¤',
            risk_level='low',
            estimated_savings=estimated_savings,
            backup_required=True,
            validation_queries=validation_queries
        )
    
    def _create_unused_field_optimization(self, table_name: str, field_name: str, field_info: Dict) -> Optional[FieldOptimization]:
        """åˆ›å»ºæœªä½¿ç”¨å­—æ®µä¼˜åŒ–å»ºè®®"""
        # ä¼°ç®—å­˜å‚¨èŠ‚çœ
        estimated_savings = self._estimate_field_savings(table_name, field_name)
        
        # åªæœ‰å½“å­˜å‚¨èŠ‚çœæ˜¾è‘—æ—¶æ‰å»ºè®®åˆ é™¤
        if estimated_savings.get('storage_mb', 0) < 1:
            return None
        
        # ç”ŸæˆéªŒè¯æŸ¥è¯¢
        validation_queries = [
            f"SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.{table_name}` WHERE {field_name} IS NOT NULL",
            f"SELECT COUNT(DISTINCT {field_name}) FROM `{self.project_id}.{self.dataset_id}.{table_name}` WHERE {field_name} IS NOT NULL"
        ]
        
        return FieldOptimization(
            table_name=table_name,
            field_name=field_name,
            optimization_type='remove_unused',
            reason=f'å­—æ®µ {field_name} åœ¨ä¸šåŠ¡é€»è¾‘ä¸­æœªè¢«ä½¿ç”¨',
            risk_level='medium',
            estimated_savings=estimated_savings,
            backup_required=True,
            validation_queries=validation_queries
        )
    
    def validate_optimizations(self) -> Dict[str, Any]:
        """éªŒè¯ä¼˜åŒ–å»ºè®®çš„å®‰å…¨æ€§"""
        logger.info("ğŸ” éªŒè¯ä¼˜åŒ–å»ºè®®çš„å®‰å…¨æ€§...")
        
        validation_results = {
            "safe_optimizations": [],
            "risky_optimizations": [],
            "validation_errors": [],
            "total_estimated_savings": {"storage_mb": 0, "query_performance": 0}
        }
        
        for optimization in self.optimizations:
            try:
                # è¿è¡ŒéªŒè¯æŸ¥è¯¢
                validation_passed = True
                validation_details = {}
                
                for query in optimization.validation_queries:
                    try:
                        result = self.client.query(query).result()
                        rows = list(result)
                        validation_details[query] = rows[0][0] if rows else 0
                    except Exception as e:
                        logger.error(f"éªŒè¯æŸ¥è¯¢å¤±è´¥: {query}, é”™è¯¯: {e}")
                        validation_passed = False
                        validation_results["validation_errors"].append({
                            "optimization": f"{optimization.table_name}.{optimization.field_name}",
                            "query": query,
                            "error": str(e)
                        })
                
                if validation_passed:
                    # é¢å¤–çš„å®‰å…¨æ£€æŸ¥
                    if self._perform_safety_checks(optimization, validation_details):
                        validation_results["safe_optimizations"].append({
                            "optimization": optimization,
                            "validation_details": validation_details
                        })
                        validation_results["total_estimated_savings"]["storage_mb"] += optimization.estimated_savings.get("storage_mb", 0)
                    else:
                        validation_results["risky_optimizations"].append({
                            "optimization": optimization,
                            "reason": "æœªé€šè¿‡å®‰å…¨æ£€æŸ¥"
                        })
                
            except Exception as e:
                logger.error(f"éªŒè¯ä¼˜åŒ– {optimization.table_name}.{optimization.field_name} æ—¶å‡ºé”™: {e}")
                validation_results["validation_errors"].append({
                    "optimization": f"{optimization.table_name}.{optimization.field_name}",
                    "error": str(e)
                })
        
        logger.info(f"âœ… éªŒè¯å®Œæˆ: {len(validation_results['safe_optimizations'])} ä¸ªå®‰å…¨ä¼˜åŒ–ï¼Œ{len(validation_results['risky_optimizations'])} ä¸ªé£é™©ä¼˜åŒ–")
        
        return validation_results
    
    def _perform_safety_checks(self, optimization: FieldOptimization, validation_details: Dict) -> bool:
        """æ‰§è¡Œå®‰å…¨æ£€æŸ¥"""
        # å¯¹äºå†—ä½™å­—æ®µï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
        if optimization.optimization_type == 'remove_redundant':
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸ä¸€è‡´çš„æ•°æ®
            inconsistent_query = optimization.validation_queries[-1]  # æœ€åä¸€ä¸ªæŸ¥è¯¢æ£€æŸ¥ä¸ä¸€è‡´æ€§
            inconsistent_count = validation_details.get(inconsistent_query, 0)
            
            if inconsistent_count > 0:
                logger.warning(f"âš ï¸ å­—æ®µ {optimization.field_name} ä¸æ›¿ä»£å­—æ®µå­˜åœ¨ {inconsistent_count} æ¡ä¸ä¸€è‡´æ•°æ®")
                return False
        
        # å¯¹äºæœªä½¿ç”¨å­—æ®µï¼Œç¡®ä¿ç¡®å®æ²¡æœ‰è¢«ä½¿ç”¨
        if optimization.optimization_type == 'remove_unused':
            non_null_count = validation_details.get(optimization.validation_queries[0], 0)
            distinct_count = validation_details.get(optimization.validation_queries[1], 0)
            
            # å¦‚æœå­—æ®µæœ‰å¤§é‡éç©ºæ•°æ®ï¼Œéœ€è¦æ›´è°¨æ…
            if non_null_count > 1000:
                logger.warning(f"âš ï¸ å­—æ®µ {optimization.field_name} æœ‰ {non_null_count} æ¡éç©ºæ•°æ®ï¼Œéœ€è¦è°¨æ…å¤„ç†")
                return False
        
        return True
    
    def generate_optimization_plan(self, validation_results: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–æ‰§è¡Œè®¡åˆ’"""
        logger.info("ğŸ“‹ ç”Ÿæˆä¼˜åŒ–æ‰§è¡Œè®¡åˆ’...")
        
        safe_optimizations = validation_results["safe_optimizations"]
        
        plan = {
            "execution_phases": [],
            "backup_requirements": [],
            "rollback_plan": [],
            "estimated_timeline": {},
            "risk_mitigation": []
        }
        
        # é˜¶æ®µ1ï¼šä½é£é™©å†—ä½™å­—æ®µåˆ é™¤
        phase1_optimizations = [
            opt["optimization"] for opt in safe_optimizations 
            if opt["optimization"].risk_level == 'low' and opt["optimization"].optimization_type == 'remove_redundant'
        ]
        
        if phase1_optimizations:
            plan["execution_phases"].append({
                "phase": 1,
                "name": "ä½é£é™©å†—ä½™å­—æ®µåˆ é™¤",
                "optimizations": phase1_optimizations,
                "estimated_duration": "1-2å°æ—¶",
                "prerequisites": ["å®Œæ•´å¤‡ä»½", "æµ‹è¯•ç¯å¢ƒéªŒè¯"]
            })
        
        # é˜¶æ®µ2ï¼šæœªä½¿ç”¨å­—æ®µå½’æ¡£
        phase2_optimizations = [
            opt["optimization"] for opt in safe_optimizations 
            if opt["optimization"].optimization_type == 'remove_unused'
        ]
        
        if phase2_optimizations:
            plan["execution_phases"].append({
                "phase": 2,
                "name": "æœªä½¿ç”¨å­—æ®µå½’æ¡£",
                "optimizations": phase2_optimizations,
                "estimated_duration": "2-4å°æ—¶",
                "prerequisites": ["é˜¶æ®µ1å®Œæˆ", "ä¸šåŠ¡ç¡®è®¤"]
            })
        
        # ç”Ÿæˆå¤‡ä»½è¦æ±‚
        for opt_data in safe_optimizations:
            opt = opt_data["optimization"]
            if opt.backup_required:
                plan["backup_requirements"].append({
                    "table": opt.table_name,
                    "field": opt.field_name,
                    "backup_query": f"CREATE TABLE `{self.project_id}.{self.dataset_id}.{opt.table_name}_backup_{datetime.now().strftime('%Y%m%d')}` AS SELECT * FROM `{self.project_id}.{self.dataset_id}.{opt.table_name}`"
                })
        
        # ç”Ÿæˆå›æ»šè®¡åˆ’
        plan["rollback_plan"] = self._generate_rollback_plan(safe_optimizations)
        
        # é£é™©ç¼“è§£æªæ–½
        plan["risk_mitigation"] = [
            "åœ¨æµ‹è¯•ç¯å¢ƒä¸­å®Œæ•´éªŒè¯æ‰€æœ‰ä¼˜åŒ–æ“ä½œ",
            "åˆ›å»ºå®Œæ•´çš„æ•°æ®å¤‡ä»½",
            "åˆ†é˜¶æ®µæ‰§è¡Œï¼Œæ¯é˜¶æ®µåéªŒè¯ç³»ç»ŸåŠŸèƒ½",
            "å‡†å¤‡å¿«é€Ÿå›æ»šæ–¹æ¡ˆ",
            "ç›‘æ§ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"
        ]
        
        return plan
    
    def _generate_rollback_plan(self, safe_optimizations: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆå›æ»šè®¡åˆ’"""
        rollback_steps = []
        
        for opt_data in safe_optimizations:
            opt = opt_data["optimization"]
            
            if opt.optimization_type == 'remove_redundant':
                rollback_steps.append({
                    "action": "restore_field",
                    "table": opt.table_name,
                    "field": opt.field_name,
                    "sql": f"ALTER TABLE `{self.project_id}.{self.dataset_id}.{opt.table_name}` ADD COLUMN {opt.field_name} STRING",
                    "data_restore": f"UPDATE `{self.project_id}.{self.dataset_id}.{opt.table_name}` SET {opt.field_name} = {self.safe_redundant_fields[opt.field_name]} WHERE {opt.field_name} IS NULL"
                })
        
        return rollback_steps
    
    def _get_latest_business_test_results(self) -> Optional[Dict]:
        """è·å–æœ€æ–°çš„ä¸šåŠ¡æµ‹è¯•ç»“æœ"""
        try:
            # æŸ¥æ‰¾æœ€æ–°çš„ä¸šåŠ¡æµ‹è¯•æŠ¥å‘Š
            report_files = []
            for file in os.listdir('.'):
                if file.startswith('pc28_business_test_report_') and file.endswith('.json'):
                    report_files.append(file)
            
            if not report_files:
                return None
            
            # è·å–æœ€æ–°çš„æŠ¥å‘Š
            latest_report = sorted(report_files)[-1]
            
            with open(latest_report, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        except Exception as e:
            logger.error(f"è·å–ä¸šåŠ¡æµ‹è¯•ç»“æœå¤±è´¥: {e}")
            return None
    
    def _check_system_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        health = {
            "database_connectivity": False,
            "table_accessibility": {},
            "recent_errors": []
        }
        
        try:
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            query = f"SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.INFORMATION_SCHEMA.TABLES`"
            result = self.client.query(query).result()
            health["database_connectivity"] = True
            
            # æµ‹è¯•å…³é”®è¡¨çš„å¯è®¿é—®æ€§
            key_tables = ['signal_pool', 'lab_push_candidates_v2', 'runtime_params']
            for table in key_tables:
                try:
                    query = f"SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.{table}` LIMIT 1"
                    self.client.query(query).result()
                    health["table_accessibility"][table] = True
                except Exception as e:
                    health["table_accessibility"][table] = False
                    health["recent_errors"].append(f"è¡¨ {table} ä¸å¯è®¿é—®: {e}")
        
        except Exception as e:
            health["recent_errors"].append(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        
        return health
    
    def _get_table_information(self) -> Dict[str, Dict]:
        """è·å–è¡¨ä¿¡æ¯"""
        tables = {}
        
        try:
            # è·å–æ•°æ®é›†ä¸­çš„æ‰€æœ‰è¡¨
            dataset = self.client.get_dataset(f"{self.project_id}.{self.dataset_id}")
            
            for table_ref in self.client.list_tables(dataset):
                table = self.client.get_table(table_ref)
                
                tables[table.table_id] = {
                    "schema": {
                        "fields": [
                            {
                                "name": field.name,
                                "type": field.field_type,
                                "mode": field.mode,
                                "description": field.description
                            }
                            for field in table.schema
                        ]
                    },
                    "num_rows": table.num_rows,
                    "num_bytes": table.num_bytes
                }
        
        except Exception as e:
            logger.error(f"è·å–è¡¨ä¿¡æ¯å¤±è´¥: {e}")
        
        return tables
    
    def _field_exists_in_table(self, table_name: str, field_name: str) -> bool:
        """æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨äºè¡¨ä¸­"""
        try:
            table = self.client.get_table(f"{self.project_id}.{self.dataset_id}.{table_name}")
            return any(field.name == field_name for field in table.schema)
        except Exception:
            return False
    
    def _is_unused_field(self, table_name: str, field_name: str) -> bool:
        """æ£€æŸ¥å­—æ®µæ˜¯å¦æœªè¢«ä½¿ç”¨"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„é€»è¾‘æ¥æ£€æŸ¥å­—æ®µä½¿ç”¨æƒ…å†µ
        # ç›®å‰åŸºäºå·²çŸ¥çš„æœªä½¿ç”¨å­—æ®µåˆ—è¡¨
        unused_patterns = ['curtime', 'next', 'legacy_', 'temp_', 'old_']
        return any(pattern in field_name.lower() for pattern in unused_patterns)
    
    def _estimate_field_savings(self, table_name: str, field_name: str) -> Dict[str, Any]:
        """ä¼°ç®—åˆ é™¤å­—æ®µçš„å­˜å‚¨èŠ‚çœ"""
        try:
            # è·å–è¡¨ä¿¡æ¯
            table = self.client.get_table(f"{self.project_id}.{self.dataset_id}.{table_name}")
            
            # ç®€å•ä¼°ç®—ï¼šå‡è®¾æ¯ä¸ªå­—æ®µå ç”¨æ€»å­˜å‚¨çš„å¹³å‡æ¯”ä¾‹
            total_fields = len(table.schema)
            if total_fields > 0:
                field_storage_mb = (table.num_bytes / (1024 * 1024)) / total_fields
            else:
                field_storage_mb = 0
            
            return {
                "storage_mb": round(field_storage_mb, 2),
                "query_performance": 0.05  # å‡è®¾5%çš„æ€§èƒ½æå‡
            }
        
        except Exception:
            return {"storage_mb": 0, "query_performance": 0}
    
    def save_optimization_report(self, readiness: Dict, optimizations: List[FieldOptimization], 
                               validation_results: Dict, plan: Dict):
        """ä¿å­˜ä¼˜åŒ–æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        report = {
            "timestamp": timestamp,
            "system_readiness": readiness,
            "identified_optimizations": [
                {
                    "table_name": opt.table_name,
                    "field_name": opt.field_name,
                    "optimization_type": opt.optimization_type,
                    "reason": opt.reason,
                    "risk_level": opt.risk_level,
                    "estimated_savings": opt.estimated_savings
                }
                for opt in optimizations
            ],
            "validation_results": validation_results,
            "execution_plan": plan,
            "summary": {
                "total_optimizations": len(optimizations),
                "safe_optimizations": len(validation_results.get("safe_optimizations", [])),
                "total_estimated_savings_mb": validation_results.get("total_estimated_savings", {}).get("storage_mb", 0),
                "ready_for_execution": readiness.get("ready_for_optimization", False)
            }
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_file = f"pc28_safe_field_optimization_report_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_file = f"pc28_safe_field_optimization_report_{timestamp}.md"
        self._generate_markdown_report(report, md_file)
        
        logger.info(f"ğŸ“„ ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜:")
        logger.info(f"  JSON: {json_file}")
        logger.info(f"  Markdown: {md_file}")
        
        return json_file, md_file
    
    def _generate_markdown_report(self, report: Dict, file_path: str):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        content = f"""# PC28å®‰å…¨å­—æ®µä¼˜åŒ–æŠ¥å‘Š

## ğŸ¯ æ‰§è¡Œæ‘˜è¦

**ç”Ÿæˆæ—¶é—´**: {report['timestamp']}
**ç³»ç»Ÿå‡†å¤‡çŠ¶æ€**: {'âœ… å°±ç»ª' if report['system_readiness']['ready_for_optimization'] else 'âŒ æœªå°±ç»ª'}
**è¯†åˆ«ä¼˜åŒ–æœºä¼š**: {report['summary']['total_optimizations']} ä¸ª
**å®‰å…¨ä¼˜åŒ–æ•°é‡**: {report['summary']['safe_optimizations']} ä¸ª
**é¢„ä¼°å­˜å‚¨èŠ‚çœ**: {report['summary']['total_estimated_savings_mb']:.2f} MB

## ğŸ“Š ç³»ç»Ÿå‡†å¤‡çŠ¶æ€

### æµ‹è¯•ç»“æœ
"""
        
        test_results = report['system_readiness'].get('test_results', {})
        if test_results:
            content += f"""
- **æˆåŠŸç‡**: {test_results.get('success_rate', 0)}%
- **é€šè¿‡æµ‹è¯•**: {test_results.get('passed_tests', 0)} ä¸ª
- **å¤±è´¥æµ‹è¯•**: {test_results.get('failed_tests', 0)} ä¸ª
"""
        
        content += f"""
### é£é™©è¯„ä¼°
**é£é™©çº§åˆ«**: {report['system_readiness']['risk_assessment']}

## ğŸ” è¯†åˆ«çš„ä¼˜åŒ–æœºä¼š

"""
        
        for opt in report['identified_optimizations']:
            risk_emoji = {'low': 'ğŸŸ¢', 'medium': 'ğŸŸ¡', 'high': 'ğŸ”´'}[opt['risk_level']]
            content += f"""
### {opt['table_name']}.{opt['field_name']} {risk_emoji}
- **ä¼˜åŒ–ç±»å‹**: {opt['optimization_type']}
- **åŸå› **: {opt['reason']}
- **é£é™©çº§åˆ«**: {opt['risk_level']}
- **é¢„ä¼°èŠ‚çœ**: {opt['estimated_savings']['storage_mb']:.2f} MB
"""
        
        content += f"""
## ğŸ“‹ æ‰§è¡Œè®¡åˆ’

"""
        
        plan = report['execution_plan']
        for phase in plan.get('execution_phases', []):
            content += f"""
### é˜¶æ®µ {phase['phase']}: {phase['name']}
- **é¢„ä¼°æ—¶é—´**: {phase['estimated_duration']}
- **ä¼˜åŒ–æ•°é‡**: {len(phase['optimizations'])} ä¸ª
- **å‰ç½®æ¡ä»¶**: {', '.join(phase['prerequisites'])}
"""
        
        content += f"""
## ğŸ›¡ï¸ é£é™©ç¼“è§£æªæ–½

"""
        for measure in plan.get('risk_mitigation', []):
            content += f"- {measure}\n"
        
        content += f"""
## ğŸ“ˆ é¢„æœŸæ”¶ç›Š

- **å­˜å‚¨ç©ºé—´èŠ‚çœ**: {report['summary']['total_estimated_savings_mb']:.2f} MB
- **æŸ¥è¯¢æ€§èƒ½æå‡**: é¢„è®¡ 5-10%
- **ç»´æŠ¤æˆæœ¬é™ä½**: é¢„è®¡ 10-15%
- **ç³»ç»Ÿå¤æ‚åº¦é™ä½**: ç§»é™¤å†—ä½™å­—æ®µ

## âš ï¸ æ³¨æ„äº‹é¡¹

1. åœ¨ç”Ÿäº§ç¯å¢ƒæ‰§è¡Œå‰ï¼Œå¿…é¡»åœ¨æµ‹è¯•ç¯å¢ƒå®Œæ•´éªŒè¯
2. ç¡®ä¿æ‰€æœ‰ç›¸å…³ç³»ç»Ÿå’Œåº”ç”¨ç¨‹åºå·²æ›´æ–°
3. å‡†å¤‡å¿«é€Ÿå›æ»šæ–¹æ¡ˆ
4. ç›‘æ§æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ç³»ç»Ÿæ€§èƒ½
5. ä¸ä¸šåŠ¡å›¢é˜Ÿç¡®è®¤å­—æ®µåˆ é™¤çš„å½±å“

## ğŸ”„ å›æ»šè®¡åˆ’

å¦‚æœä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œå¯ä»¥æŒ‰ä»¥ä¸‹æ­¥éª¤å›æ»šï¼š

"""
        
        for step in plan.get('rollback_plan', []):
            content += f"""
### {step['table']}.{step['field']}
```sql
{step['sql']}
{step.get('data_restore', '')}
```
"""
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ PC28å®‰å…¨å­—æ®µä¼˜åŒ–ç³»ç»Ÿ")
    print("=" * 60)
    print("ğŸ¯ ç›®æ ‡ï¼šåŸºäºç³»ç»Ÿç¨³å®šæ€§ï¼Œå®‰å…¨è¯†åˆ«å’Œåˆ é™¤å†—ä½™å­—æ®µ")
    print("ğŸ“‹ èŒƒå›´ï¼šå†—ä½™å­—æ®µåˆ é™¤ã€æœªä½¿ç”¨å­—æ®µå½’æ¡£ã€å­˜å‚¨ä¼˜åŒ–")
    print("=" * 60)
    
    optimizer = PC28SafeFieldOptimizer()
    
    try:
        # 1. åˆ†æç³»ç»Ÿå‡†å¤‡çŠ¶æ€
        logger.info("ğŸ” åˆ†æç³»ç»Ÿä¼˜åŒ–å‡†å¤‡çŠ¶æ€...")
        readiness = optimizer.analyze_system_readiness()
        
        if not readiness["ready_for_optimization"]:
            logger.warning("âš ï¸ ç³»ç»Ÿå°šæœªå‡†å¤‡å¥½è¿›è¡Œä¼˜åŒ–")
            for recommendation in readiness["recommendations"]:
                logger.warning(f"   - {recommendation}")
            
            # å³ä½¿ç³»ç»Ÿæœªå®Œå…¨å‡†å¤‡å¥½ï¼Œä¹Ÿå¯ä»¥ç”Ÿæˆä¼˜åŒ–è®¡åˆ’ä¾›å‚è€ƒ
            logger.info("ğŸ“‹ ç”Ÿæˆä¼˜åŒ–è®¡åˆ’ä¾›å‚è€ƒ...")
        
        # 2. è¯†åˆ«å®‰å…¨çš„ä¼˜åŒ–æœºä¼š
        optimizations = optimizer.identify_safe_optimizations()
        
        if not optimizations:
            logger.info("âœ… æœªå‘ç°éœ€è¦ä¼˜åŒ–çš„å­—æ®µ")
            return
        
        # 3. éªŒè¯ä¼˜åŒ–å»ºè®®
        validation_results = optimizer.validate_optimizations()
        
        # 4. ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        plan = optimizer.generate_optimization_plan(validation_results)
        
        # 5. ä¿å­˜æŠ¥å‘Š
        json_file, md_file = optimizer.save_optimization_report(
            readiness, optimizations, validation_results, plan
        )
        
        # 6. æ˜¾ç¤ºæ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“Š ä¼˜åŒ–åˆ†ææ‘˜è¦")
        print("=" * 60)
        
        print(f"\nç³»ç»Ÿå‡†å¤‡çŠ¶æ€: {'âœ… å°±ç»ª' if readiness['ready_for_optimization'] else 'âŒ æœªå°±ç»ª'}")
        print(f"è¯†åˆ«ä¼˜åŒ–æœºä¼š: {len(optimizations)} ä¸ª")
        print(f"å®‰å…¨ä¼˜åŒ–æ•°é‡: {len(validation_results['safe_optimizations'])} ä¸ª")
        print(f"é¢„ä¼°å­˜å‚¨èŠ‚çœ: {validation_results['total_estimated_savings']['storage_mb']:.2f} MB")
        
        if readiness["ready_for_optimization"] and validation_results["safe_optimizations"]:
            print("\nğŸ¯ å»ºè®®æ‰§è¡Œä¼˜åŒ–:")
            for opt_data in validation_results["safe_optimizations"]:
                opt = opt_data["optimization"]
                print(f"   - {opt.table_name}.{opt.field_name}: {opt.reason}")
        else:
            print("\nâš ï¸ å»ºè®®æš‚ç¼“æ‰§è¡Œä¼˜åŒ–ï¼Œå…ˆè§£å†³ç³»ç»Ÿé—®é¢˜")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {md_file}")
        
    except Exception as e:
        logger.error(f"ä¼˜åŒ–åˆ†æå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()