#!/usr/bin/env python3
"""
PC28å­—æ®µä¸åŒ¹é…ä¿®å¤ç³»ç»Ÿ
ä¸“é—¨è§£å†³ts_utcå­—æ®µé—®é¢˜å’Œå…¶ä»–å­—æ®µä¸åŒ¹é…é—®é¢˜
"""

import logging
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from google.cloud import bigquery

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PC28FieldMismatchFixer:
    def __init__(self):
        self.client = bigquery.Client()
        self.project_id = "wprojectl"
        self.dataset_id = "pc28_lab"
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def analyze_field_issues(self) -> Dict[str, Any]:
        """åˆ†æå­—æ®µä¸åŒ¹é…é—®é¢˜"""
        logger.info("ğŸ” åˆ†æå­—æ®µä¸åŒ¹é…é—®é¢˜...")
        
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "source_tables": {},
            "problematic_views": {},
            "field_mappings": {},
            "issues_found": []
        }
        
        # 1. æ£€æŸ¥æºè¡¨å­—æ®µ
        source_tables = [
            "cloud_pred_today_norm",
            "p_map_clean_merged_dedup_v", 
            "p_size_clean_merged_dedup_v"
        ]
        
        for table_name in source_tables:
            try:
                table_ref = f"{self.project_id}.{self.dataset_id}.{table_name}"
                table = self.client.get_table(table_ref)
                
                fields = {}
                for field in table.schema:
                    fields[field.name] = field.field_type
                
                analysis["source_tables"][table_name] = {
                    "fields": fields,
                    "accessible": True,
                    "row_count": self._get_table_count(table_ref)
                }
                
                logger.info(f"âœ… {table_name}: {len(fields)} å­—æ®µ, {analysis['source_tables'][table_name]['row_count']} è¡Œ")
                
            except Exception as e:
                analysis["source_tables"][table_name] = {
                    "fields": {},
                    "accessible": False,
                    "error": str(e)
                }
                logger.error(f"âŒ {table_name}: {e}")
        
        # 2. æ£€æŸ¥é—®é¢˜è§†å›¾
        problematic_views = [
            "p_cloud_clean_merged_dedup_v",
            "p_cloud_today_v"
        ]
        
        for view_name in problematic_views:
            try:
                # å°è¯•è·å–è§†å›¾å®šä¹‰
                view_ref = f"{self.project_id}.{self.dataset_id}.{view_name}"
                view = self.client.get_table(view_ref)
                
                analysis["problematic_views"][view_name] = {
                    "type": view.table_type,
                    "view_query": view.view_query if hasattr(view, 'view_query') else None,
                    "accessible": False,  # æˆ‘ä»¬çŸ¥é“è¿™äº›è§†å›¾æœ‰é—®é¢˜
                    "error": "ts_utc field not found"
                }
                
            except Exception as e:
                analysis["problematic_views"][view_name] = {
                    "accessible": False,
                    "error": str(e)
                }
        
        # 3. åˆ†æå­—æ®µæ˜ å°„é—®é¢˜
        cloud_fields = analysis["source_tables"].get("cloud_pred_today_norm", {}).get("fields", {})
        if cloud_fields:
            if "timestamp" in cloud_fields and "ts_utc" not in cloud_fields:
                analysis["issues_found"].append({
                    "issue_type": "field_name_mismatch",
                    "description": "cloud_pred_today_normè¡¨ä½¿ç”¨'timestamp'å­—æ®µï¼Œä½†è§†å›¾æœŸæœ›'ts_utc'å­—æ®µ",
                    "source_table": "cloud_pred_today_norm",
                    "expected_field": "ts_utc",
                    "actual_field": "timestamp",
                    "fix_strategy": "update_view_definition"
                })
        
        return analysis
    
    def _get_table_count(self, table_ref: str) -> int:
        """è·å–è¡¨è¡Œæ•°"""
        try:
            query = f"SELECT COUNT(*) as count FROM `{table_ref}`"
            result = self.client.query(query).result()
            return list(result)[0].count
        except:
            return 0
    
    def fix_field_mismatches(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿®å¤å­—æ®µä¸åŒ¹é…é—®é¢˜"""
        logger.info("ğŸ”§ å¼€å§‹ä¿®å¤å­—æ®µä¸åŒ¹é…é—®é¢˜...")
        
        fix_results = {
            "timestamp": datetime.now().isoformat(),
            "fixes_attempted": [],
            "fixes_successful": [],
            "fixes_failed": [],
            "overall_success": False
        }
        
        # ä¿®å¤p_cloud_clean_merged_dedup_vè§†å›¾
        fix_results["fixes_attempted"].append("p_cloud_clean_merged_dedup_v")
        
        try:
            # æ–°çš„è§†å›¾å®šä¹‰ï¼Œä½¿ç”¨timestampè€Œä¸æ˜¯ts_utc
            new_view_sql = f"""
            CREATE OR REPLACE VIEW `{self.project_id}.{self.dataset_id}.p_cloud_clean_merged_dedup_v` AS
            SELECT 
                period,
                timestamp as ts_utc,  -- å°†timestampå­—æ®µæ˜ å°„ä¸ºts_utc
                p_win as p_even,
                source as src
            FROM (
                SELECT 
                    period,
                    timestamp,
                    p_win,
                    source,
                    ROW_NUMBER() OVER (PARTITION BY period ORDER BY timestamp DESC) as rn
                FROM `{self.project_id}.{self.dataset_id}.cloud_pred_today_norm`
                WHERE p_win IS NOT NULL
            )
            WHERE rn = 1
            """
            
            job = self.client.query(new_view_sql)
            job.result()  # ç­‰å¾…å®Œæˆ
            
            fix_results["fixes_successful"].append("p_cloud_clean_merged_dedup_v")
            logger.info("âœ… p_cloud_clean_merged_dedup_v ä¿®å¤æˆåŠŸ")
            
        except Exception as e:
            fix_results["fixes_failed"].append({
                "view": "p_cloud_clean_merged_dedup_v",
                "error": str(e)
            })
            logger.error(f"âŒ p_cloud_clean_merged_dedup_v ä¿®å¤å¤±è´¥: {e}")
        
        # ä¿®å¤p_cloud_today_vè§†å›¾
        fix_results["fixes_attempted"].append("p_cloud_today_v")
        
        try:
            # æ–°çš„è§†å›¾å®šä¹‰
            new_view_sql = f"""
            CREATE OR REPLACE VIEW `{self.project_id}.{self.dataset_id}.p_cloud_today_v` AS
            SELECT 
                period,
                timestamp as ts_utc,  -- å°†timestampå­—æ®µæ˜ å°„ä¸ºts_utc
                p_win as p_even,
                source as src
            FROM `{self.project_id}.{self.dataset_id}.p_cloud_clean_merged_dedup_v`
            WHERE DATE(timestamp) = CURRENT_DATE('Asia/Shanghai')
            """
            
            job = self.client.query(new_view_sql)
            job.result()  # ç­‰å¾…å®Œæˆ
            
            fix_results["fixes_successful"].append("p_cloud_today_v")
            logger.info("âœ… p_cloud_today_v ä¿®å¤æˆåŠŸ")
            
        except Exception as e:
            fix_results["fixes_failed"].append({
                "view": "p_cloud_today_v", 
                "error": str(e)
            })
            logger.error(f"âŒ p_cloud_today_v ä¿®å¤å¤±è´¥: {e}")
        
        # è®¡ç®—æ•´ä½“æˆåŠŸç‡
        total_attempted = len(fix_results["fixes_attempted"])
        total_successful = len(fix_results["fixes_successful"])
        fix_results["overall_success"] = total_successful == total_attempted and total_attempted > 0
        
        logger.info(f"ğŸ¯ å­—æ®µä¿®å¤å®Œæˆ: {total_successful}/{total_attempted} æˆåŠŸ")
        
        return fix_results
    
    def verify_fixes(self) -> Dict[str, Any]:
        """éªŒè¯ä¿®å¤æ•ˆæœ"""
        logger.info("ğŸ” éªŒè¯ä¿®å¤æ•ˆæœ...")
        
        verification = {
            "timestamp": datetime.now().isoformat(),
            "view_tests": {},
            "data_flow_test": {},
            "overall_health": False
        }
        
        # æµ‹è¯•ä¿®å¤çš„è§†å›¾
        views_to_test = [
            "p_cloud_clean_merged_dedup_v",
            "p_cloud_today_v"
        ]
        
        for view_name in views_to_test:
            try:
                # æµ‹è¯•è§†å›¾æ˜¯å¦å¯è®¿é—®
                query = f"SELECT COUNT(*) as count FROM `{self.project_id}.{self.dataset_id}.{view_name}`"
                result = self.client.query(query).result()
                count = list(result)[0].count
                
                verification["view_tests"][view_name] = {
                    "accessible": True,
                    "row_count": count,
                    "healthy": count > 0
                }
                
                logger.info(f"âœ… {view_name}: {count} è¡Œ")
                
            except Exception as e:
                verification["view_tests"][view_name] = {
                    "accessible": False,
                    "error": str(e),
                    "healthy": False
                }
                logger.error(f"âŒ {view_name}: {e}")
        
        # æµ‹è¯•æ•°æ®æµ
        try:
            # æµ‹è¯•signal_pool_union_v3
            query = f"SELECT COUNT(*) as count FROM `{self.project_id}.{self.dataset_id}.signal_pool_union_v3`"
            result = self.client.query(query).result()
            signal_count = list(result)[0].count
            
            # æµ‹è¯•lab_push_candidates_v2
            query = f"SELECT COUNT(*) as count FROM `{self.project_id}.{self.dataset_id}.lab_push_candidates_v2`"
            result = self.client.query(query).result()
            candidates_count = list(result)[0].count
            
            verification["data_flow_test"] = {
                "signal_pool_count": signal_count,
                "candidates_count": candidates_count,
                "data_flow_healthy": signal_count > 0 and candidates_count > 0
            }
            
            logger.info(f"ğŸ“Š æ•°æ®æµæµ‹è¯•: ä¿¡å·æ±  {signal_count} è¡Œ, å†³ç­–å€™é€‰ {candidates_count} è¡Œ")
            
        except Exception as e:
            verification["data_flow_test"] = {
                "error": str(e),
                "data_flow_healthy": False
            }
            logger.error(f"âŒ æ•°æ®æµæµ‹è¯•å¤±è´¥: {e}")
        
        # è®¡ç®—æ•´ä½“å¥åº·çŠ¶æ€
        all_views_healthy = all(
            test.get("healthy", False) 
            for test in verification["view_tests"].values()
        )
        data_flow_healthy = verification["data_flow_test"].get("data_flow_healthy", False)
        
        verification["overall_health"] = all_views_healthy and data_flow_healthy
        
        return verification
    
    def run_complete_field_fix(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„å­—æ®µä¿®å¤æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹PC28å­—æ®µä¸åŒ¹é…ä¿®å¤...")
        
        complete_results = {
            "fix_timestamp": self.timestamp,
            "analysis": {},
            "fixes": {},
            "verification": {},
            "overall_success": False
        }
        
        # 1. åˆ†æé—®é¢˜
        complete_results["analysis"] = self.analyze_field_issues()
        
        # 2. ä¿®å¤é—®é¢˜
        complete_results["fixes"] = self.fix_field_mismatches(complete_results["analysis"])
        
        # 3. éªŒè¯ä¿®å¤
        complete_results["verification"] = self.verify_fixes()
        
        # 4. è®¡ç®—æ•´ä½“æˆåŠŸ
        complete_results["overall_success"] = (
            complete_results["fixes"]["overall_success"] and
            complete_results["verification"]["overall_health"]
        )
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        self._generate_field_fix_report(complete_results)
        
        return complete_results
    
    def _generate_field_fix_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆå­—æ®µä¿®å¤æŠ¥å‘Š"""
        report_path = f"/Users/a606/cloud_function_source/pc28_field_fix_report_{self.timestamp}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ å­—æ®µä¿®å¤æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    fixer = PC28FieldMismatchFixer()
    
    print("ğŸ”§ PC28å­—æ®µä¸åŒ¹é…ä¿®å¤ç³»ç»Ÿ")
    print("=" * 50)
    print("ğŸ¯ ç›®æ ‡ï¼šè§£å†³ts_utcå­—æ®µé—®é¢˜å’Œå…¶ä»–å­—æ®µä¸åŒ¹é…")
    print("ğŸ“‹ ä¿®å¤èŒƒå›´ï¼šp_cloud_clean_merged_dedup_v, p_cloud_today_v")
    print("=" * 50)
    
    # è¿è¡Œå®Œæ•´ä¿®å¤
    results = fixer.run_complete_field_fix()
    
    # è¾“å‡ºç»“æœ
    analysis = results["analysis"]
    fixes = results["fixes"]
    verification = results["verification"]
    
    print(f"\nğŸ“Š åˆ†æç»“æœ:")
    print(f"  å‘ç°é—®é¢˜: {len(analysis['issues_found'])} ä¸ª")
    print(f"  æºè¡¨çŠ¶æ€: {len([t for t in analysis['source_tables'].values() if t.get('accessible', False)])}/{len(analysis['source_tables'])} å¯è®¿é—®")
    
    print(f"\nğŸ”§ ä¿®å¤ç»“æœ:")
    print(f"  ä¿®å¤å°è¯•: {len(fixes['fixes_attempted'])} ä¸ª")
    print(f"  ä¿®å¤æˆåŠŸ: {len(fixes['fixes_successful'])} ä¸ª")
    print(f"  ä¿®å¤å¤±è´¥: {len(fixes['fixes_failed'])} ä¸ª")
    
    print(f"\nğŸ” éªŒè¯ç»“æœ:")
    healthy_views = len([v for v in verification['view_tests'].values() if v.get('healthy', False)])
    total_views = len(verification['view_tests'])
    print(f"  è§†å›¾å¥åº·: {healthy_views}/{total_views}")
    
    if verification.get('data_flow_test', {}).get('data_flow_healthy', False):
        signal_count = verification['data_flow_test']['signal_pool_count']
        candidates_count = verification['data_flow_test']['candidates_count']
        print(f"  æ•°æ®æµ: âœ… ä¿¡å·æ±  {signal_count} è¡Œ, å†³ç­–å€™é€‰ {candidates_count} è¡Œ")
    else:
        print(f"  æ•°æ®æµ: âŒ å¼‚å¸¸")
    
    if results["overall_success"]:
        print(f"\nğŸ‰ å­—æ®µä¿®å¤å®Œæˆï¼")
        print(f"ğŸ’¡ æ‰€æœ‰å­—æ®µä¸åŒ¹é…é—®é¢˜å·²è§£å†³ï¼Œæ•°æ®æµæ¢å¤æ­£å¸¸")
    else:
        print(f"\nâš ï¸ å­—æ®µä¿®å¤æœªå®Œå…¨æˆåŠŸï¼Œè¯·æ£€æŸ¥è¯¦ç»†æŠ¥å‘Š")
    
    return results

if __name__ == "__main__":
    main()