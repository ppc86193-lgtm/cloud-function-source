#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PC28ç³»ç»Ÿè‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œå™¨
è‡ªåŠ¨è¿è¡Œpytestå¹¶æ•è·å®Œæ•´çš„æ‰§è¡Œæ—¥å¿—ï¼Œç¡®ä¿æ—¥å¿—çš„è¯æ®èƒ½åŠ›
"""

import subprocess
import json
import time
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging

class AutomatedTestRunner:
    """è‡ªåŠ¨åŒ–æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.output_dir = self.project_root / "test_evidence" / f"run_{self.timestamp}"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_file = self.output_dir / "test_execution.log"
        
        # åˆ›å»ºlogger
        self.logger = logging.getLogger('AutomatedTestRunner')
        self.logger.setLevel(logging.INFO)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
    def run_pytest_with_full_logging(self, test_paths: List[str] = None, 
                                   markers: List[str] = None,
                                   verbose: bool = True) -> Dict[str, Any]:
        """
        è¿è¡Œpytestå¹¶æ•è·å®Œæ•´æ—¥å¿—
        
        Args:
            test_paths: æµ‹è¯•è·¯å¾„åˆ—è¡¨
            markers: æµ‹è¯•æ ‡è®°åˆ—è¡¨
            verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
            
        Returns:
            æµ‹è¯•æ‰§è¡Œç»“æœå­—å…¸
        """
        self.logger.info("="*80)
        self.logger.info(f"å¼€å§‹è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œ - {self.timestamp}")
        self.logger.info("="*80)
        
        # æ„å»ºpytestå‘½ä»¤
        cmd = self._build_pytest_command(test_paths, markers, verbose)
        
        self.logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        # è®°å½•ç¯å¢ƒä¿¡æ¯
        self._log_environment_info()
        
        # æ‰§è¡Œæµ‹è¯•
        start_time = time.time()
        result = self._execute_pytest(cmd)
        execution_time = time.time() - start_time
        
        # å¤„ç†ç»“æœ
        test_result = self._process_test_results(result, execution_time)
        
        # ä¿å­˜ç»“æœ
        self._save_test_evidence(test_result)
        
        self.logger.info("="*80)
        self.logger.info(f"æµ‹è¯•æ‰§è¡Œå®Œæˆ - æ€»è€—æ—¶: {execution_time:.2f}ç§’")
        self.logger.info("="*80)
        
        return test_result
        
    def _build_pytest_command(self, test_paths: List[str] = None,
                            markers: List[str] = None,
                            verbose: bool = True) -> List[str]:
        """æ„å»ºpytestå‘½ä»¤"""
        cmd = [
            sys.executable, "-m", "pytest",
            "--tb=long",  # è¯¦ç»†çš„é”™è¯¯å›æº¯
            "--capture=no",  # ä¸æ•è·è¾“å‡ºï¼Œæ˜¾ç¤ºæ‰€æœ‰print
            "--durations=10",  # æ˜¾ç¤ºæœ€æ…¢çš„10ä¸ªæµ‹è¯•
            "--strict-markers",  # ä¸¥æ ¼æ ‡è®°æ¨¡å¼
            f"--junitxml={self.output_dir}/junit_report.xml",  # JUnit XMLæŠ¥å‘Š
            f"--html={self.output_dir}/html_report.html",  # HTMLæŠ¥å‘Š
            "--self-contained-html",  # è‡ªåŒ…å«HTML
            f"--json-report",  # JSONæŠ¥å‘Š
            f"--json-report-file={self.output_dir}/json_report.json"
        ]
        
        if verbose:
            cmd.append("-v")
            
        # æ·»åŠ æµ‹è¯•è·¯å¾„
        if test_paths:
            cmd.extend(test_paths)
        else:
            # é»˜è®¤æµ‹è¯•è·¯å¾„
            if (self.project_root / "tests").exists():
                cmd.append("tests/")
            else:
                cmd.append(".")
                
        # æ·»åŠ æ ‡è®°è¿‡æ»¤
        if markers:
            cmd.extend(["-m", " or ".join(markers)])
            
        return cmd
        
    def _log_environment_info(self):
        """è®°å½•ç¯å¢ƒä¿¡æ¯"""
        self.logger.info("ç¯å¢ƒä¿¡æ¯:")
        self.logger.info(f"  Pythonç‰ˆæœ¬: {sys.version}")
        self.logger.info(f"  å·¥ä½œç›®å½•: {os.getcwd()}")
        self.logger.info(f"  é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        self.logger.info(f"  è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # è®°å½•å·²å®‰è£…çš„åŒ…
        try:
            import pkg_resources
            installed_packages = [f"{d.project_name}=={d.version}" 
                                for d in pkg_resources.working_set]
            self.logger.info(f"  å·²å®‰è£…åŒ…æ•°é‡: {len(installed_packages)}")
            
            # ä¿å­˜åŒ…åˆ—è¡¨åˆ°æ–‡ä»¶
            with open(self.output_dir / "installed_packages.txt", "w") as f:
                f.write("\n".join(sorted(installed_packages)))
                
        except Exception as e:
            self.logger.warning(f"æ— æ³•è·å–åŒ…ä¿¡æ¯: {e}")
            
    def _execute_pytest(self, cmd: List[str]) -> subprocess.CompletedProcess:
        """æ‰§è¡Œpytestå‘½ä»¤"""
        self.logger.info("å¼€å§‹æ‰§è¡Œpytest...")
        
        try:
            # æ‰§è¡Œå‘½ä»¤å¹¶å®æ—¶æ•è·è¾“å‡º
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                cwd=self.project_root
            )
            
            # å®æ—¶è®°å½•è¾“å‡º
            output_lines = []
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    line = output.strip()
                    output_lines.append(line)
                    self.logger.info(f"PYTEST: {line}")
                    
            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            return_code = process.poll()
            
            # åˆ›å»ºç»“æœå¯¹è±¡
            result = subprocess.CompletedProcess(
                cmd, return_code, 
                stdout="\n".join(output_lines),
                stderr=""
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"æ‰§è¡Œpytestå¤±è´¥: {e}")
            raise
            
    def _process_test_results(self, result: subprocess.CompletedProcess, 
                            execution_time: float) -> Dict[str, Any]:
        """å¤„ç†æµ‹è¯•ç»“æœ"""
        self.logger.info("å¤„ç†æµ‹è¯•ç»“æœ...")
        
        test_result = {
            "execution_info": {
                "timestamp": self.timestamp,
                "execution_time": execution_time,
                "return_code": result.returncode,
                "command": " ".join(result.args),
                "success": result.returncode == 0
            },
            "output": {
                "stdout": result.stdout,
                "stderr": result.stderr or ""
            },
            "files_generated": [],
            "summary": {}
        }
        
        # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
        for file_path in self.output_dir.glob("*"):
            if file_path.is_file():
                test_result["files_generated"].append({
                    "name": file_path.name,
                    "size": file_path.stat().st_size,
                    "path": str(file_path)
                })
                
        # å°è¯•è§£æJSONæŠ¥å‘Š
        json_report_path = self.output_dir / "json_report.json"
        if json_report_path.exists():
            try:
                with open(json_report_path, 'r', encoding='utf-8') as f:
                    json_report = json.load(f)
                    test_result["summary"] = {
                        "total": json_report.get("summary", {}).get("total", 0),
                        "passed": json_report.get("summary", {}).get("passed", 0),
                        "failed": json_report.get("summary", {}).get("failed", 0),
                        "skipped": json_report.get("summary", {}).get("skipped", 0),
                        "error": json_report.get("summary", {}).get("error", 0)
                    }
            except Exception as e:
                self.logger.warning(f"æ— æ³•è§£æJSONæŠ¥å‘Š: {e}")
                
        # è®°å½•ç»“æœæ‘˜è¦
        self.logger.info(f"æµ‹è¯•æ‰§è¡Œç»“æœ:")
        self.logger.info(f"  è¿”å›ç : {result.returncode}")
        self.logger.info(f"  æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        self.logger.info(f"  æˆåŠŸ: {'æ˜¯' if result.returncode == 0 else 'å¦'}")
        
        if test_result["summary"]:
            summary = test_result["summary"]
            self.logger.info(f"  æµ‹è¯•ç»Ÿè®¡:")
            self.logger.info(f"    æ€»è®¡: {summary.get('total', 0)}")
            self.logger.info(f"    é€šè¿‡: {summary.get('passed', 0)}")
            self.logger.info(f"    å¤±è´¥: {summary.get('failed', 0)}")
            self.logger.info(f"    è·³è¿‡: {summary.get('skipped', 0)}")
            self.logger.info(f"    é”™è¯¯: {summary.get('error', 0)}")
            
        return test_result
        
    def _save_test_evidence(self, test_result: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•è¯æ®"""
        self.logger.info("ä¿å­˜æµ‹è¯•è¯æ®...")
        
        # ä¿å­˜å®Œæ•´çš„æµ‹è¯•ç»“æœ
        evidence_file = self.output_dir / "test_evidence.json"
        with open(evidence_file, 'w', encoding='utf-8') as f:
            json.dump(test_result, f, ensure_ascii=False, indent=2)
            
        # åˆ›å»ºè¯æ®æ‘˜è¦
        summary_file = self.output_dir / "evidence_summary.md"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"# æµ‹è¯•æ‰§è¡Œè¯æ®æ‘˜è¦\n\n")
            f.write(f"**æ‰§è¡Œæ—¶é—´**: {test_result['execution_info']['timestamp']}\n")
            f.write(f"**æ‰§è¡Œè€—æ—¶**: {test_result['execution_info']['execution_time']:.2f}ç§’\n")
            f.write(f"**æ‰§è¡Œå‘½ä»¤**: `{test_result['execution_info']['command']}`\n")
            f.write(f"**è¿”å›ç **: {test_result['execution_info']['return_code']}\n")
            f.write(f"**æ‰§è¡ŒçŠ¶æ€**: {'æˆåŠŸ' if test_result['execution_info']['success'] else 'å¤±è´¥'}\n\n")
            
            if test_result["summary"]:
                f.write("## æµ‹è¯•ç»Ÿè®¡\n\n")
                summary = test_result["summary"]
                f.write(f"- æ€»æµ‹è¯•æ•°: {summary.get('total', 0)}\n")
                f.write(f"- é€šè¿‡: {summary.get('passed', 0)}\n")
                f.write(f"- å¤±è´¥: {summary.get('failed', 0)}\n")
                f.write(f"- è·³è¿‡: {summary.get('skipped', 0)}\n")
                f.write(f"- é”™è¯¯: {summary.get('error', 0)}\n\n")
                
            f.write("## ç”Ÿæˆçš„æ–‡ä»¶\n\n")
            for file_info in test_result["files_generated"]:
                f.write(f"- {file_info['name']} ({file_info['size']} bytes)\n")
                
        self.logger.info(f"æµ‹è¯•è¯æ®å·²ä¿å­˜åˆ°: {self.output_dir}")
        
    def run_specific_test_suites(self) -> Dict[str, Any]:
        """è¿è¡Œç‰¹å®šçš„æµ‹è¯•å¥—ä»¶"""
        all_results = {}
        
        # å®šä¹‰æµ‹è¯•å¥—ä»¶
        test_suites = [
            {
                "name": "unit_tests",
                "markers": ["unit"],
                "description": "å•å…ƒæµ‹è¯•"
            },
            {
                "name": "integration_tests", 
                "markers": ["integration"],
                "description": "é›†æˆæµ‹è¯•"
            },
            {
                "name": "performance_tests",
                "markers": ["performance"],
                "description": "æ€§èƒ½æµ‹è¯•"
            },
            {
                "name": "realtime_tests",
                "markers": ["realtime"],
                "description": "å®æ—¶ç³»ç»Ÿæµ‹è¯•"
            },
            {
                "name": "consistency_tests",
                "markers": ["consistency"],
                "description": "æ•°æ®ä¸€è‡´æ€§æµ‹è¯•"
            }
        ]
        
        for suite in test_suites:
            self.logger.info(f"æ‰§è¡Œæµ‹è¯•å¥—ä»¶: {suite['description']}")
            
            try:
                result = self.run_pytest_with_full_logging(
                    markers=suite["markers"]
                )
                all_results[suite["name"]] = result
                
            except Exception as e:
                self.logger.error(f"æµ‹è¯•å¥—ä»¶ {suite['name']} æ‰§è¡Œå¤±è´¥: {e}")
                all_results[suite["name"]] = {
                    "error": str(e),
                    "success": False
                }
                
        return all_results

def main():
    """ä¸»å‡½æ•°"""
    print("PC28ç³»ç»Ÿè‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œå™¨")
    print("="*50)
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = AutomatedTestRunner()
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        print("å¼€å§‹æ‰§è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶...")
        results = runner.run_pytest_with_full_logging()
        
        if results["execution_info"]["success"]:
            print("âœ… æ‰€æœ‰æµ‹è¯•æ‰§è¡ŒæˆåŠŸ")
        else:
            print("âŒ æµ‹è¯•æ‰§è¡Œä¸­å‘ç°é—®é¢˜")
            
        print(f"ğŸ“ æµ‹è¯•è¯æ®ä¿å­˜åœ¨: {runner.output_dir}")
        
        # æ˜¾ç¤ºæ‘˜è¦
        if results.get("summary"):
            summary = results["summary"]
            print(f"\nğŸ“Š æµ‹è¯•æ‘˜è¦:")
            print(f"   æ€»è®¡: {summary.get('total', 0)}")
            print(f"   é€šè¿‡: {summary.get('passed', 0)}")
            print(f"   å¤±è´¥: {summary.get('failed', 0)}")
            print(f"   è·³è¿‡: {summary.get('skipped', 0)}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 1
        
    return 0

if __name__ == "__main__":
    sys.exit(main())