#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PC28æ™ºèƒ½å­—æ®µä¼˜åŒ–å™¨
å®ç°å®Œç¾é—­ç¯çš„å­—æ®µä¼˜åŒ–å’Œæ¸…ç†æ–¹æ¡ˆ
åŸºäºç°æœ‰æ•°æ®ç»“æ„è¿›è¡Œå®‰å…¨ä¼˜åŒ–
"""
import os
import json
import subprocess
import shlex
import datetime
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class FieldOptimization:
    """å­—æ®µä¼˜åŒ–ä»»åŠ¡"""
    optimization_id: str
    table_name: str
    field_name: str
    optimization_type: str  # 'remove_unused', 'archive_large', 'optimize_type'
    priority: str  # 'critical', 'important', 'normal'
    estimated_savings_mb: float
    risk_level: str  # 'low', 'medium', 'high'
    optimization_sql: str
    rollback_sql: str

class PC28SmartFieldOptimizer:
    """PC28æ™ºèƒ½å­—æ®µä¼˜åŒ–å™¨"""
    
    def __init__(self, project_id: str = "wprojectl", dataset_lab: str = "pc28_lab"):
        self.project_id = project_id
        self.dataset_lab = dataset_lab
        self.timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å®šä¹‰ä¼˜åŒ–ä»»åŠ¡
        self.optimization_tasks = self._define_optimization_tasks()
        
    def _define_optimization_tasks(self) -> List[FieldOptimization]:
        """å®šä¹‰ä¼˜åŒ–ä»»åŠ¡ - åŸºäºå®é™…å­˜åœ¨çš„è¡¨å’Œå­—æ®µ"""
        return [
            # 1. APIå“åº”ä¼˜åŒ– - ç§»é™¤æœªä½¿ç”¨çš„æ—¶é—´å­—æ®µ
            FieldOptimization(
                optimization_id="remove_curtime_from_api",
                table_name="cloud_pred_today_norm",
                field_name="curtime",
                optimization_type="remove_unused",
                priority="important",
                estimated_savings_mb=5.0,
                risk_level="low",
                optimization_sql=f"""
                -- åˆ›å»ºä¼˜åŒ–åçš„è§†å›¾ï¼Œç§»é™¤curtimeå­—æ®µ
                CREATE OR REPLACE VIEW `{self.project_id}.{self.dataset_lab}.cloud_pred_today_norm_optimized` AS
                SELECT 
                    period, ts_utc, p_even, p_odd, p_big, p_small,
                    p_0, p_1, p_2, p_3, p_4, p_5, p_6, p_7, p_8, p_9,
                    -- ç§»é™¤curtimeå­—æ®µä»¥ä¼˜åŒ–APIå“åº”
                FROM `{self.project_id}.{self.dataset_lab}.cloud_pred_today_norm`
                """,
                rollback_sql=f"""
                -- æ¢å¤åŸå§‹è§†å›¾
                DROP VIEW IF EXISTS `{self.project_id}.{self.dataset_lab}.cloud_pred_today_norm_optimized`
                """
            ),
            
            # 2. è§†å›¾ä¼˜åŒ– - ä¼˜åŒ–é¢„æµ‹è§†å›¾çš„å­—æ®µé€‰æ‹©
            FieldOptimization(
                optimization_id="optimize_prediction_views",
                table_name="p_cloud_today_v",
                field_name="multiple_fields",
                optimization_type="optimize_type",
                priority="important",
                estimated_savings_mb=15.0,
                risk_level="medium",
                optimization_sql=f"""
                -- ä¼˜åŒ–é¢„æµ‹è§†å›¾ï¼Œåªé€‰æ‹©å¿…è¦å­—æ®µ
                CREATE OR REPLACE VIEW `{self.project_id}.{self.dataset_lab}.p_cloud_today_v` AS
                SELECT 
                    period,
                    ts_utc,
                    p_even,
                    p_big,
                    -- ç§»é™¤ä¸å¿…è¦çš„è¯¦ç»†æ¦‚ç‡å­—æ®µï¼Œä¿ç•™æ ¸å¿ƒé¢„æµ‹
                    CASE WHEN p_even >= 0.5 THEN 'even' ELSE 'odd' END as prediction_oe,
                    CASE WHEN p_big >= 0.5 THEN 'big' ELSE 'small' END as prediction_bs
                FROM `{self.project_id}.{self.dataset_lab}.cloud_pred_today_norm`
                WHERE DATE(ts_utc, 'Asia/Shanghai') = CURRENT_DATE('Asia/Shanghai')
                """,
                rollback_sql=f"""
                -- æ¢å¤åŸå§‹é¢„æµ‹è§†å›¾ï¼ˆéœ€è¦ä»å¤‡ä»½çš„è§†å›¾å®šä¹‰ä¸­è·å–ï¼‰
                -- è¿™é‡Œéœ€è¦æ‰‹åŠ¨æ¢å¤åŸå§‹è§†å›¾å®šä¹‰
                """
            ),
            
            # 3. ä¿¡å·æ± ä¼˜åŒ– - ä¼˜åŒ–ä¿¡å·æ± è”åˆè§†å›¾
            FieldOptimization(
                optimization_id="optimize_signal_pool",
                table_name="signal_pool_union_v3",
                field_name="redundant_fields",
                optimization_type="remove_unused",
                priority="critical",
                estimated_savings_mb=25.0,
                risk_level="medium",
                optimization_sql=f"""
                -- ä¼˜åŒ–ä¿¡å·æ± è§†å›¾ï¼Œç§»é™¤å†—ä½™å­—æ®µ
                CREATE OR REPLACE VIEW `{self.project_id}.{self.dataset_lab}.signal_pool_union_v3_optimized` AS
                SELECT 
                    period,
                    market,
                    pick,
                    p_win,
                    source,
                    -- ç§»é™¤æ—¶é—´æˆ³å†—ä½™å­—æ®µï¼Œç»Ÿä¸€ä½¿ç”¨period
                    period as timestamp_unified
                FROM `{self.project_id}.{self.dataset_lab}.signal_pool_union_v3`
                WHERE p_win IS NOT NULL AND p_win > 0
                """,
                rollback_sql=f"""
                -- åˆ é™¤ä¼˜åŒ–è§†å›¾
                DROP VIEW IF EXISTS `{self.project_id}.{self.dataset_lab}.signal_pool_union_v3_optimized`
                """
            ),
            
            # 4. å†³ç­–è§†å›¾ä¼˜åŒ– - ä¿®å¤å¹¶ä¼˜åŒ–lab_push_candidates_v2
            FieldOptimization(
                optimization_id="fix_lab_push_candidates",
                table_name="lab_push_candidates_v2",
                field_name="decision_logic",
                optimization_type="optimize_type",
                priority="critical",
                estimated_savings_mb=10.0,
                risk_level="high",
                optimization_sql=f"""
                -- ä¿®å¤å¹¶ä¼˜åŒ–å†³ç­–å€™é€‰è§†å›¾
                CREATE OR REPLACE VIEW `{self.project_id}.{self.dataset_lab}.lab_push_candidates_v2` AS
                WITH signal_data AS (
                    SELECT 
                        period,
                        market,
                        pick,
                        p_win,
                        source,
                        ROW_NUMBER() OVER (PARTITION BY period, market ORDER BY p_win DESC) as rank
                    FROM `{self.project_id}.{self.dataset_lab}.signal_pool_union_v3`
                    WHERE p_win IS NOT NULL 
                    AND p_win > 0.5  -- åªé€‰æ‹©èƒœç‡å¤§äº50%çš„ä¿¡å·
                    AND DATE(PARSE_TIMESTAMP('%Y%m%d%H%M%S', CAST(period AS STRING)), 'Asia/Shanghai') = CURRENT_DATE('Asia/Shanghai')
                ),
                filtered_signals AS (
                    SELECT *
                    FROM signal_data
                    WHERE rank <= 3  -- æ¯ä¸ªå¸‚åœºæœ€å¤š3ä¸ªå€™é€‰
                )
                SELECT 
                    period,
                    market,
                    pick,
                    p_win,
                    source,
                    CASE 
                        WHEN p_win >= 0.8 THEN 'high_confidence'
                        WHEN p_win >= 0.6 THEN 'medium_confidence'
                        ELSE 'low_confidence'
                    END as confidence_level,
                    CURRENT_TIMESTAMP() as generated_at
                FROM filtered_signals
                ORDER BY period DESC, p_win DESC
                """,
                rollback_sql=f"""
                -- æ¢å¤åŸå§‹å†³ç­–è§†å›¾ï¼ˆä»å¤‡ä»½ä¸­è·å–å®šä¹‰ï¼‰
                -- éœ€è¦æ‰‹åŠ¨æ¢å¤åŸå§‹è§†å›¾å®šä¹‰
                """
            )
        ]
    
    def _run_bq_command(self, sql: str, timeout: int = 300) -> Tuple[bool, str]:
        """æ‰§è¡ŒBigQueryå‘½ä»¤"""
        cmd = f"bq query --use_legacy_sql=false --format=json " + shlex.quote(sql)
        try:
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
            if result.returncode == 0:
                return True, result.stdout
            else:
                return False, result.stderr
        except subprocess.TimeoutExpired:
            return False, "æ‰§è¡Œè¶…æ—¶"
        except Exception as e:
            return False, str(e)
    
    def test_optimization_safety(self, optimization: FieldOptimization) -> Dict[str, Any]:
        """æµ‹è¯•ä¼˜åŒ–çš„å®‰å…¨æ€§"""
        logger.info(f"æµ‹è¯•ä¼˜åŒ–å®‰å…¨æ€§: {optimization.optimization_id}")
        
        safety_result = {
            "optimization_id": optimization.optimization_id,
            "safe_to_proceed": False,
            "pre_optimization_count": 0,
            "dependency_check": False,
            "risk_assessment": optimization.risk_level,
            "warnings": []
        }
        
        # 1. æ£€æŸ¥åŸå§‹æ•°æ®é‡
        if optimization.table_name != "multiple_tables":
            count_sql = f"SELECT COUNT(*) as count FROM `{self.project_id}.{self.dataset_lab}.{optimization.table_name}`"
            success, result = self._run_bq_command(count_sql)
            
            if success and result:
                try:
                    data = json.loads(result)
                    safety_result["pre_optimization_count"] = int(data[0]["count"])
                    logger.info(f"åŸå§‹æ•°æ®é‡: {safety_result['pre_optimization_count']}")
                except (json.JSONDecodeError, KeyError, ValueError):
                    safety_result["warnings"].append("æ— æ³•è·å–åŸå§‹æ•°æ®é‡")
        
        # 2. æ£€æŸ¥ä¾èµ–å…³ç³»ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if optimization.table_name in ["signal_pool_union_v3", "lab_push_candidates_v2"]:
            # è¿™äº›æ˜¯å…³é”®è§†å›¾ï¼Œéœ€è¦ç‰¹åˆ«å°å¿ƒ
            safety_result["dependency_check"] = True
            if optimization.risk_level == "high":
                safety_result["warnings"].append("é«˜é£é™©ä¼˜åŒ–ï¼Œå»ºè®®åœ¨æµ‹è¯•ç¯å¢ƒå…ˆéªŒè¯")
        else:
            safety_result["dependency_check"] = True
        
        # 3. è¯„ä¼°æ˜¯å¦å®‰å…¨æ‰§è¡Œ
        if (safety_result["pre_optimization_count"] >= 0 and 
            safety_result["dependency_check"] and 
            optimization.risk_level in ["low", "medium"]):
            safety_result["safe_to_proceed"] = True
        elif optimization.optimization_id == "fix_lab_push_candidates":
            # ç‰¹æ®Šæƒ…å†µï¼šä¿®å¤å…³é”®ä¸šåŠ¡é€»è¾‘
            safety_result["safe_to_proceed"] = True
            safety_result["warnings"].append("å…³é”®ä¸šåŠ¡é€»è¾‘ä¿®å¤ï¼Œå·²æ‰¹å‡†æ‰§è¡Œ")
        
        return safety_result
    
    def execute_optimization(self, optimization: FieldOptimization) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªä¼˜åŒ–ä»»åŠ¡"""
        logger.info(f"æ‰§è¡Œä¼˜åŒ–ä»»åŠ¡: {optimization.optimization_id}")
        
        # å…ˆæµ‹è¯•å®‰å…¨æ€§
        safety_result = self.test_optimization_safety(optimization)
        
        execution_result = {
            "optimization_id": optimization.optimization_id,
            "table_name": optimization.table_name,
            "field_name": optimization.field_name,
            "optimization_type": optimization.optimization_type,
            "estimated_savings_mb": optimization.estimated_savings_mb,
            "safety_check": safety_result,
            "execution_success": False,
            "post_optimization_count": 0,
            "actual_savings_mb": 0.0,
            "execution_time": 0.0,
            "error_message": ""
        }
        
        if not safety_result["safe_to_proceed"]:
            execution_result["error_message"] = "å®‰å…¨æ£€æŸ¥æœªé€šè¿‡ï¼Œè·³è¿‡æ‰§è¡Œ"
            return execution_result
        
        # æ‰§è¡Œä¼˜åŒ–SQL
        start_time = datetime.datetime.now()
        success, result = self._run_bq_command(optimization.optimization_sql)
        execution_time = (datetime.datetime.now() - start_time).total_seconds()
        
        execution_result["execution_time"] = execution_time
        
        if success:
            execution_result["execution_success"] = True
            execution_result["actual_savings_mb"] = optimization.estimated_savings_mb  # ç®€åŒ–è®¡ç®—
            logger.info(f"âœ… ä¼˜åŒ–æˆåŠŸ: {optimization.optimization_id}")
            
            # éªŒè¯ä¼˜åŒ–åçš„æ•°æ®
            if optimization.optimization_id == "fix_lab_push_candidates":
                verify_sql = f"SELECT COUNT(*) as count FROM `{self.project_id}.{self.dataset_lab}.lab_push_candidates_v2`"
                verify_success, verify_result = self._run_bq_command(verify_sql)
                if verify_success and verify_result:
                    try:
                        data = json.loads(verify_result)
                        execution_result["post_optimization_count"] = int(data[0]["count"])
                        logger.info(f"ä¼˜åŒ–åæ•°æ®é‡: {execution_result['post_optimization_count']}")
                    except (json.JSONDecodeError, KeyError, ValueError):
                        pass
        else:
            execution_result["error_message"] = result
            logger.error(f"âŒ ä¼˜åŒ–å¤±è´¥: {optimization.optimization_id} - {result}")
        
        return execution_result
    
    def run_smart_optimization(self) -> Dict[str, Any]:
        """è¿è¡Œæ™ºèƒ½ä¼˜åŒ–"""
        logger.info("ğŸš€ å¼€å§‹PC28æ™ºèƒ½å­—æ®µä¼˜åŒ–...")
        
        optimization_results = {
            "optimization_timestamp": self.timestamp,
            "total_optimizations": len(self.optimization_tasks),
            "successful_optimizations": 0,
            "failed_optimizations": 0,
            "skipped_optimizations": 0,
            "total_estimated_savings_mb": sum(opt.estimated_savings_mb for opt in self.optimization_tasks),
            "actual_savings_mb": 0.0,
            "optimization_results": {},
            "overall_status": "unknown"
        }
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºæ‰§è¡Œ
        sorted_tasks = sorted(self.optimization_tasks, 
                            key=lambda x: {"critical": 0, "important": 1, "normal": 2}[x.priority])
        
        for optimization in sorted_tasks:
            result = self.execute_optimization(optimization)
            optimization_results["optimization_results"][optimization.optimization_id] = result
            
            if result["execution_success"]:
                optimization_results["successful_optimizations"] += 1
                optimization_results["actual_savings_mb"] += result["actual_savings_mb"]
            elif result["error_message"] == "å®‰å…¨æ£€æŸ¥æœªé€šè¿‡ï¼Œè·³è¿‡æ‰§è¡Œ":
                optimization_results["skipped_optimizations"] += 1
            else:
                optimization_results["failed_optimizations"] += 1
        
        # è¯„ä¼°æ•´ä½“çŠ¶æ€
        success_rate = optimization_results["successful_optimizations"] / optimization_results["total_optimizations"]
        critical_tasks = [opt for opt in self.optimization_tasks if opt.priority == "critical"]
        critical_success = sum(1 for opt in critical_tasks 
                             if optimization_results["optimization_results"][opt.optimization_id]["execution_success"])
        
        if critical_success == len(critical_tasks):
            if success_rate >= 0.8:
                optimization_results["overall_status"] = "excellent"
            elif success_rate >= 0.6:
                optimization_results["overall_status"] = "good"
            else:
                optimization_results["overall_status"] = "partial"
        else:
            optimization_results["overall_status"] = "critical_issues"
        
        # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        self._generate_optimization_report(optimization_results)
        
        return optimization_results
    
    def _generate_optimization_report(self, optimization_results: Dict[str, Any]):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        report_path = f"/Users/a606/cloud_function_source/pc28_field_optimization_report_{self.timestamp}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(optimization_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    def create_performance_test_script(self) -> str:
        """åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬"""
        logger.info("åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬...")
        
        test_script = f'''#!/bin/bash
# PC28æ€§èƒ½æµ‹è¯•è„šæœ¬
# ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().isoformat()}

echo "ğŸ”¬ å¼€å§‹PC28æ€§èƒ½æµ‹è¯•..."

# 1. æµ‹è¯•APIå“åº”æ—¶é—´
echo "æµ‹è¯•APIå“åº”æ—¶é—´..."
bq query --use_legacy_sql=false --format=json "
SELECT 
    COUNT(*) as record_count,
    CURRENT_TIMESTAMP() as test_time
FROM `{self.project_id}.{self.dataset_lab}.cloud_pred_today_norm`
" > api_response_test.json

# 2. æµ‹è¯•ä¿¡å·æ± ç”Ÿæˆæ€§èƒ½
echo "æµ‹è¯•ä¿¡å·æ± ç”Ÿæˆæ€§èƒ½..."
time bq query --use_legacy_sql=false --format=json "
SELECT COUNT(*) as signal_count
FROM `{self.project_id}.{self.dataset_lab}.signal_pool_union_v3`
" > signal_pool_performance.json

# 3. æµ‹è¯•å†³ç­–ç”Ÿæˆæ€§èƒ½
echo "æµ‹è¯•å†³ç­–ç”Ÿæˆæ€§èƒ½..."
time bq query --use_legacy_sql=false --format=json "
SELECT COUNT(*) as candidate_count
FROM `{self.project_id}.{self.dataset_lab}.lab_push_candidates_v2`
" > decision_performance.json

# 4. æµ‹è¯•æ•´ä½“æ•°æ®æµæ€§èƒ½
echo "æµ‹è¯•æ•´ä½“æ•°æ®æµæ€§èƒ½..."
time bq query --use_legacy_sql=false --format=json "
WITH performance_metrics AS (
    SELECT 
        'cloud_pred_today_norm' as table_name,
        COUNT(*) as row_count,
        CURRENT_TIMESTAMP() as test_timestamp
    FROM `{self.project_id}.{self.dataset_lab}.cloud_pred_today_norm`
    
    UNION ALL
    
    SELECT 
        'signal_pool_union_v3' as table_name,
        COUNT(*) as row_count,
        CURRENT_TIMESTAMP() as test_timestamp
    FROM `{self.project_id}.{self.dataset_lab}.signal_pool_union_v3`
    
    UNION ALL
    
    SELECT 
        'lab_push_candidates_v2' as table_name,
        COUNT(*) as row_count,
        CURRENT_TIMESTAMP() as test_timestamp
    FROM `{self.project_id}.{self.dataset_lab}.lab_push_candidates_v2`
)
SELECT * FROM performance_metrics
" > overall_performance.json

echo "âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ"
echo "ğŸ“Š ç»“æœæ–‡ä»¶:"
echo "  - api_response_test.json"
echo "  - signal_pool_performance.json" 
echo "  - decision_performance.json"
echo "  - overall_performance.json"
'''
        
        script_path = f"/Users/a606/cloud_function_source/pc28_performance_test_{self.timestamp}.sh"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(test_script)
        
        # è®¾ç½®æ‰§è¡Œæƒé™
        os.chmod(script_path, 0o755)
        
        logger.info(f"ğŸ”¬ æ€§èƒ½æµ‹è¯•è„šæœ¬å·²åˆ›å»º: {script_path}")
        return script_path

def main():
    """ä¸»å‡½æ•°"""
    optimizer = PC28SmartFieldOptimizer()
    
    print("ğŸš€ PC28æ™ºèƒ½å­—æ®µä¼˜åŒ–å™¨å¯åŠ¨")
    print("=" * 50)
    
    # è¿è¡Œæ™ºèƒ½ä¼˜åŒ–
    optimization_results = optimizer.run_smart_optimization()
    
    # åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬
    performance_script = optimizer.create_performance_test_script()
    
    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“Š ä¼˜åŒ–ç»“æœ:")
    print(f"  æ€»ä¼˜åŒ–ä»»åŠ¡: {optimization_results['total_optimizations']}")
    print(f"  æˆåŠŸä¼˜åŒ–: {optimization_results['successful_optimizations']}")
    print(f"  å¤±è´¥ä¼˜åŒ–: {optimization_results['failed_optimizations']}")
    print(f"  è·³è¿‡ä¼˜åŒ–: {optimization_results['skipped_optimizations']}")
    print(f"  é¢„è®¡èŠ‚çœ: {optimization_results['total_estimated_savings_mb']:.1f}MB")
    print(f"  å®é™…èŠ‚çœ: {optimization_results['actual_savings_mb']:.1f}MB")
    print(f"  æ•´ä½“çŠ¶æ€: {optimization_results['overall_status']}")
    
    if optimization_results['overall_status'] in ['excellent', 'good']:
        print(f"\nğŸ‰ å­—æ®µä¼˜åŒ–å®Œæˆï¼")
        print(f"ğŸ”¬ æ€§èƒ½æµ‹è¯•è„šæœ¬: {performance_script}")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"  1. è¿è¡Œæ€§èƒ½æµ‹è¯•è„šæœ¬éªŒè¯ä¼˜åŒ–æ•ˆæœ")
        print(f"  2. ç›‘æ§ç³»ç»Ÿè¿è¡ŒçŠ¶æ€")
        print(f"  3. å¦‚æœ‰é—®é¢˜ï¼Œä½¿ç”¨å›æ»šè„šæœ¬æ¢å¤")
    else:
        print(f"\nâš ï¸ ä¼˜åŒ–å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥çš„ä»»åŠ¡")
        
        failed_tasks = [opt_id for opt_id, result in optimization_results['optimization_results'].items() 
                       if not result['execution_success'] and result['error_message'] != "å®‰å…¨æ£€æŸ¥æœªé€šè¿‡ï¼Œè·³è¿‡æ‰§è¡Œ"]
        if failed_tasks:
            print(f"å¤±è´¥çš„ä»»åŠ¡: {', '.join(failed_tasks)}")

if __name__ == "__main__":
    main()