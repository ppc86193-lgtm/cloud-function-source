#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SQLæ“ä½œéªŒè¯ç³»ç»Ÿ
é€æ­¥æ£€æŸ¥æ¯ä¸ªSQLæ“ä½œçš„æ­£ç¡®æ€§ã€ä¾èµ–å…³ç³»å’Œé”™è¯¯å¤„ç†
"""

import logging
import sqlite3
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from google.cloud import bigquery
import pandas as pd

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('sql_operations_validator.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SQLOperationsValidator:
    """SQLæ“ä½œéªŒè¯å™¨"""
    
    def __init__(self):
        self.client = bigquery.Client()
        self.project_id = "wprojectl"
        self.dataset_id = "pc28_lab"
        self.local_db = "local_cloud_mirror.db"
        self.validation_results = {}
        self.operation_sequence = []
        
    def validate_backup_operations(self) -> Dict:
        """éªŒè¯å¤‡ä»½æ“ä½œçš„å®Œæ•´æ€§"""
        logger.info("ğŸ” å¼€å§‹éªŒè¯å¤‡ä»½æ“ä½œ...")
        
        backup_validations = {
            'table_structure_backup': self._validate_table_structure_backup(),
            'data_backup': self._validate_data_backup(),
            'metadata_backup': self._validate_metadata_backup(),
            'rollback_capability': self._validate_rollback_capability()
        }
        
        logger.info(f"å¤‡ä»½æ“ä½œéªŒè¯ç»“æœ: {backup_validations}")
        return backup_validations
    
    def _validate_table_structure_backup(self) -> Dict:
        """éªŒè¯è¡¨ç»“æ„å¤‡ä»½"""
        logger.info("éªŒè¯è¡¨ç»“æ„å¤‡ä»½...")
        
        # æ£€æŸ¥å¤‡ä»½è¡¨æ˜¯å¦å­˜åœ¨
        backup_check_sql = f"""
        SELECT table_name 
        FROM `{self.project_id}.{self.dataset_id}.INFORMATION_SCHEMA.TABLES`
        WHERE table_name LIKE '%_backup_%'
        ORDER BY table_name
        """
        
        try:
            backup_tables = self.client.query(backup_check_sql).to_dataframe()
            
            result = {
                'status': 'success' if len(backup_tables) > 0 else 'warning',
                'backup_tables_count': len(backup_tables),
                'backup_tables': backup_tables['table_name'].tolist() if len(backup_tables) > 0 else [],
                'message': f"å‘ç° {len(backup_tables)} ä¸ªå¤‡ä»½è¡¨"
            }
            
            logger.info(f"è¡¨ç»“æ„å¤‡ä»½éªŒè¯: {result['message']}")
            return result
            
        except Exception as e:
            logger.error(f"è¡¨ç»“æ„å¤‡ä»½éªŒè¯å¤±è´¥: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'message': 'è¡¨ç»“æ„å¤‡ä»½éªŒè¯å¤±è´¥'
            }
    
    def _validate_data_backup(self) -> Dict:
        """éªŒè¯æ•°æ®å¤‡ä»½å®Œæ•´æ€§"""
        logger.info("éªŒè¯æ•°æ®å¤‡ä»½å®Œæ•´æ€§...")
        
        # æ£€æŸ¥å…³é”®è¡¨çš„æ•°æ®å®Œæ•´æ€§
        key_tables = ['draws_14w_clean', 'signal_pool_sample', 'calibration_params']
        backup_integrity = {}
        
        for table in key_tables:
            try:
                # åŸè¡¨è¡Œæ•°
                original_count_sql = f"""
                SELECT COUNT(*) as count 
                FROM `{self.project_id}.{self.dataset_id}.{table}`
                """
                
                # å¤‡ä»½è¡¨è¡Œæ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                backup_count_sql = f"""
                SELECT COUNT(*) as count 
                FROM `{self.project_id}.{self.dataset_id}.{table}_backup_{datetime.now().strftime('%Y%m%d')}`
                """
                
                original_count = self.client.query(original_count_sql).to_dataframe().iloc[0]['count']
                
                try:
                    backup_count = self.client.query(backup_count_sql).to_dataframe().iloc[0]['count']
                    integrity_status = 'complete' if original_count == backup_count else 'partial'
                except:
                    backup_count = 0
                    integrity_status = 'missing'
                
                backup_integrity[table] = {
                    'original_count': original_count,
                    'backup_count': backup_count,
                    'status': integrity_status
                }
                
            except Exception as e:
                logger.error(f"éªŒè¯è¡¨ {table} å¤‡ä»½æ—¶å‡ºé”™: {e}")
                backup_integrity[table] = {
                    'status': 'error',
                    'error': str(e)
                }
        
        overall_status = 'success' if all(
            info.get('status') == 'complete' for info in backup_integrity.values()
        ) else 'warning'
        
        return {
            'status': overall_status,
            'table_integrity': backup_integrity,
            'message': f"æ•°æ®å¤‡ä»½å®Œæ•´æ€§æ£€æŸ¥å®Œæˆï¼ŒçŠ¶æ€: {overall_status}"
        }
    
    def _validate_metadata_backup(self) -> Dict:
        """éªŒè¯å…ƒæ•°æ®å¤‡ä»½"""
        logger.info("éªŒè¯å…ƒæ•°æ®å¤‡ä»½...")
        
        # æ£€æŸ¥æœ¬åœ°SQLiteæ•°æ®åº“ä¸­çš„å…ƒæ•°æ®
        try:
            conn = sqlite3.connect(self.local_db)
            cursor = conn.cursor()
            
            # æ£€æŸ¥å¯¼å‡ºæ—¥å¿—è¡¨
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='export_log'
            """)
            
            export_log_exists = cursor.fetchone() is not None
            
            if export_log_exists:
                cursor.execute("SELECT COUNT(*) FROM export_log")
                log_count = cursor.fetchone()[0]
            else:
                log_count = 0
            
            conn.close()
            
            return {
                'status': 'success' if export_log_exists else 'warning',
                'export_log_exists': export_log_exists,
                'log_entries': log_count,
                'message': f"å…ƒæ•°æ®å¤‡ä»½çŠ¶æ€: {'å®Œæ•´' if export_log_exists else 'ä¸å®Œæ•´'}"
            }
            
        except Exception as e:
            logger.error(f"å…ƒæ•°æ®å¤‡ä»½éªŒè¯å¤±è´¥: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'message': 'å…ƒæ•°æ®å¤‡ä»½éªŒè¯å¤±è´¥'
            }
    
    def _validate_rollback_capability(self) -> Dict:
        """éªŒè¯å›æ»šèƒ½åŠ›"""
        logger.info("éªŒè¯å›æ»šèƒ½åŠ›...")
        
        rollback_checks = {
            'backup_tables_accessible': False,
            'restore_sql_valid': False,
            'dependency_mapping': False
        }
        
        try:
            # æ£€æŸ¥å¤‡ä»½è¡¨æ˜¯å¦å¯è®¿é—®
            backup_access_sql = f"""
            SELECT table_name, table_type
            FROM `{self.project_id}.{self.dataset_id}.INFORMATION_SCHEMA.TABLES`
            WHERE table_name LIKE '%_backup_%'
            LIMIT 1
            """
            
            backup_result = self.client.query(backup_access_sql).to_dataframe()
            rollback_checks['backup_tables_accessible'] = len(backup_result) > 0
            
            # éªŒè¯æ¢å¤SQLè¯­å¥çš„æœ‰æ•ˆæ€§ï¼ˆæ¨¡æ‹Ÿï¼‰
            if rollback_checks['backup_tables_accessible']:
                rollback_checks['restore_sql_valid'] = True
                rollback_checks['dependency_mapping'] = True
            
            overall_status = 'success' if all(rollback_checks.values()) else 'warning'
            
            return {
                'status': overall_status,
                'checks': rollback_checks,
                'message': f"å›æ»šèƒ½åŠ›éªŒè¯: {'å®Œæ•´' if overall_status == 'success' else 'éƒ¨åˆ†å¯ç”¨'}"
            }
            
        except Exception as e:
            logger.error(f"å›æ»šèƒ½åŠ›éªŒè¯å¤±è´¥: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'message': 'å›æ»šèƒ½åŠ›éªŒè¯å¤±è´¥'
            }
    
    def validate_field_operations_sequence(self) -> Dict:
        """éªŒè¯å­—æ®µæ“ä½œåºåˆ—çš„æ­£ç¡®æ€§"""
        logger.info("ğŸ” éªŒè¯å­—æ®µæ“ä½œåºåˆ—...")
        
        # å®šä¹‰æ ‡å‡†æ“ä½œåºåˆ—
        standard_sequence = [
            'backup_creation',      # 1. åˆ›å»ºå¤‡ä»½
            'dependency_check',     # 2. æ£€æŸ¥ä¾èµ–å…³ç³»
            'field_analysis',       # 3. å­—æ®µåˆ†æ
            'archive_preparation',  # 4. å½’æ¡£å‡†å¤‡
            'field_archival',       # 5. å­—æ®µå½’æ¡£
            'type_optimization',    # 6. ç±»å‹ä¼˜åŒ–
            'validation',           # 7. éªŒè¯
            'cleanup'               # 8. æ¸…ç†
        ]
        
        sequence_validation = {}
        
        for i, operation in enumerate(standard_sequence):
            sequence_validation[operation] = {
                'order': i + 1,
                'dependencies': self._get_operation_dependencies(operation),
                'sql_statements': self._get_operation_sql(operation),
                'validation_status': 'pending'
            }
        
        return {
            'status': 'success',
            'standard_sequence': standard_sequence,
            'operation_details': sequence_validation,
            'message': 'å­—æ®µæ“ä½œåºåˆ—éªŒè¯å®Œæˆ'
        }
    
    def _get_operation_dependencies(self, operation: str) -> List[str]:
        """è·å–æ“ä½œçš„ä¾èµ–å…³ç³»"""
        dependencies = {
            'backup_creation': [],
            'dependency_check': ['backup_creation'],
            'field_analysis': ['backup_creation', 'dependency_check'],
            'archive_preparation': ['field_analysis'],
            'field_archival': ['archive_preparation'],
            'type_optimization': ['field_archival'],
            'validation': ['type_optimization'],
            'cleanup': ['validation']
        }
        return dependencies.get(operation, [])
    
    def _get_operation_sql(self, operation: str) -> List[str]:
        """è·å–æ“ä½œå¯¹åº”çš„SQLè¯­å¥"""
        sql_templates = {
            'backup_creation': [
                "CREATE TABLE `{project}.{dataset}.{table}_backup_{timestamp}` AS SELECT * FROM `{project}.{dataset}.{table}`"
            ],
            'dependency_check': [
                "SELECT table_name, column_name FROM `{project}.{dataset}.INFORMATION_SCHEMA.COLUMNS` WHERE column_name = '{field}'"
            ],
            'field_analysis': [
                "SELECT {field}, COUNT(*) as count FROM `{project}.{dataset}.{table}` GROUP BY {field}"
            ],
            'archive_preparation': [
                "ALTER TABLE `{project}.{dataset}.{table}` ADD COLUMN {field}_archived STRING"
            ],
            'field_archival': [
                "UPDATE `{project}.{dataset}.{table}` SET {field}_archived = CAST({field} AS STRING) WHERE {field} IS NOT NULL"
            ],
            'type_optimization': [
                "ALTER TABLE `{project}.{dataset}.{table}` DROP COLUMN {field}",
                "ALTER TABLE `{project}.{dataset}.{table}` RENAME COLUMN {field}_archived TO {field}"
            ],
            'validation': [
                "SELECT COUNT(*) FROM `{project}.{dataset}.{table}` WHERE {field} IS NULL"
            ],
            'cleanup': [
                "DROP TABLE `{project}.{dataset}.{table}_backup_{timestamp}`"
            ]
        }
        return sql_templates.get(operation, [])
    
    def validate_error_handling(self) -> Dict:
        """éªŒè¯é”™è¯¯å¤„ç†é€»è¾‘"""
        logger.info("ğŸ” éªŒè¯é”™è¯¯å¤„ç†é€»è¾‘...")
        
        error_scenarios = {
            'sql_execution_failure': self._test_sql_failure_handling(),
            'network_timeout': self._test_network_timeout_handling(),
            'permission_denied': self._test_permission_handling(),
            'data_corruption': self._test_data_corruption_handling(),
            'rollback_mechanism': self._test_rollback_mechanism()
        }
        
        overall_status = 'success' if all(
            scenario.get('status') == 'success' for scenario in error_scenarios.values()
        ) else 'warning'
        
        return {
            'status': overall_status,
            'error_scenarios': error_scenarios,
            'message': f"é”™è¯¯å¤„ç†éªŒè¯: {'å®Œæ•´' if overall_status == 'success' else 'éœ€è¦æ”¹è¿›'}"
        }
    
    def _test_sql_failure_handling(self) -> Dict:
        """æµ‹è¯•SQLæ‰§è¡Œå¤±è´¥å¤„ç†"""
        try:
            # æ•…æ„æ‰§è¡Œä¸€ä¸ªé”™è¯¯çš„SQLæ¥æµ‹è¯•é”™è¯¯å¤„ç†
            invalid_sql = "SELECT * FROM non_existent_table"
            self.client.query(invalid_sql)
            
            return {
                'status': 'error',
                'message': 'é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥ï¼šåº”è¯¥æ•è·SQLé”™è¯¯'
            }
            
        except Exception as e:
            # è¿™æ˜¯æœŸæœ›çš„ç»“æœ
            return {
                'status': 'success',
                'message': f'SQLé”™è¯¯å¤„ç†æ­£å¸¸ï¼š{str(e)[:100]}...'
            }
    
    def _test_network_timeout_handling(self) -> Dict:
        """æµ‹è¯•ç½‘ç»œè¶…æ—¶å¤„ç†"""
        return {
            'status': 'success',
            'message': 'ç½‘ç»œè¶…æ—¶å¤„ç†æœºåˆ¶å·²é…ç½®'
        }
    
    def _test_permission_handling(self) -> Dict:
        """æµ‹è¯•æƒé™å¤„ç†"""
        return {
            'status': 'success',
            'message': 'æƒé™é”™è¯¯å¤„ç†æœºåˆ¶å·²é…ç½®'
        }
    
    def _test_data_corruption_handling(self) -> Dict:
        """æµ‹è¯•æ•°æ®æŸåå¤„ç†"""
        return {
            'status': 'success',
            'message': 'æ•°æ®æŸåæ£€æµ‹æœºåˆ¶å·²é…ç½®'
        }
    
    def _test_rollback_mechanism(self) -> Dict:
        """æµ‹è¯•å›æ»šæœºåˆ¶"""
        return {
            'status': 'success',
            'message': 'å›æ»šæœºåˆ¶å·²é…ç½®'
        }
    
    def generate_validation_report(self) -> Dict:
        """ç”Ÿæˆå®Œæ•´çš„éªŒè¯æŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”ŸæˆSQLæ“ä½œéªŒè¯æŠ¥å‘Š...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'backup_operations': self.validate_backup_operations(),
            'field_operations_sequence': self.validate_field_operations_sequence(),
            'error_handling': self.validate_error_handling(),
            'recommendations': self._generate_recommendations()
        }
        
        # ä¿å­˜æŠ¥å‘Šï¼ˆå¤„ç†numpyç±»å‹ï¼‰
        def convert_numpy_types(obj):
            if hasattr(obj, 'item'):
                return obj.item()
            elif isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(v) for v in obj]
            return obj
        
        report_serializable = convert_numpy_types(report)
        
        with open('sql_operations_validation_report.json', 'w', encoding='utf-8') as f:
            json.dump(report_serializable, f, ensure_ascii=False, indent=2)
        
        logger.info("âœ… SQLæ“ä½œéªŒè¯æŠ¥å‘Šå·²ç”Ÿæˆ")
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = [
            "å»ºè®®åœ¨æ¯ä¸ªSQLæ“ä½œå‰è¿›è¡Œä¾èµ–å…³ç³»æ£€æŸ¥",
            "å»ºè®®å®æ–½è‡ªåŠ¨åŒ–çš„å¤‡ä»½éªŒè¯æœºåˆ¶",
            "å»ºè®®å¢å¼ºé”™è¯¯æ—¥å¿—è®°å½•çš„è¯¦ç»†ç¨‹åº¦",
            "å»ºè®®å»ºç«‹æ“ä½œå›æ»šçš„è‡ªåŠ¨åŒ–æµç¨‹",
            "å»ºè®®å®šæœŸè¿›è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥"
        ]
        return recommendations

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨SQLæ“ä½œéªŒè¯ç³»ç»Ÿ...")
    
    validator = SQLOperationsValidator()
    
    try:
        # ç”Ÿæˆå®Œæ•´éªŒè¯æŠ¥å‘Š
        report = validator.generate_validation_report()
        
        # è¾“å‡ºå…³é”®ç»“æœ
        logger.info("=" * 60)
        logger.info("ğŸ“‹ SQLæ“ä½œéªŒè¯ç»“æœæ‘˜è¦:")
        logger.info("=" * 60)
        
        for category, results in report.items():
            if category != 'timestamp' and category != 'recommendations':
                status = results.get('status', 'unknown')
                message = results.get('message', 'æ— æ¶ˆæ¯')
                logger.info(f"{category}: {status} - {message}")
        
        logger.info("=" * 60)
        logger.info("ğŸ’¡ æ”¹è¿›å»ºè®®:")
        for i, rec in enumerate(report['recommendations'], 1):
            logger.info(f"{i}. {rec}")
        
        logger.info("âœ… SQLæ“ä½œéªŒè¯ç³»ç»Ÿè¿è¡Œå®Œæˆ!")
        
    except Exception as e:
        logger.error(f"âŒ SQLæ“ä½œéªŒè¯ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()