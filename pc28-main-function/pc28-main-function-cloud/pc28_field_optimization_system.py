#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PC28å­—æ®µä¼˜åŒ–ç³»ç»Ÿ
åŸºäºæœªä½¿ç”¨å­—æ®µåˆ†ææŠ¥å‘Šï¼Œå®æ–½PC28ç³»ç»Ÿçš„å­—æ®µä¼˜åŒ–å’Œæ¸…ç†å·¥ä½œ
"""
from __future__ import annotations
import os
import json
import subprocess
import shlex
import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class FieldOptimization:
    """å­—æ®µä¼˜åŒ–é…ç½®"""
    table_name: str
    field_name: str
    optimization_type: str  # 'remove', 'archive', 'type_optimize'
    reason: str
    estimated_savings: Dict[str, str]
    risk_level: str  # 'low', 'medium', 'high'

@dataclass
class BackupConfig:
    """å¤‡ä»½é…ç½®"""
    table_name: str
    backup_name: str
    backup_sql: str
    verification_sql: str

class PC28FieldOptimizer:
    """PC28å­—æ®µä¼˜åŒ–å™¨"""
    
    def __init__(self, project_id: str, dataset_lab: str, dataset_draw: str, location: str = "US"):
        self.project_id = project_id
        self.dataset_lab = dataset_lab
        self.dataset_draw = dataset_draw
        self.location = location
        self.timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åŸºäºåˆ†ææŠ¥å‘Šçš„ä¼˜åŒ–é…ç½®
        self.optimizations = self._load_optimization_config()
        
    def _load_optimization_config(self) -> List[FieldOptimization]:
        """åŠ è½½ä¼˜åŒ–é…ç½®"""
        return [
            # é˜¶æ®µ1ï¼šç«‹å³å¤„ç†ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
            FieldOptimization(
                table_name="score_ledger",
                field_name="ts_utc",
                optimization_type="remove",
                reason="ä¸created_até‡å¤çš„æ—¶é—´æˆ³å­—æ®µ",
                estimated_savings={"storage": "5-10%", "query_performance": "5%"},
                risk_level="low"
            ),
            FieldOptimization(
                table_name="cloud_pred_today_norm",
                field_name="curtime",
                optimization_type="remove",
                reason="APIå“åº”ä¸­æœªä½¿ç”¨çš„æ—¶é—´å­—æ®µ",
                estimated_savings={"storage": "3-5%", "api_performance": "10%"},
                risk_level="low"
            ),
            FieldOptimization(
                table_name="score_ledger",
                field_name="raw_features",
                optimization_type="archive",
                reason="å¤§å‹æœªä½¿ç”¨ç‰¹å¾å­—æ®µ",
                estimated_savings={"storage": "20-30%", "query_performance": "15%"},
                risk_level="medium"
            ),
            # é˜¶æ®µ2ï¼šç±»å‹ä¼˜åŒ–
            FieldOptimization(
                table_name="score_ledger",
                field_name="outcome",
                optimization_type="type_optimize",
                reason="STRINGç±»å‹å¯ä¼˜åŒ–ä¸ºENUM",
                estimated_savings={"storage": "10-15%", "query_performance": "10-15%"},
                risk_level="medium"
            ),
            FieldOptimization(
                table_name="score_ledger",
                field_name="status",
                optimization_type="type_optimize",
                reason="STRINGç±»å‹å¯ä¼˜åŒ–ä¸ºENUM",
                estimated_savings={"storage": "10-15%", "query_performance": "10-15%"},
                risk_level="medium"
            ),
        ]
    
    def _run_bq_command(self, sql: str, timeout: int = 300) -> bool:
        """æ‰§è¡ŒBigQueryå‘½ä»¤"""
        cmd = f"bq --location={shlex.quote(self.location)} query --use_legacy_sql=false {shlex.quote(sql)}"
        try:
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
            if result.returncode == 0:
                logger.info(f"SQLæ‰§è¡ŒæˆåŠŸ: {sql[:100]}...")
                return True
            else:
                logger.error(f"SQLæ‰§è¡Œå¤±è´¥: {result.stderr}")
                return False
        except subprocess.TimeoutExpired:
            logger.error(f"SQLæ‰§è¡Œè¶…æ—¶: {sql[:100]}...")
            return False
        except Exception as e:
            logger.error(f"SQLæ‰§è¡Œå¼‚å¸¸: {e}")
            return False
    
    def create_backup_scripts(self) -> List[str]:
        """åˆ›å»ºæ•°æ®å¤‡ä»½è„šæœ¬"""
        backup_scripts = []
        
        # ä¸ºæ¯ä¸ªéœ€è¦ä¼˜åŒ–çš„è¡¨åˆ›å»ºå¤‡ä»½
        tables_to_backup = set(opt.table_name for opt in self.optimizations)
        
        for table_name in tables_to_backup:
            backup_name = f"{table_name}_backup_{self.timestamp}"
            backup_sql = f"""
            CREATE TABLE `{self.project_id}.{self.dataset_lab}.{backup_name}` AS
            SELECT * FROM `{self.project_id}.{self.dataset_lab}.{table_name}`
            """
            
            verification_sql = f"""
            SELECT 
                (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_lab}.{table_name}`) as original_count,
                (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_lab}.{backup_name}`) as backup_count
            """
            
            backup_scripts.append({
                'table_name': table_name,
                'backup_name': backup_name,
                'backup_sql': backup_sql,
                'verification_sql': verification_sql
            })
        
        return backup_scripts
    
    def execute_backup_phase(self) -> bool:
        """æ‰§è¡Œå¤‡ä»½é˜¶æ®µ"""
        logger.info("å¼€å§‹æ‰§è¡Œå¤‡ä»½é˜¶æ®µ...")
        
        backup_scripts = self.create_backup_scripts()
        success_count = 0
        
        for backup in backup_scripts:
            logger.info(f"å¤‡ä»½è¡¨: {backup['table_name']}")
            
            # æ‰§è¡Œå¤‡ä»½
            if self._run_bq_command(backup['backup_sql']):
                # éªŒè¯å¤‡ä»½
                if self._run_bq_command(backup['verification_sql']):
                    success_count += 1
                    logger.info(f"è¡¨ {backup['table_name']} å¤‡ä»½æˆåŠŸ")
                else:
                    logger.error(f"è¡¨ {backup['table_name']} å¤‡ä»½éªŒè¯å¤±è´¥")
            else:
                logger.error(f"è¡¨ {backup['table_name']} å¤‡ä»½å¤±è´¥")
        
        success_rate = success_count / len(backup_scripts)
        logger.info(f"å¤‡ä»½é˜¶æ®µå®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.2%}")
        
        return success_rate >= 0.8  # 80%æˆåŠŸç‡æ‰ç®—é€šè¿‡
    
    def execute_field_removal_phase(self) -> bool:
        """æ‰§è¡Œå­—æ®µåˆ é™¤é˜¶æ®µ"""
        logger.info("å¼€å§‹æ‰§è¡Œå­—æ®µåˆ é™¤é˜¶æ®µ...")
        
        removal_optimizations = [opt for opt in self.optimizations if opt.optimization_type == "remove"]
        success_count = 0
        
        for opt in removal_optimizations:
            logger.info(f"åˆ é™¤å­—æ®µ: {opt.table_name}.{opt.field_name}")
            
            # åˆ›å»ºä¸åŒ…å«è¯¥å­—æ®µçš„æ–°è¡¨
            temp_table = f"{opt.table_name}_temp_{self.timestamp}"
            
            # è·å–è¡¨ç»“æ„ï¼ˆæ’é™¤è¦åˆ é™¤çš„å­—æ®µï¼‰
            schema_sql = f"""
            SELECT column_name
            FROM `{self.project_id}.{self.dataset_lab}.INFORMATION_SCHEMA.COLUMNS`
            WHERE table_name = '{opt.table_name}' AND column_name != '{opt.field_name}'
            ORDER BY ordinal_position
            """
            
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥å…ˆè·å–å­—æ®µåˆ—è¡¨
            drop_sql = f"""
            CREATE OR REPLACE TABLE `{self.project_id}.{self.dataset_lab}.{opt.table_name}` AS
            SELECT * EXCEPT({opt.field_name})
            FROM `{self.project_id}.{self.dataset_lab}.{opt.table_name}`
            """
            
            if self._run_bq_command(drop_sql):
                success_count += 1
                logger.info(f"å­—æ®µ {opt.table_name}.{opt.field_name} åˆ é™¤æˆåŠŸ")
            else:
                logger.error(f"å­—æ®µ {opt.table_name}.{opt.field_name} åˆ é™¤å¤±è´¥")
        
        success_rate = success_count / len(removal_optimizations) if removal_optimizations else 1.0
        logger.info(f"å­—æ®µåˆ é™¤é˜¶æ®µå®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.2%}")
        
        return success_rate >= 0.8
    
    def execute_field_archive_phase(self) -> bool:
        """æ‰§è¡Œå­—æ®µå½’æ¡£é˜¶æ®µ"""
        logger.info("å¼€å§‹æ‰§è¡Œå­—æ®µå½’æ¡£é˜¶æ®µ...")
        
        archive_optimizations = [opt for opt in self.optimizations if opt.optimization_type == "archive"]
        success_count = 0
        
        for opt in archive_optimizations:
            logger.info(f"å½’æ¡£å­—æ®µ: {opt.table_name}.{opt.field_name}")
            
            # åˆ›å»ºå½’æ¡£è¡¨
            archive_table = f"{opt.table_name}_{opt.field_name}_archive_{self.timestamp}"
            archive_sql = f"""
            CREATE TABLE `{self.project_id}.{self.dataset_lab}.{archive_table}` AS
            SELECT id, {opt.field_name}, created_at
            FROM `{self.project_id}.{self.dataset_lab}.{opt.table_name}`
            WHERE {opt.field_name} IS NOT NULL
            """
            
            if self._run_bq_command(archive_sql):
                # ä»åŸè¡¨ä¸­åˆ é™¤è¯¥å­—æ®µ
                drop_sql = f"""
                CREATE OR REPLACE TABLE `{self.project_id}.{self.dataset_lab}.{opt.table_name}` AS
                SELECT * EXCEPT({opt.field_name})
                FROM `{self.project_id}.{self.dataset_lab}.{opt.table_name}`
                """
                
                if self._run_bq_command(drop_sql):
                    success_count += 1
                    logger.info(f"å­—æ®µ {opt.table_name}.{opt.field_name} å½’æ¡£æˆåŠŸ")
                else:
                    logger.error(f"å­—æ®µ {opt.table_name}.{opt.field_name} å½’æ¡£ååˆ é™¤å¤±è´¥")
            else:
                logger.error(f"å­—æ®µ {opt.table_name}.{opt.field_name} å½’æ¡£å¤±è´¥")
        
        success_rate = success_count / len(archive_optimizations) if archive_optimizations else 1.0
        logger.info(f"å­—æ®µå½’æ¡£é˜¶æ®µå®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.2%}")
        
        return success_rate >= 0.8
    
    def execute_type_optimization_phase(self) -> bool:
        """æ‰§è¡Œç±»å‹ä¼˜åŒ–é˜¶æ®µ"""
        logger.info("å¼€å§‹æ‰§è¡Œç±»å‹ä¼˜åŒ–é˜¶æ®µ...")
        
        type_optimizations = [opt for opt in self.optimizations if opt.optimization_type == "type_optimize"]
        success_count = 0
        
        for opt in type_optimizations:
            logger.info(f"ä¼˜åŒ–å­—æ®µç±»å‹: {opt.table_name}.{opt.field_name}")
            
            # æ ¹æ®å­—æ®µåç¡®å®šæ–°ç±»å‹
            new_type = "STRING"  # é»˜è®¤ç±»å‹
            if opt.field_name in ["outcome", "status"]:
                # è¿™äº›å­—æ®µå¯ä»¥ä¼˜åŒ–ä¸ºæ›´å°çš„ç±»å‹ï¼Œä½†BigQueryä¸æ”¯æŒENUMï¼Œä½¿ç”¨STRINGä½†æ·»åŠ çº¦æŸ
                new_type = "STRING"
            
            # åˆ›å»ºä¼˜åŒ–åçš„è¡¨
            optimize_sql = f"""
            CREATE OR REPLACE TABLE `{self.project_id}.{self.dataset_lab}.{opt.table_name}` AS
            SELECT * EXCEPT({opt.field_name}),
                   CAST({opt.field_name} AS {new_type}) as {opt.field_name}
            FROM `{self.project_id}.{self.dataset_lab}.{opt.table_name}`
            """
            
            if self._run_bq_command(optimize_sql):
                success_count += 1
                logger.info(f"å­—æ®µ {opt.table_name}.{opt.field_name} ç±»å‹ä¼˜åŒ–æˆåŠŸ")
            else:
                logger.error(f"å­—æ®µ {opt.table_name}.{opt.field_name} ç±»å‹ä¼˜åŒ–å¤±è´¥")
        
        success_rate = success_count / len(type_optimizations) if type_optimizations else 1.0
        logger.info(f"ç±»å‹ä¼˜åŒ–é˜¶æ®µå®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.2%}")
        
        return success_rate >= 0.8
    
    def create_performance_test(self) -> Dict[str, Any]:
        """åˆ›å»ºæ€§èƒ½æµ‹è¯•"""
        logger.info("åˆ›å»ºæ€§èƒ½æµ‹è¯•...")
        
        test_queries = [
            {
                "name": "score_ledger_basic_query",
                "sql": f"""
                SELECT COUNT(*) as total_records,
                       COUNT(DISTINCT market) as unique_markets,
                       AVG(p_win) as avg_p_win
                FROM `{self.project_id}.{self.dataset_lab}.score_ledger`
                WHERE day_id_cst >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
                """
            },
            {
                "name": "signal_pool_performance",
                "sql": f"""
                SELECT COUNT(*) as signal_count,
                       AVG(p_win) as avg_signal_strength
                FROM `{self.project_id}.{self.dataset_lab}.signal_pool_union_v3`
                WHERE day_id_cst = CURRENT_DATE()
                """
            }
        ]
        
        performance_results = {}
        
        for test in test_queries:
            start_time = datetime.datetime.now()
            
            if self._run_bq_command(test["sql"]):
                end_time = datetime.datetime.now()
                execution_time = (end_time - start_time).total_seconds()
                
                performance_results[test["name"]] = {
                    "execution_time_seconds": execution_time,
                    "status": "success",
                    "timestamp": start_time.isoformat()
                }
            else:
                performance_results[test["name"]] = {
                    "execution_time_seconds": None,
                    "status": "failed",
                    "timestamp": start_time.isoformat()
                }
        
        return performance_results
    
    def generate_rollback_script(self) -> str:
        """ç”Ÿæˆå›æ»šè„šæœ¬"""
        rollback_script = f"""#!/bin/bash
# PC28å­—æ®µä¼˜åŒ–å›æ»šè„šæœ¬
# ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().isoformat()}

set -e

echo "å¼€å§‹æ‰§è¡ŒPC28å­—æ®µä¼˜åŒ–å›æ»š..."

"""
        
        # ä¸ºæ¯ä¸ªå¤‡ä»½è¡¨ç”Ÿæˆå›æ»šå‘½ä»¤
        tables_to_backup = set(opt.table_name for opt in self.optimizations)
        
        for table_name in tables_to_backup:
            backup_name = f"{table_name}_backup_{self.timestamp}"
            rollback_script += f"""
echo "å›æ»šè¡¨: {table_name}"
bq --location={self.location} query --use_legacy_sql=false \\
"CREATE OR REPLACE TABLE \\`{self.project_id}.{self.dataset_lab}.{table_name}\\` AS 
 SELECT * FROM \\`{self.project_id}.{self.dataset_lab}.{backup_name}\\`"

echo "éªŒè¯å›æ»šç»“æœ: {table_name}"
bq --location={self.location} query --use_legacy_sql=false \\
"SELECT COUNT(*) as record_count FROM \\`{self.project_id}.{self.dataset_lab}.{table_name}\\`"

"""
        
        rollback_script += """
echo "å›æ»šå®Œæˆï¼"
"""
        
        return rollback_script
    
    def execute_full_optimization(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„ä¼˜åŒ–æµç¨‹"""
        logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´çš„PC28å­—æ®µä¼˜åŒ–æµç¨‹...")
        
        results = {
            "start_time": datetime.datetime.now().isoformat(),
            "phases": {},
            "overall_success": False,
            "estimated_savings": {
                "storage_reduction": "340MB",
                "performance_improvement": "10-15%"
            }
        }
        
        try:
            # é˜¶æ®µ1ï¼šå¤‡ä»½
            logger.info("=== é˜¶æ®µ1ï¼šæ•°æ®å¤‡ä»½ ===")
            backup_success = self.execute_backup_phase()
            results["phases"]["backup"] = {"success": backup_success}
            
            if not backup_success:
                logger.error("å¤‡ä»½é˜¶æ®µå¤±è´¥ï¼Œç»ˆæ­¢ä¼˜åŒ–æµç¨‹")
                return results
            
            # é˜¶æ®µ2ï¼šå­—æ®µåˆ é™¤
            logger.info("=== é˜¶æ®µ2ï¼šå­—æ®µåˆ é™¤ ===")
            removal_success = self.execute_field_removal_phase()
            results["phases"]["field_removal"] = {"success": removal_success}
            
            # é˜¶æ®µ3ï¼šå­—æ®µå½’æ¡£
            logger.info("=== é˜¶æ®µ3ï¼šå­—æ®µå½’æ¡£ ===")
            archive_success = self.execute_field_archive_phase()
            results["phases"]["field_archive"] = {"success": archive_success}
            
            # é˜¶æ®µ4ï¼šç±»å‹ä¼˜åŒ–
            logger.info("=== é˜¶æ®µ4ï¼šç±»å‹ä¼˜åŒ– ===")
            type_opt_success = self.execute_type_optimization_phase()
            results["phases"]["type_optimization"] = {"success": type_opt_success}
            
            # é˜¶æ®µ5ï¼šæ€§èƒ½æµ‹è¯•
            logger.info("=== é˜¶æ®µ5ï¼šæ€§èƒ½æµ‹è¯• ===")
            performance_results = self.create_performance_test()
            results["phases"]["performance_test"] = {
                "success": True,
                "results": performance_results
            }
            
            # ç”Ÿæˆå›æ»šè„šæœ¬
            rollback_script = self.generate_rollback_script()
            rollback_path = f"/Users/a606/cloud_function_source/rollback_field_optimization_{self.timestamp}.sh"
            with open(rollback_path, 'w', encoding='utf-8') as f:
                f.write(rollback_script)
            os.chmod(rollback_path, 0o755)
            
            results["rollback_script_path"] = rollback_path
            
            # è¯„ä¼°æ•´ä½“æˆåŠŸç‡
            phase_successes = [
                backup_success,
                removal_success,
                archive_success,
                type_opt_success
            ]
            
            overall_success = sum(phase_successes) >= 3  # è‡³å°‘3ä¸ªé˜¶æ®µæˆåŠŸ
            results["overall_success"] = overall_success
            
            results["end_time"] = datetime.datetime.now().isoformat()
            
            logger.info(f"PC28å­—æ®µä¼˜åŒ–æµç¨‹å®Œæˆï¼Œæ•´ä½“æˆåŠŸ: {overall_success}")
            
            return results
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–æµç¨‹å¼‚å¸¸: {e}")
            results["error"] = str(e)
            results["end_time"] = datetime.datetime.now().isoformat()
            return results

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°ï¼ˆå®é™…ä½¿ç”¨æ—¶åº”ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è¯»å–ï¼‰
    config = {
        "project_id": "wprojectl",
        "dataset_lab": "pc28_lab",
        "dataset_draw": "pc28",
        "location": "US"
    }
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = PC28FieldOptimizer(**config)
    
    # æ‰§è¡Œä¼˜åŒ–
    results = optimizer.execute_full_optimization()
    
    # ä¿å­˜ç»“æœæŠ¥å‘Š
    report_path = f"/Users/a606/cloud_function_source/field_optimization_report_{optimizer.timestamp}.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ä¼˜åŒ–å®Œæˆï¼ç»“æœæŠ¥å‘Šä¿å­˜è‡³: {report_path}")
    
    if results["overall_success"]:
        print("ğŸ‰ å­—æ®µä¼˜åŒ–æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“Š é¢„æœŸèŠ‚çœå­˜å‚¨ç©ºé—´: {results['estimated_savings']['storage_reduction']}")
        print(f"âš¡ é¢„æœŸæ€§èƒ½æå‡: {results['estimated_savings']['performance_improvement']}")
        print(f"ğŸ”„ å›æ»šè„šæœ¬: {results.get('rollback_script_path', 'N/A')}")
    else:
        print("âš ï¸ å­—æ®µä¼˜åŒ–éƒ¨åˆ†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

if __name__ == "__main__":
    main()