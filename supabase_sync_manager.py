"""
Supabase æ•°æ®åŒæ­¥ç®¡ç†å™¨
æ”¯æŒ PC28 ç³»ç»Ÿåˆ° Supabase çš„å®æ—¶å’Œæ‰¹é‡æ•°æ®åŒæ­¥
"""

import os
import logging
import sqlite3
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib

from supabase_config import get_supabase_client, supabase_manager
from data_type_mapper import data_type_mapper

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SupabaseSyncManager:
    """Supabase æ•°æ®åŒæ­¥ç®¡ç†å™¨"""
    
    # æ ¸å¿ƒæ•°æ®è¡¨é…ç½®
    CORE_TABLES = {
        'lab_push_candidates_v2': {
            'primary_key': 'draw_id',
            'timestamp_column': 'created_at',
            'batch_size': 1000,
            'sync_mode': 'incremental'
        },
        'cloud_pred_today_norm': {
            'primary_key': 'draw_id',
            'timestamp_column': 'created_at',
            'batch_size': 1000,
            'sync_mode': 'incremental'
        },
        'signal_pool_union_v3': {
            'primary_key': 'signal_id',
            'timestamp_column': 'last_seen',
            'batch_size': 500,
            'sync_mode': 'incremental'
        },
        'p_size_clean_merged_dedup_v': {
            'primary_key': 'size_category',
            'timestamp_column': 'last_updated',
            'batch_size': 100,
            'sync_mode': 'full'
        },
        'draws_14w_dedup_v': {
            'primary_key': 'draw_id',
            'timestamp_column': 'created_at',
            'batch_size': 2000,
            'sync_mode': 'incremental'
        },
        'score_ledger': {
            'primary_key': 'draw_id',
            'timestamp_column': 'evaluation_date',
            'batch_size': 1000,
            'sync_mode': 'incremental'
        }
    }
    
    def __init__(self, sqlite_db_path: Optional[str] = None):
        self.sqlite_db_path = sqlite_db_path or os.getenv('SQLITE_DB_PATH', 'pc28_data.db')
        self.supabase_client = get_supabase_client(use_service_role=True)
        self.sync_stats = {
            'total_syncs': 0,
            'successful_syncs': 0,
            'failed_syncs': 0,
            'total_records_synced': 0,
            'sync_history': []
        }
        
        # åŒæ­¥é…ç½®
        self.max_workers = int(os.getenv('SYNC_MAX_WORKERS', '4'))
        self.sync_timeout = int(os.getenv('SYNC_TIMEOUT_SECONDS', '300'))
        self.retry_attempts = int(os.getenv('SYNC_RETRY_ATTEMPTS', '3'))
        
    def get_sqlite_connection(self) -> sqlite3.Connection:
        """è·å– SQLite æ•°æ®åº“è¿æ¥"""
        try:
            conn = sqlite3.connect(self.sqlite_db_path)
            conn.row_factory = sqlite3.Row  # ä½¿ç»“æœå¯ä»¥æŒ‰åˆ—åè®¿é—®
            return conn
        except Exception as e:
            logger.error(f"Failed to connect to SQLite database: {e}")
            raise
    
    def get_table_schema(self, table_name: str, conn: sqlite3.Connection) -> Dict[str, str]:
        """
        è·å–è¡¨çš„ schema ä¿¡æ¯
        
        Args:
            table_name: è¡¨å
            conn: SQLite è¿æ¥
            
        Returns:
            åˆ—ååˆ°æ•°æ®ç±»å‹çš„æ˜ å°„
        """
        try:
            cursor = conn.execute(f"PRAGMA table_info({table_name})")
            columns = cursor.fetchall()
            
            schema = {}
            for column in columns:
                column_name = column[1]
                column_type = column[2]
                schema[column_name] = column_type
            
            logger.debug(f"Retrieved schema for table {table_name}: {schema}")
            return schema
            
        except Exception as e:
            logger.error(f"Failed to get schema for table {table_name}: {e}")
            return {}
    
    def get_last_sync_timestamp(self, table_name: str) -> Optional[datetime]:
        """
        è·å–è¡¨çš„æœ€ååŒæ­¥æ—¶é—´æˆ³
        
        Args:
            table_name: è¡¨å
            
        Returns:
            æœ€ååŒæ­¥æ—¶é—´æˆ³
        """
        try:
            response = self.supabase_client.table('sync_status') \
                .select('last_sync_timestamp') \
                .eq('table_name', table_name) \
                .order('created_at', desc=True) \
                .limit(1) \
                .execute()
            
            if response.data:
                timestamp_str = response.data[0]['last_sync_timestamp']
                if timestamp_str:
                    return datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to get last sync timestamp for {table_name}: {e}")
            return None
    
    def update_sync_status(self, table_name: str, status: str, 
                          records_synced: int = 0, error_message: str = None,
                          sync_duration: float = 0.0, sync_mode: str = 'incremental'):
        """
        æ›´æ–°åŒæ­¥çŠ¶æ€
        
        Args:
            table_name: è¡¨å
            status: åŒæ­¥çŠ¶æ€
            records_synced: åŒæ­¥çš„è®°å½•æ•°
            error_message: é”™è¯¯æ¶ˆæ¯
            sync_duration: åŒæ­¥æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
            sync_mode: åŒæ­¥æ¨¡å¼
        """
        try:
            sync_data = {
                'table_name': table_name,
                'last_sync_timestamp': datetime.now().isoformat(),
                'sync_mode': sync_mode,
                'records_synced': records_synced,
                'sync_duration_seconds': sync_duration,
                'status': status,
                'error_message': error_message
            }
            
            self.supabase_client.table('sync_status').insert(sync_data).execute()
            logger.info(f"Updated sync status for {table_name}: {status}")
            
        except Exception as e:
            logger.error(f"Failed to update sync status for {table_name}: {e}")
    
    def create_data_hash(self, data: Dict[str, Any]) -> str:
        """
        ä¸ºæ•°æ®è¡Œåˆ›å»ºå“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹å˜æ›´
        
        Args:
            data: æ•°æ®è¡Œ
            
        Returns:
            æ•°æ®å“ˆå¸Œå€¼
        """
        # æ’é™¤å…ƒæ•°æ®å­—æ®µ
        filtered_data = {k: v for k, v in data.items() 
                        if k not in ['id', 'created_at', 'updated_at', 'sync_source', 'sync_timestamp']}
        
        data_str = json.dumps(filtered_data, sort_keys=True, default=str)
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def sync_table_incremental(self, table_name: str) -> Tuple[bool, int, str]:
        """
        å¢é‡åŒæ­¥è¡¨æ•°æ®
        
        Args:
            table_name: è¡¨å
            
        Returns:
            (æˆåŠŸæ ‡å¿—, åŒæ­¥è®°å½•æ•°, é”™è¯¯æ¶ˆæ¯)
        """
        start_time = time.time()
        
        try:
            table_config = self.CORE_TABLES.get(table_name, {})
            batch_size = table_config.get('batch_size', 1000)
            timestamp_column = table_config.get('timestamp_column', 'created_at')
            
            # è·å–æœ€ååŒæ­¥æ—¶é—´æˆ³
            last_sync = self.get_last_sync_timestamp(table_name)
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            with self.get_sqlite_connection() as conn:
                if last_sync:
                    query = f"""
                    SELECT * FROM {table_name} 
                    WHERE {timestamp_column} > ? 
                    ORDER BY {timestamp_column}
                    """
                    cursor = conn.execute(query, (last_sync.isoformat(),))
                else:
                    query = f"SELECT * FROM {table_name} ORDER BY {timestamp_column}"
                    cursor = conn.execute(query)
                
                # è·å–è¡¨ç»“æ„
                schema = self.get_table_schema(table_name, conn)
                
                # æ‰¹é‡å¤„ç†æ•°æ®
                records_synced = 0
                batch_data = []
                
                for row in cursor:
                    # è½¬æ¢ä¸ºå­—å…¸
                    row_dict = dict(row)
                    
                    # æ•°æ®ç±»å‹è½¬æ¢
                    converted_row = {}
                    for column, value in row_dict.items():
                        if column in schema:
                            postgres_type = data_type_mapper.map_sqlite_type(schema[column])
                            converted_row[column] = data_type_mapper.convert_value(value, postgres_type)
                        else:
                            converted_row[column] = value
                    
                    # æ·»åŠ åŒæ­¥å…ƒæ•°æ®
                    converted_row['sync_source'] = 'sqlite'
                    converted_row['sync_timestamp'] = datetime.now().isoformat()
                    
                    batch_data.append(converted_row)
                    
                    # æ‰¹é‡æ’å…¥
                    if len(batch_data) >= batch_size:
                        self._insert_batch(table_name, batch_data)
                        records_synced += len(batch_data)
                        batch_data = []
                        logger.info(f"Synced {records_synced} records for {table_name}")
                
                # å¤„ç†å‰©ä½™æ•°æ®
                if batch_data:
                    self._insert_batch(table_name, batch_data)
                    records_synced += len(batch_data)
            
            # æ›´æ–°åŒæ­¥çŠ¶æ€
            sync_duration = time.time() - start_time
            self.update_sync_status(table_name, 'completed', records_synced, 
                                  sync_duration=sync_duration, sync_mode='incremental')
            
            logger.info(f"Incremental sync completed for {table_name}: {records_synced} records in {sync_duration:.2f}s")
            return True, records_synced, None
            
        except Exception as e:
            error_msg = f"Incremental sync failed for {table_name}: {str(e)}"
            logger.error(error_msg)
            
            sync_duration = time.time() - start_time
            self.update_sync_status(table_name, 'failed', 0, error_msg, 
                                  sync_duration=sync_duration, sync_mode='incremental')
            
            return False, 0, error_msg
    
    def sync_table_full(self, table_name: str) -> Tuple[bool, int, str]:
        """
        å…¨é‡åŒæ­¥è¡¨æ•°æ®
        
        Args:
            table_name: è¡¨å
            
        Returns:
            (æˆåŠŸæ ‡å¿—, åŒæ­¥è®°å½•æ•°, é”™è¯¯æ¶ˆæ¯)
        """
        start_time = time.time()
        
        try:
            table_config = self.CORE_TABLES.get(table_name, {})
            batch_size = table_config.get('batch_size', 1000)
            
            # æ¸…ç©ºç›®æ ‡è¡¨
            self.supabase_client.table(table_name).delete().neq('id', '00000000-0000-0000-0000-000000000000').execute()
            logger.info(f"Cleared existing data in {table_name}")
            
            # ä» SQLite è¯»å–æ‰€æœ‰æ•°æ®
            with self.get_sqlite_connection() as conn:
                query = f"SELECT * FROM {table_name}"
                cursor = conn.execute(query)
                
                # è·å–è¡¨ç»“æ„
                schema = self.get_table_schema(table_name, conn)
                
                # æ‰¹é‡å¤„ç†æ•°æ®
                records_synced = 0
                batch_data = []
                
                for row in cursor:
                    # è½¬æ¢ä¸ºå­—å…¸
                    row_dict = dict(row)
                    
                    # æ•°æ®ç±»å‹è½¬æ¢
                    converted_row = {}
                    for column, value in row_dict.items():
                        if column in schema:
                            postgres_type = data_type_mapper.map_sqlite_type(schema[column])
                            converted_row[column] = data_type_mapper.convert_value(value, postgres_type)
                        else:
                            converted_row[column] = value
                    
                    # æ·»åŠ åŒæ­¥å…ƒæ•°æ®
                    converted_row['sync_source'] = 'sqlite'
                    converted_row['sync_timestamp'] = datetime.now().isoformat()
                    
                    batch_data.append(converted_row)
                    
                    # æ‰¹é‡æ’å…¥
                    if len(batch_data) >= batch_size:
                        self._insert_batch(table_name, batch_data)
                        records_synced += len(batch_data)
                        batch_data = []
                        logger.info(f"Synced {records_synced} records for {table_name}")
                
                # å¤„ç†å‰©ä½™æ•°æ®
                if batch_data:
                    self._insert_batch(table_name, batch_data)
                    records_synced += len(batch_data)
            
            # æ›´æ–°åŒæ­¥çŠ¶æ€
            sync_duration = time.time() - start_time
            self.update_sync_status(table_name, 'completed', records_synced, 
                                  sync_duration=sync_duration, sync_mode='full')
            
            logger.info(f"Full sync completed for {table_name}: {records_synced} records in {sync_duration:.2f}s")
            return True, records_synced, None
            
        except Exception as e:
            error_msg = f"Full sync failed for {table_name}: {str(e)}"
            logger.error(error_msg)
            
            sync_duration = time.time() - start_time
            self.update_sync_status(table_name, 'failed', 0, error_msg, 
                                  sync_duration=sync_duration, sync_mode='full')
            
            return False, 0, error_msg
    
    def _insert_batch(self, table_name: str, batch_data: List[Dict[str, Any]]):
        """
        æ‰¹é‡æ’å…¥æ•°æ®åˆ° Supabase
        
        Args:
            table_name: è¡¨å
            batch_data: æ‰¹é‡æ•°æ®
        """
        try:
            # ä½¿ç”¨ upsert æ¥å¤„ç†é‡å¤æ•°æ®
            response = self.supabase_client.table(table_name).upsert(batch_data).execute()
            
            if not response.data:
                logger.warning(f"No data returned from upsert for {table_name}")
            
        except Exception as e:
            logger.error(f"Failed to insert batch for {table_name}: {e}")
            # å°è¯•é€æ¡æ’å…¥ä»¥è¯†åˆ«é—®é¢˜è®°å½•
            self._insert_records_individually(table_name, batch_data)
    
    def _insert_records_individually(self, table_name: str, records: List[Dict[str, Any]]):
        """
        é€æ¡æ’å…¥è®°å½•ï¼Œç”¨äºé”™è¯¯æ¢å¤
        
        Args:
            table_name: è¡¨å
            records: è®°å½•åˆ—è¡¨
        """
        successful_inserts = 0
        failed_inserts = 0
        
        for record in records:
            try:
                self.supabase_client.table(table_name).upsert([record]).execute()
                successful_inserts += 1
            except Exception as e:
                failed_inserts += 1
                logger.error(f"Failed to insert individual record in {table_name}: {e}")
                logger.debug(f"Problematic record: {record}")
        
        logger.info(f"Individual insert results for {table_name}: {successful_inserts} successful, {failed_inserts} failed")
    
    def sync_all_tables(self, sync_mode: str = 'auto') -> Dict[str, Any]:
        """
        åŒæ­¥æ‰€æœ‰æ ¸å¿ƒè¡¨
        
        Args:
            sync_mode: åŒæ­¥æ¨¡å¼ ('auto', 'incremental', 'full')
            
        Returns:
            åŒæ­¥ç»“æœæ‘˜è¦
        """
        start_time = time.time()
        results = {
            'start_time': datetime.now().isoformat(),
            'sync_mode': sync_mode,
            'table_results': {},
            'total_records_synced': 0,
            'successful_tables': 0,
            'failed_tables': 0
        }
        
        logger.info(f"Starting sync for all tables with mode: {sync_mode}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡ŒåŒæ­¥
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_table = {}
            
            for table_name, config in self.CORE_TABLES.items():
                # ç¡®å®šåŒæ­¥æ¨¡å¼
                if sync_mode == 'auto':
                    table_sync_mode = config.get('sync_mode', 'incremental')
                else:
                    table_sync_mode = sync_mode
                
                # æäº¤åŒæ­¥ä»»åŠ¡
                if table_sync_mode == 'full':
                    future = executor.submit(self.sync_table_full, table_name)
                else:
                    future = executor.submit(self.sync_table_incremental, table_name)
                
                future_to_table[future] = table_name
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_table, timeout=self.sync_timeout):
                table_name = future_to_table[future]
                
                try:
                    success, records_synced, error_msg = future.result()
                    
                    results['table_results'][table_name] = {
                        'success': success,
                        'records_synced': records_synced,
                        'error_message': error_msg
                    }
                    
                    if success:
                        results['successful_tables'] += 1
                        results['total_records_synced'] += records_synced
                    else:
                        results['failed_tables'] += 1
                    
                except Exception as e:
                    error_msg = f"Sync task failed for {table_name}: {str(e)}"
                    logger.error(error_msg)
                    
                    results['table_results'][table_name] = {
                        'success': False,
                        'records_synced': 0,
                        'error_message': error_msg
                    }
                    results['failed_tables'] += 1
        
        # å®ŒæˆåŒæ­¥
        results['end_time'] = datetime.now().isoformat()
        results['duration_seconds'] = time.time() - start_time
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.sync_stats['total_syncs'] += 1
        if results['failed_tables'] == 0:
            self.sync_stats['successful_syncs'] += 1
        else:
            self.sync_stats['failed_syncs'] += 1
        
        self.sync_stats['total_records_synced'] += results['total_records_synced']
        self.sync_stats['sync_history'].append(results)
        
        # ä¿ç•™æœ€è¿‘ 10 æ¬¡åŒæ­¥å†å²
        if len(self.sync_stats['sync_history']) > 10:
            self.sync_stats['sync_history'] = self.sync_stats['sync_history'][-10:]
        
        logger.info(f"Sync completed: {results['successful_tables']} successful, {results['failed_tables']} failed, "
                   f"{results['total_records_synced']} total records in {results['duration_seconds']:.2f}s")
        
        return results
    
    def get_sync_stats(self) -> Dict[str, Any]:
        """è·å–åŒæ­¥ç»Ÿè®¡ä¿¡æ¯"""
        return self.sync_stats.copy()
    
    def validate_sync_integrity(self, table_name: str) -> Dict[str, Any]:
        """
        éªŒè¯åŒæ­¥æ•°æ®å®Œæ•´æ€§
        
        Args:
            table_name: è¡¨å
            
        Returns:
            éªŒè¯ç»“æœ
        """
        try:
            # è·å– SQLite è®°å½•æ•°
            with self.get_sqlite_connection() as conn:
                cursor = conn.execute(f"SELECT COUNT(*) FROM {table_name}")
                sqlite_count = cursor.fetchone()[0]
            
            # è·å– Supabase è®°å½•æ•°
            response = self.supabase_client.table(table_name).select('id', count='exact').execute()
            supabase_count = response.count
            
            # è®¡ç®—å·®å¼‚
            difference = abs(sqlite_count - supabase_count)
            integrity_score = 1.0 - (difference / max(sqlite_count, 1))
            
            result = {
                'table_name': table_name,
                'sqlite_count': sqlite_count,
                'supabase_count': supabase_count,
                'difference': difference,
                'integrity_score': integrity_score,
                'status': 'passed' if integrity_score >= 0.95 else 'failed',
                'timestamp': datetime.now().isoformat()
            }
            
            logger.info(f"Integrity check for {table_name}: {result['status']} (score: {integrity_score:.3f})")
            return result
            
        except Exception as e:
            error_msg = f"Integrity validation failed for {table_name}: {str(e)}"
            logger.error(error_msg)
            
            return {
                'table_name': table_name,
                'status': 'error',
                'error_message': error_msg,
                'timestamp': datetime.now().isoformat()
            }

# å…¨å±€åŒæ­¥ç®¡ç†å™¨å®ä¾‹
sync_manager = SupabaseSyncManager()

def sync_table(table_name: str, sync_mode: str = 'incremental') -> Tuple[bool, int, str]:
    """
    åŒæ­¥å•ä¸ªè¡¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        table_name: è¡¨å
        sync_mode: åŒæ­¥æ¨¡å¼
        
    Returns:
        (æˆåŠŸæ ‡å¿—, åŒæ­¥è®°å½•æ•°, é”™è¯¯æ¶ˆæ¯)
    """
    if sync_mode == 'full':
        return sync_manager.sync_table_full(table_name)
    else:
        return sync_manager.sync_table_incremental(table_name)

def sync_all_tables(sync_mode: str = 'auto') -> Dict[str, Any]:
    """
    åŒæ­¥æ‰€æœ‰è¡¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        sync_mode: åŒæ­¥æ¨¡å¼
        
    Returns:
        åŒæ­¥ç»“æœæ‘˜è¦
    """
    return sync_manager.sync_all_tables(sync_mode)

if __name__ == "__main__":
    # æµ‹è¯•åŒæ­¥ç®¡ç†å™¨
    print("Testing Supabase Sync Manager...")
    
    try:
        manager = SupabaseSyncManager()
        
        # æµ‹è¯•è¿æ¥
        if supabase_manager.test_connection()['status'] == 'success':
            print("âœ… Supabase connection successful")
            
            # è¿è¡ŒåŒæ­¥æµ‹è¯•
            print("\nğŸ”„ Running sync test...")
            results = manager.sync_all_tables('incremental')
            
            print(f"Sync results:")
            print(f"  Successful tables: {results['successful_tables']}")
            print(f"  Failed tables: {results['failed_tables']}")
            print(f"  Total records synced: {results['total_records_synced']}")
            print(f"  Duration: {results['duration_seconds']:.2f}s")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = manager.get_sync_stats()
            print(f"\nSync statistics: {stats}")
            
        else:
            print("âŒ Supabase connection failed")
            
    except Exception as e:
        print(f"âŒ Sync manager test failed: {e}")