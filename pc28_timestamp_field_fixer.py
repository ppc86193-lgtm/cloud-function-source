#!/usr/bin/env python3
"""
PC28æ—¶é—´æˆ³å­—æ®µä¿®å¤ç³»ç»Ÿ
ä¸“é—¨è§£å†³timestampå­—æ®µç±»å‹å’ŒDATEå‡½æ•°é—®é¢˜
"""

import logging
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from google.cloud import bigquery

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PC28TimestampFieldFixer:
    def __init__(self):
        self.client = bigquery.Client()
        self.project_id = "wprojectl"
        self.dataset_id = "pc28_lab"
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def analyze_timestamp_issues(self) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´æˆ³å­—æ®µé—®é¢˜"""
        logger.info("ğŸ” åˆ†ææ—¶é—´æˆ³å­—æ®µé—®é¢˜...")
        
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "field_types": {},
            "issues_found": []
        }
        
        # æ£€æŸ¥cloud_pred_today_normè¡¨çš„timestampå­—æ®µç±»å‹
        try:
            table_ref = f"{self.project_id}.{self.dataset_id}.cloud_pred_today_norm"
            table = self.client.get_table(table_ref)
            
            for field in table.schema:
                if field.name == "timestamp":
                    analysis["field_types"]["cloud_pred_today_norm_timestamp"] = field.field_type
                    
                    if field.field_type == "STRING":
                        analysis["issues_found"].append({
                            "issue_type": "timestamp_type_mismatch",
                            "description": "timestampå­—æ®µæ˜¯STRINGç±»å‹ï¼Œä½†DATEå‡½æ•°éœ€è¦TIMESTAMPç±»å‹",
                            "table": "cloud_pred_today_norm",
                            "field": "timestamp",
                            "current_type": "STRING",
                            "expected_type": "TIMESTAMP",
                            "fix_strategy": "cast_to_timestamp"
                        })
                        
        except Exception as e:
            logger.error(f"åˆ†ætimestampå­—æ®µå¤±è´¥: {e}")
            
        return analysis
    
    def fix_timestamp_views(self) -> Dict[str, Any]:
        """ä¿®å¤æ—¶é—´æˆ³ç›¸å…³è§†å›¾"""
        logger.info("ğŸ”§ å¼€å§‹ä¿®å¤æ—¶é—´æˆ³ç›¸å…³è§†å›¾...")
        
        fix_results = {
            "timestamp": datetime.now().isoformat(),
            "fixes_attempted": [],
            "fixes_successful": [],
            "fixes_failed": [],
            "overall_success": False
        }
        
        # ä¿®å¤p_cloud_clean_merged_dedup_vè§†å›¾
        fix_results["fixes_attempted"].append("p_cloud_clean_merged_dedup_v")
        
        try:
            # æ–°çš„è§†å›¾å®šä¹‰ï¼Œæ­£ç¡®å¤„ç†STRINGç±»å‹çš„timestamp
            new_view_sql = f"""
            CREATE OR REPLACE VIEW `{self.project_id}.{self.dataset_id}.p_cloud_clean_merged_dedup_v` AS
            SELECT 
                period,
                PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', timestamp) as ts_utc,  -- å°†STRINGè½¬æ¢ä¸ºTIMESTAMP
                p_win as p_even,
                source as src
            FROM (
                SELECT 
                    period,
                    timestamp,
                    p_win,
                    source,
                    ROW_NUMBER() OVER (PARTITION BY period ORDER BY timestamp DESC) as rn
                FROM `{self.project_id}.{self.dataset_id}.cloud_pred_today_norm`
                WHERE p_win IS NOT NULL
            )
            WHERE rn = 1
            """
            
            job = self.client.query(new_view_sql)
            job.result()  # ç­‰å¾…å®Œæˆ
            
            fix_results["fixes_successful"].append("p_cloud_clean_merged_dedup_v")
            logger.info("âœ… p_cloud_clean_merged_dedup_v ä¿®å¤æˆåŠŸ")
            
        except Exception as e:
            fix_results["fixes_failed"].append({
                "view": "p_cloud_clean_merged_dedup_v",
                "error": str(e)
            })
            logger.error(f"âŒ p_cloud_clean_merged_dedup_v ä¿®å¤å¤±è´¥: {e}")
        
        # ä¿®å¤p_cloud_today_vè§†å›¾
        fix_results["fixes_attempted"].append("p_cloud_today_v")
        
        try:
            # æ–°çš„è§†å›¾å®šä¹‰ï¼Œæ­£ç¡®å¤„ç†DATEå‡½æ•°
            new_view_sql = f"""
            CREATE OR REPLACE VIEW `{self.project_id}.{self.dataset_id}.p_cloud_today_v` AS
            SELECT 
                period,
                ts_utc,
                p_even,
                src
            FROM `{self.project_id}.{self.dataset_id}.p_cloud_clean_merged_dedup_v`
            WHERE DATE(ts_utc) = CURRENT_DATE('Asia/Shanghai')
            """
            
            job = self.client.query(new_view_sql)
            job.result()  # ç­‰å¾…å®Œæˆ
            
            fix_results["fixes_successful"].append("p_cloud_today_v")
            logger.info("âœ… p_cloud_today_v ä¿®å¤æˆåŠŸ")
            
        except Exception as e:
            fix_results["fixes_failed"].append({
                "view": "p_cloud_today_v", 
                "error": str(e)
            })
            logger.error(f"âŒ p_cloud_today_v ä¿®å¤å¤±è´¥: {e}")
        
        # è®¡ç®—æ•´ä½“æˆåŠŸç‡
        total_attempted = len(fix_results["fixes_attempted"])
        total_successful = len(fix_results["fixes_successful"])
        fix_results["overall_success"] = total_successful == total_attempted and total_attempted > 0
        
        logger.info(f"ğŸ¯ æ—¶é—´æˆ³ä¿®å¤å®Œæˆ: {total_successful}/{total_attempted} æˆåŠŸ")
        
        return fix_results
    
    def verify_timestamp_fixes(self) -> Dict[str, Any]:
        """éªŒè¯æ—¶é—´æˆ³ä¿®å¤æ•ˆæœ"""
        logger.info("ğŸ” éªŒè¯æ—¶é—´æˆ³ä¿®å¤æ•ˆæœ...")
        
        verification = {
            "timestamp": datetime.now().isoformat(),
            "view_tests": {},
            "data_flow_test": {},
            "overall_health": False
        }
        
        # æµ‹è¯•ä¿®å¤çš„è§†å›¾
        views_to_test = [
            "p_cloud_clean_merged_dedup_v",
            "p_cloud_today_v"
        ]
        
        for view_name in views_to_test:
            try:
                # æµ‹è¯•è§†å›¾æ˜¯å¦å¯è®¿é—®å¹¶è·å–æ ·æœ¬æ•°æ®
                query = f"""
                SELECT 
                    COUNT(*) as count,
                    MIN(ts_utc) as min_ts,
                    MAX(ts_utc) as max_ts
                FROM `{self.project_id}.{self.dataset_id}.{view_name}`
                """
                result = self.client.query(query).result()
                row = list(result)[0]
                
                verification["view_tests"][view_name] = {
                    "accessible": True,
                    "row_count": row.count,
                    "min_timestamp": str(row.min_ts) if row.min_ts else None,
                    "max_timestamp": str(row.max_ts) if row.max_ts else None,
                    "healthy": row.count > 0
                }
                
                logger.info(f"âœ… {view_name}: {row.count} è¡Œ, æ—¶é—´èŒƒå›´: {row.min_ts} ~ {row.max_ts}")
                
            except Exception as e:
                verification["view_tests"][view_name] = {
                    "accessible": False,
                    "error": str(e),
                    "healthy": False
                }
                logger.error(f"âŒ {view_name}: {e}")
        
        # æµ‹è¯•å®Œæ•´æ•°æ®æµ
        try:
            # æµ‹è¯•signal_pool_union_v3
            query = f"SELECT COUNT(*) as count FROM `{self.project_id}.{self.dataset_id}.signal_pool_union_v3`"
            result = self.client.query(query).result()
            signal_count = list(result)[0].count
            
            # æµ‹è¯•lab_push_candidates_v2
            query = f"SELECT COUNT(*) as count FROM `{self.project_id}.{self.dataset_id}.lab_push_candidates_v2`"
            result = self.client.query(query).result()
            candidates_count = list(result)[0].count
            
            verification["data_flow_test"] = {
                "signal_pool_count": signal_count,
                "candidates_count": candidates_count,
                "data_flow_healthy": signal_count > 0 and candidates_count > 0
            }
            
            logger.info(f"ğŸ“Š æ•°æ®æµæµ‹è¯•: ä¿¡å·æ±  {signal_count} è¡Œ, å†³ç­–å€™é€‰ {candidates_count} è¡Œ")
            
        except Exception as e:
            verification["data_flow_test"] = {
                "error": str(e),
                "data_flow_healthy": False
            }
            logger.error(f"âŒ æ•°æ®æµæµ‹è¯•å¤±è´¥: {e}")
        
        # è®¡ç®—æ•´ä½“å¥åº·çŠ¶æ€
        all_views_healthy = all(
            test.get("healthy", False) 
            for test in verification["view_tests"].values()
        )
        data_flow_healthy = verification["data_flow_test"].get("data_flow_healthy", False)
        
        verification["overall_health"] = all_views_healthy and data_flow_healthy
        
        return verification
    
    def run_complete_timestamp_fix(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ—¶é—´æˆ³ä¿®å¤æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹PC28æ—¶é—´æˆ³å­—æ®µä¿®å¤...")
        
        complete_results = {
            "fix_timestamp": self.timestamp,
            "analysis": {},
            "fixes": {},
            "verification": {},
            "overall_success": False
        }
        
        # 1. åˆ†æé—®é¢˜
        complete_results["analysis"] = self.analyze_timestamp_issues()
        
        # 2. ä¿®å¤é—®é¢˜
        complete_results["fixes"] = self.fix_timestamp_views()
        
        # 3. éªŒè¯ä¿®å¤
        complete_results["verification"] = self.verify_timestamp_fixes()
        
        # 4. è®¡ç®—æ•´ä½“æˆåŠŸ
        complete_results["overall_success"] = (
            complete_results["fixes"]["overall_success"] and
            complete_results["verification"]["overall_health"]
        )
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        self._generate_timestamp_fix_report(complete_results)
        
        return complete_results
    
    def _generate_timestamp_fix_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆæ—¶é—´æˆ³ä¿®å¤æŠ¥å‘Š"""
        report_path = f"/Users/a606/cloud_function_source/pc28_timestamp_fix_report_{self.timestamp}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ æ—¶é—´æˆ³ä¿®å¤æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    fixer = PC28TimestampFieldFixer()
    
    print("ğŸ”§ PC28æ—¶é—´æˆ³å­—æ®µä¿®å¤ç³»ç»Ÿ")
    print("=" * 50)
    print("ğŸ¯ ç›®æ ‡ï¼šè§£å†³timestampå­—æ®µç±»å‹å’ŒDATEå‡½æ•°é—®é¢˜")
    print("ğŸ“‹ ä¿®å¤èŒƒå›´ï¼šp_cloud_clean_merged_dedup_v, p_cloud_today_v")
    print("=" * 50)
    
    # è¿è¡Œå®Œæ•´ä¿®å¤
    results = fixer.run_complete_timestamp_fix()
    
    # è¾“å‡ºç»“æœ
    analysis = results["analysis"]
    fixes = results["fixes"]
    verification = results["verification"]
    
    print(f"\nğŸ“Š åˆ†æç»“æœ:")
    print(f"  å‘ç°é—®é¢˜: {len(analysis['issues_found'])} ä¸ª")
    for issue in analysis['issues_found']:
        print(f"    - {issue['description']}")
    
    print(f"\nğŸ”§ ä¿®å¤ç»“æœ:")
    print(f"  ä¿®å¤å°è¯•: {len(fixes['fixes_attempted'])} ä¸ª")
    print(f"  ä¿®å¤æˆåŠŸ: {len(fixes['fixes_successful'])} ä¸ª")
    print(f"  ä¿®å¤å¤±è´¥: {len(fixes['fixes_failed'])} ä¸ª")
    
    if fixes['fixes_failed']:
        print(f"  å¤±è´¥è¯¦æƒ…:")
        for failed in fixes['fixes_failed']:
            print(f"    - {failed['view']}: {failed['error'][:100]}...")
    
    print(f"\nğŸ” éªŒè¯ç»“æœ:")
    healthy_views = len([v for v in verification['view_tests'].values() if v.get('healthy', False)])
    total_views = len(verification['view_tests'])
    print(f"  è§†å›¾å¥åº·: {healthy_views}/{total_views}")
    
    for view_name, test in verification['view_tests'].items():
        if test.get('healthy', False):
            print(f"    âœ… {view_name}: {test['row_count']} è¡Œ")
        else:
            print(f"    âŒ {view_name}: {test.get('error', 'Unknown error')[:50]}...")
    
    if verification.get('data_flow_test', {}).get('data_flow_healthy', False):
        signal_count = verification['data_flow_test']['signal_pool_count']
        candidates_count = verification['data_flow_test']['candidates_count']
        print(f"  æ•°æ®æµ: âœ… ä¿¡å·æ±  {signal_count} è¡Œ, å†³ç­–å€™é€‰ {candidates_count} è¡Œ")
    else:
        print(f"  æ•°æ®æµ: âŒ å¼‚å¸¸")
    
    if results["overall_success"]:
        print(f"\nğŸ‰ æ—¶é—´æˆ³ä¿®å¤å®Œæˆï¼")
        print(f"ğŸ’¡ æ‰€æœ‰æ—¶é—´æˆ³å­—æ®µé—®é¢˜å·²è§£å†³ï¼Œæ•°æ®æµæ¢å¤æ­£å¸¸")
    else:
        print(f"\nâš ï¸ æ—¶é—´æˆ³ä¿®å¤æœªå®Œå…¨æˆåŠŸï¼Œè¯·æ£€æŸ¥è¯¦ç»†æŠ¥å‘Š")
    
    return results

if __name__ == "__main__":
    main()