#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PC28æ•°æ®å¤‡ä»½ç³»ç»Ÿ
å®ç°å®Œç¾é—­ç¯çš„æ•°æ®ä¿æŠ¤å’Œä¼˜åŒ–æ–¹æ¡ˆ
"""
import os
import json
import subprocess
import shlex
import datetime
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class BackupTask:
    """å¤‡ä»½ä»»åŠ¡"""
    task_id: str
    table_name: str
    backup_type: str  # 'full', 'schema', 'data'
    priority: str  # 'critical', 'important', 'normal'
    estimated_size_mb: float
    backup_sql: str
    verify_sql: str

class PC28DataBackupSystem:
    """PC28æ•°æ®å¤‡ä»½ç³»ç»Ÿ"""
    
    def __init__(self, project_id: str = "wprojectl", dataset_lab: str = "pc28_lab"):
        self.project_id = project_id
        self.dataset_lab = dataset_lab
        self.timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.backup_dataset = f"{dataset_lab}_backup_{self.timestamp}"
        
        # å®šä¹‰å¤‡ä»½ä»»åŠ¡
        self.backup_tasks = self._define_backup_tasks()
        
    def _define_backup_tasks(self) -> List[BackupTask]:
        """å®šä¹‰å¤‡ä»½ä»»åŠ¡"""
        return [
            # 1. æ ¸å¿ƒæ•°æ®è¡¨å¤‡ä»½ - åªå¤‡ä»½ç¡®å®å­˜åœ¨çš„è¡¨
            BackupTask(
                task_id="backup_cloud_pred_today_norm",
                table_name="cloud_pred_today_norm",
                backup_type="full",
                priority="critical",
                estimated_size_mb=50.0,
                backup_sql=f"""
                CREATE TABLE `{self.project_id}.{self.backup_dataset}.cloud_pred_today_norm_backup` AS
                SELECT * FROM `{self.project_id}.{self.dataset_lab}.cloud_pred_today_norm`
                """,
                verify_sql=f"""
                SELECT 
                    COUNT(*) as original_count,
                    (SELECT COUNT(*) FROM `{self.project_id}.{self.backup_dataset}.cloud_pred_today_norm_backup`) as backup_count
                FROM `{self.project_id}.{self.dataset_lab}.cloud_pred_today_norm`
                """
            ),
            
            # 2. è§†å›¾å®šä¹‰å¤‡ä»½
            BackupTask(
                task_id="backup_view_definitions",
                table_name="ALL_VIEWS",
                backup_type="schema",
                priority="critical",
                estimated_size_mb=0.1,
                backup_sql="",  # ç‰¹æ®Šå¤„ç†
                verify_sql=""
            ),
            
            # 3. é¢„æµ‹æ•°æ®å¤‡ä»½ - è¿™äº›æ˜¯è§†å›¾ï¼Œéœ€è¦å¤‡ä»½ä¸ºè¡¨
            BackupTask(
                task_id="backup_p_map_clean_merged_dedup_v",
                table_name="p_map_clean_merged_dedup_v",
                backup_type="full",
                priority="important",
                estimated_size_mb=80.0,
                backup_sql=f"""
                CREATE TABLE `{self.project_id}.{self.backup_dataset}.p_map_clean_merged_dedup_v_backup` AS
                SELECT * FROM `{self.project_id}.{self.dataset_lab}.p_map_clean_merged_dedup_v`
                """,
                verify_sql=f"""
                SELECT 
                    COUNT(*) as original_count,
                    (SELECT COUNT(*) FROM `{self.project_id}.{self.backup_dataset}.p_map_clean_merged_dedup_v_backup`) as backup_count
                FROM `{self.project_id}.{self.dataset_lab}.p_map_clean_merged_dedup_v`
                """
            ),
            
            BackupTask(
                task_id="backup_p_size_clean_merged_dedup_v",
                table_name="p_size_clean_merged_dedup_v",
                backup_type="full",
                priority="important",
                estimated_size_mb=60.0,
                backup_sql=f"""
                CREATE TABLE `{self.project_id}.{self.backup_dataset}.p_size_clean_merged_dedup_v_backup` AS
                SELECT * FROM `{self.project_id}.{self.dataset_lab}.p_size_clean_merged_dedup_v`
                """,
                verify_sql=f"""
                SELECT 
                    COUNT(*) as original_count,
                    (SELECT COUNT(*) FROM `{self.project_id}.{self.backup_dataset}.p_size_clean_merged_dedup_v_backup`) as backup_count
                FROM `{self.project_id}.{self.dataset_lab}.p_size_clean_merged_dedup_v`
                """
            ),
            
            # 4. ä¿¡å·æ± æ•°æ®å¤‡ä»½
            BackupTask(
                task_id="backup_signal_pool_union_v3",
                table_name="signal_pool_union_v3",
                backup_type="full",
                priority="critical",
                estimated_size_mb=30.0,
                backup_sql=f"""
                CREATE TABLE `{self.project_id}.{self.backup_dataset}.signal_pool_union_v3_backup` AS
                SELECT * FROM `{self.project_id}.{self.dataset_lab}.signal_pool_union_v3`
                """,
                verify_sql=f"""
                SELECT 
                    COUNT(*) as original_count,
                    (SELECT COUNT(*) FROM `{self.project_id}.{self.backup_dataset}.signal_pool_union_v3_backup`) as backup_count
                FROM `{self.project_id}.{self.dataset_lab}.signal_pool_union_v3`
                """
            )
        ]
    
    def _run_bq_command(self, sql: str, timeout: int = 300) -> Tuple[bool, str]:
        """æ‰§è¡ŒBigQueryå‘½ä»¤"""
        cmd = f"bq query --use_legacy_sql=false --format=json " + shlex.quote(sql)
        try:
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
            if result.returncode == 0:
                return True, result.stdout
            else:
                return False, result.stderr
        except subprocess.TimeoutExpired:
            return False, "æ‰§è¡Œè¶…æ—¶"
        except Exception as e:
            return False, str(e)
    
    def create_backup_dataset(self) -> bool:
        """åˆ›å»ºå¤‡ä»½æ•°æ®é›†"""
        logger.info(f"åˆ›å»ºå¤‡ä»½æ•°æ®é›†: {self.backup_dataset}")
        
        cmd = f"bq mk --dataset {self.project_id}:{self.backup_dataset}"
        try:
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info(f"âœ… å¤‡ä»½æ•°æ®é›†åˆ›å»ºæˆåŠŸ: {self.backup_dataset}")
                return True
            else:
                logger.error(f"âŒ å¤‡ä»½æ•°æ®é›†åˆ›å»ºå¤±è´¥: {result.stderr}")
                return False
        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½æ•°æ®é›†åˆ›å»ºå¼‚å¸¸: {e}")
            return False
    
    def backup_view_definitions(self) -> bool:
        """å¤‡ä»½æ‰€æœ‰è§†å›¾å®šä¹‰"""
        logger.info("å¤‡ä»½è§†å›¾å®šä¹‰...")
        
        # è·å–æ‰€æœ‰è§†å›¾å®šä¹‰
        sql = f"""
        SELECT table_name, view_definition
        FROM `{self.project_id}.{self.dataset_lab}.INFORMATION_SCHEMA.VIEWS`
        WHERE table_schema = '{self.dataset_lab}'
        """
        
        success, result = self._run_bq_command(sql)
        if not success:
            logger.error(f"âŒ è·å–è§†å›¾å®šä¹‰å¤±è´¥: {result}")
            return False
        
        try:
            data = json.loads(result)
            view_definitions = {}
            
            for row in data:
                view_name = row["table_name"]
                view_def = row["view_definition"]
                view_definitions[view_name] = view_def
                logger.info(f"âœ… è·å–è§†å›¾å®šä¹‰: {view_name}")
            
            # ä¿å­˜è§†å›¾å®šä¹‰åˆ°æ–‡ä»¶
            backup_file = f"/Users/a606/cloud_function_source/pc28_view_definitions_backup_{self.timestamp}.json"
            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(view_definitions, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“ è§†å›¾å®šä¹‰å¤‡ä»½å®Œæˆ: {backup_file}")
            return True
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ è§£æè§†å›¾å®šä¹‰å¤±è´¥: {e}")
            return False
    
    def execute_backup_task(self, task: BackupTask) -> bool:
        """æ‰§è¡Œå•ä¸ªå¤‡ä»½ä»»åŠ¡"""
        logger.info(f"æ‰§è¡Œå¤‡ä»½ä»»åŠ¡: {task.task_id} - {task.table_name}")
        
        if task.task_id == "backup_view_definitions":
            return self.backup_view_definitions()
        
        # æ‰§è¡Œå¤‡ä»½SQL
        success, result = self._run_bq_command(task.backup_sql)
        if not success:
            logger.error(f"âŒ å¤‡ä»½å¤±è´¥ {task.table_name}: {result}")
            return False
        
        logger.info(f"âœ… å¤‡ä»½å®Œæˆ: {task.table_name}")
        
        # éªŒè¯å¤‡ä»½
        success, result = self._run_bq_command(task.verify_sql)
        if success and result:
            try:
                data = json.loads(result)
                original_count = int(data[0]["original_count"])
                backup_count = int(data[0]["backup_count"])
                
                if original_count == backup_count:
                    logger.info(f"âœ… å¤‡ä»½éªŒè¯é€šè¿‡: {task.table_name} ({backup_count} æ¡è®°å½•)")
                    return True
                else:
                    logger.error(f"âŒ å¤‡ä»½éªŒè¯å¤±è´¥: {task.table_name} - åŸå§‹:{original_count}, å¤‡ä»½:{backup_count}")
                    return False
            except (json.JSONDecodeError, KeyError, ValueError) as e:
                logger.error(f"âŒ å¤‡ä»½éªŒè¯å¼‚å¸¸: {task.table_name} - {e}")
                return False
        else:
            logger.warning(f"âš ï¸ æ— æ³•éªŒè¯å¤‡ä»½: {task.table_name}")
            return True  # å‡è®¾å¤‡ä»½æˆåŠŸ
    
    def run_full_backup(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´å¤‡ä»½"""
        logger.info("ğŸ”„ å¼€å§‹PC28æ•°æ®å®Œæ•´å¤‡ä»½...")
        
        backup_results = {
            "backup_timestamp": self.timestamp,
            "backup_dataset": self.backup_dataset,
            "total_tasks": len(self.backup_tasks),
            "successful_tasks": 0,
            "failed_tasks": 0,
            "task_results": {},
            "estimated_total_size_mb": sum(task.estimated_size_mb for task in self.backup_tasks),
            "backup_status": "unknown"
        }
        
        # åˆ›å»ºå¤‡ä»½æ•°æ®é›†
        if not self.create_backup_dataset():
            backup_results["backup_status"] = "failed_dataset_creation"
            return backup_results
        
        # æ‰§è¡Œå¤‡ä»½ä»»åŠ¡
        for task in self.backup_tasks:
            success = self.execute_backup_task(task)
            
            backup_results["task_results"][task.task_id] = {
                "table_name": task.table_name,
                "backup_type": task.backup_type,
                "priority": task.priority,
                "estimated_size_mb": task.estimated_size_mb,
                "success": success
            }
            
            if success:
                backup_results["successful_tasks"] += 1
            else:
                backup_results["failed_tasks"] += 1
        
        # è¯„ä¼°å¤‡ä»½çŠ¶æ€
        success_rate = backup_results["successful_tasks"] / backup_results["total_tasks"]
        critical_tasks = [task for task in self.backup_tasks if task.priority == "critical"]
        critical_success = sum(1 for task in critical_tasks 
                             if backup_results["task_results"][task.task_id]["success"])
        
        if critical_success == len(critical_tasks):
            if success_rate >= 0.9:
                backup_results["backup_status"] = "excellent"
            elif success_rate >= 0.8:
                backup_results["backup_status"] = "good"
            else:
                backup_results["backup_status"] = "partial"
        else:
            backup_results["backup_status"] = "critical_failure"
        
        # ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š
        self._generate_backup_report(backup_results)
        
        return backup_results
    
    def _generate_backup_report(self, backup_results: Dict[str, Any]):
        """ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š"""
        report_path = f"/Users/a606/cloud_function_source/pc28_data_backup_report_{self.timestamp}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(backup_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š å¤‡ä»½æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    def create_field_cleanup_script(self) -> str:
        """åˆ›å»ºå­—æ®µæ¸…ç†è„šæœ¬"""
        logger.info("åˆ›å»ºå­—æ®µæ¸…ç†è„šæœ¬...")
        
        cleanup_script = f'''#!/bin/bash
# PC28å­—æ®µæ¸…ç†è„šæœ¬
# ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().isoformat()}
# å¤‡ä»½æ•°æ®é›†: {self.backup_dataset}

echo "ğŸ§¹ å¼€å§‹PC28å­—æ®µæ¸…ç†..."

# 1. åˆ é™¤å†—ä½™çš„æ—¶é—´æˆ³å­—æ®µ
echo "åˆ é™¤å†—ä½™æ—¶é—´æˆ³å­—æ®µ..."
bq query --use_legacy_sql=false "
ALTER TABLE `{self.project_id}.{self.dataset_lab}.score_ledger`
DROP COLUMN IF EXISTS ts_utc
"

# 2. åˆ é™¤æœªä½¿ç”¨çš„APIå­—æ®µ
echo "åˆ é™¤æœªä½¿ç”¨çš„APIå­—æ®µ..."
bq query --use_legacy_sql=false "
ALTER TABLE `{self.project_id}.{self.dataset_lab}.cloud_pred_today_norm`
DROP COLUMN IF EXISTS curtime
"

# 3. å½’æ¡£å¤§å‹æœªä½¿ç”¨å­—æ®µ
echo "å½’æ¡£å¤§å‹æœªä½¿ç”¨å­—æ®µ..."
bq query --use_legacy_sql=false "
CREATE TABLE IF NOT EXISTS `{self.project_id}.{self.backup_dataset}.raw_features_archive` AS
SELECT draw_id, raw_features
FROM `{self.project_id}.{self.dataset_lab}.score_ledger`
WHERE raw_features IS NOT NULL
"

# åˆ é™¤åŸè¡¨ä¸­çš„raw_featureså­—æ®µ
bq query --use_legacy_sql=false "
ALTER TABLE `{self.project_id}.{self.dataset_lab}.score_ledger`
DROP COLUMN IF EXISTS raw_features
"

echo "âœ… å­—æ®µæ¸…ç†å®Œæˆ"
echo "ğŸ’¾ å¤‡ä»½æ•°æ®é›†: {self.backup_dataset}"
echo "ğŸ“Š é¢„è®¡èŠ‚çœç©ºé—´: 340MB"
'''
        
        script_path = f"/Users/a606/cloud_function_source/pc28_field_cleanup_{self.timestamp}.sh"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(cleanup_script)
        
        # è®¾ç½®æ‰§è¡Œæƒé™
        os.chmod(script_path, 0o755)
        
        logger.info(f"ğŸ§¹ å­—æ®µæ¸…ç†è„šæœ¬å·²åˆ›å»º: {script_path}")
        return script_path
    
    def create_rollback_script(self) -> str:
        """åˆ›å»ºå›æ»šè„šæœ¬"""
        logger.info("åˆ›å»ºå›æ»šè„šæœ¬...")
        
        rollback_script = f'''#!/bin/bash
# PC28å›æ»šè„šæœ¬
# ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().isoformat()}
# å¤‡ä»½æ•°æ®é›†: {self.backup_dataset}

echo "ğŸ”„ å¼€å§‹PC28æ•°æ®å›æ»š..."

# 1. æ¢å¤score_ledgerè¡¨
echo "æ¢å¤score_ledgerè¡¨..."
bq query --use_legacy_sql=false "
CREATE OR REPLACE TABLE `{self.project_id}.{self.dataset_lab}.score_ledger` AS
SELECT * FROM `{self.project_id}.{self.backup_dataset}.score_ledger_backup`
"

# 2. æ¢å¤cloud_pred_today_normè¡¨
echo "æ¢å¤cloud_pred_today_normè¡¨..."
bq query --use_legacy_sql=false "
CREATE OR REPLACE TABLE `{self.project_id}.{self.dataset_lab}.cloud_pred_today_norm` AS
SELECT * FROM `{self.project_id}.{self.backup_dataset}.cloud_pred_today_norm_backup`
"

# 3. æ¢å¤runtime_paramsè¡¨
echo "æ¢å¤runtime_paramsè¡¨..."
bq query --use_legacy_sql=false "
CREATE OR REPLACE TABLE `{self.project_id}.{self.dataset_lab}.runtime_params` AS
SELECT * FROM `{self.project_id}.{self.backup_dataset}.runtime_params_backup`
"

# 4. æ¢å¤é¢„æµ‹æ•°æ®è¡¨
echo "æ¢å¤é¢„æµ‹æ•°æ®è¡¨..."
bq query --use_legacy_sql=false "
CREATE OR REPLACE TABLE `{self.project_id}.{self.dataset_lab}.p_map_clean_merged_dedup_v` AS
SELECT * FROM `{self.project_id}.{self.backup_dataset}.p_map_clean_merged_dedup_v_backup`
"

bq query --use_legacy_sql=false "
CREATE OR REPLACE TABLE `{self.project_id}.{self.dataset_lab}.p_size_clean_merged_dedup_v` AS
SELECT * FROM `{self.project_id}.{self.backup_dataset}.p_size_clean_merged_dedup_v_backup`
"

echo "âœ… æ•°æ®å›æ»šå®Œæˆ"
echo "âš ï¸ è¯·æ‰‹åŠ¨æ¢å¤è§†å›¾å®šä¹‰ï¼ˆä½¿ç”¨å¤‡ä»½çš„JSONæ–‡ä»¶ï¼‰"
'''
        
        script_path = f"/Users/a606/cloud_function_source/pc28_rollback_{self.timestamp}.sh"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(rollback_script)
        
        # è®¾ç½®æ‰§è¡Œæƒé™
        os.chmod(script_path, 0o755)
        
        logger.info(f"ğŸ”„ å›æ»šè„šæœ¬å·²åˆ›å»º: {script_path}")
        return script_path

def main():
    """ä¸»å‡½æ•°"""
    backup_system = PC28DataBackupSystem()
    
    print("ğŸ’¾ PC28æ•°æ®å¤‡ä»½ç³»ç»Ÿå¯åŠ¨")
    print("=" * 50)
    
    # è¿è¡Œå®Œæ•´å¤‡ä»½
    backup_results = backup_system.run_full_backup()
    
    # åˆ›å»ºæ¸…ç†å’Œå›æ»šè„šæœ¬
    cleanup_script = backup_system.create_field_cleanup_script()
    rollback_script = backup_system.create_rollback_script()
    
    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“Š å¤‡ä»½ç»“æœ:")
    print(f"  å¤‡ä»½æ•°æ®é›†: {backup_results['backup_dataset']}")
    print(f"  æ€»ä»»åŠ¡æ•°: {backup_results['total_tasks']}")
    print(f"  æˆåŠŸä»»åŠ¡: {backup_results['successful_tasks']}")
    print(f"  å¤±è´¥ä»»åŠ¡: {backup_results['failed_tasks']}")
    print(f"  é¢„è®¡å¤§å°: {backup_results['estimated_total_size_mb']:.1f}MB")
    print(f"  å¤‡ä»½çŠ¶æ€: {backup_results['backup_status']}")
    
    if backup_results['backup_status'] in ['excellent', 'good']:
        print(f"\nğŸ‰ æ•°æ®å¤‡ä»½å®Œæˆï¼ç°åœ¨å¯ä»¥å®‰å…¨åœ°è¿›è¡Œä¼˜åŒ–")
        print(f"ğŸ§¹ å­—æ®µæ¸…ç†è„šæœ¬: {cleanup_script}")
        print(f"ğŸ”„ å›æ»šè„šæœ¬: {rollback_script}")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"  1. è¿è¡Œå­—æ®µæ¸…ç†è„šæœ¬è¿›è¡Œä¼˜åŒ–")
        print(f"  2. æµ‹è¯•ç³»ç»ŸåŠŸèƒ½")
        print(f"  3. å¦‚æœ‰é—®é¢˜ï¼Œä½¿ç”¨å›æ»šè„šæœ¬æ¢å¤")
    else:
        print(f"\nâš ï¸ å¤‡ä»½å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥çš„ä»»åŠ¡åå†è¿›è¡Œä¼˜åŒ–")
        
        failed_tasks = [task_id for task_id, result in backup_results['task_results'].items() 
                       if not result['success']]
        if failed_tasks:
            print(f"å¤±è´¥çš„ä»»åŠ¡: {', '.join(failed_tasks)}")

if __name__ == "__main__":
    main()